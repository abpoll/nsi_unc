{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad33027-9c79-4c04-8691-7784b32c92e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T13:41:57.085394Z",
     "iopub.status.busy": "2023-08-31T13:41:57.084855Z",
     "iopub.status.idle": "2023-08-31T13:41:57.093124Z",
     "shell.execute_reply": "2023-08-31T13:41:57.091231Z",
     "shell.execute_reply.started": "2023-08-31T13:41:57.085345Z"
    }
   },
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8489d926-c191-4f5d-8b36-4980fe7fb469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:21:29.037023Z",
     "iopub.status.busy": "2023-09-01T22:21:29.036437Z",
     "iopub.status.idle": "2023-09-01T22:21:29.991303Z",
     "shell.execute_reply": "2023-09-01T22:21:29.989880Z",
     "shell.execute_reply.started": "2023-09-01T22:21:29.036972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef2ce263-d546-45ee-adc1-60147f24f389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:21:29.994623Z",
     "iopub.status.busy": "2023-09-01T22:21:29.994137Z",
     "iopub.status.idle": "2023-09-01T22:21:30.031861Z",
     "shell.execute_reply": "2023-09-01T22:21:30.030422Z",
     "shell.execute_reply.started": "2023-09-01T22:21:29.994575Z"
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import zipfile_deflate64\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76ea7c32-995b-4397-a2a1-668296d87e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:21:30.034052Z",
     "iopub.status.busy": "2023-09-01T22:21:30.033581Z",
     "iopub.status.idle": "2023-09-01T22:21:30.087123Z",
     "shell.execute_reply": "2023-09-01T22:21:30.085988Z",
     "shell.execute_reply.started": "2023-09-01T22:21:30.034007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath directories\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'processed')\n",
    "\n",
    "# Directories for raw exposure, vulnerability (vuln) and \n",
    "# administrative reference files\n",
    "#  all exist so just need references\n",
    "EXP_DIR_R = join(FR, 'exposure')\n",
    "VULN_DIR_R = join(FR, 'vuln')\n",
    "REF_DIR_R = join(FR, 'ref')\n",
    "# Haz is for FEMA NFHL and depth grids\n",
    "HAZ_DIR_R = join(FR, 'haz')\n",
    "\n",
    "# Directories for interim exposure, vulnerability (vuln) and \n",
    "# hazard\n",
    "EXP_DIR_I = join(FI, 'exposure')\n",
    "VULN_DIR_I = join(FI, 'vuln')\n",
    "HAZ_DIR_I = join(FI, 'haz')\n",
    "REF_DIR_I = join(FI, 'ref')\n",
    "\n",
    "# Ensure they exist\n",
    "Path(EXP_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reference fips\n",
    "FIPS = '42101'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546fa5e-7569-4116-9e69-c0a783490295",
   "metadata": {},
   "source": [
    "# Unzip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6035f21a-49e8-4415-839d-ca5ceb0ccae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:21:30.088966Z",
     "iopub.status.busy": "2023-09-01T22:21:30.088570Z",
     "iopub.status.idle": "2023-09-01T22:22:34.946802Z",
     "shell.execute_reply": "2023-09-01T22:22:34.945251Z",
     "shell.execute_reply.started": "2023-09-01T22:21:30.088929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped and moved to interim : noaa\n",
      "Unzipped and moved to interim : tract\n",
      "Unzipped and moved to interim : block\n",
      "Unzipped and moved to interim : bg\n",
      "Unzipped and moved to interim : zcta\n",
      "Unzipped and moved to interim : county\n",
      "Unzipped and moved to interim : nfhl\n",
      "Unzipped and moved to interim : dg\n"
     ]
    }
   ],
   "source": [
    "# For each .zip directory in fr\n",
    "# Create needed subdirectories in interim/\n",
    "# Unzip in the appropriate interim/ subdirectory\n",
    "\n",
    "for path in Path(FR).rglob(\"*.zip\"):\n",
    "    # Avoid hidden files and files in directories\n",
    "    if path.name[0] != \".\":\n",
    "        # Get root for the directory this .zip file is in\n",
    "        zip_root = path.relative_to(FR).parents[0]\n",
    "\n",
    "        # Get path to interim/zip_root\n",
    "        zip_to_path = join(FI, zip_root)\n",
    "\n",
    "        # Make directory, including parents\n",
    "        # No need to check if directory exists bc\n",
    "        # it is only created when this script is run\n",
    "        Path(zip_to_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip to zip_to_path\n",
    "        with ZipFile(path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(zip_to_path)\n",
    "\n",
    "        print('Unzipped and moved to interim : '\n",
    "              + str(path.name).split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14133e0-61f3-4035-a70a-4516a4d7670e",
   "metadata": {},
   "source": [
    "# Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9e8cfc3-cb2d-4925-9466-3ace1facc334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:22:34.952391Z",
     "iopub.status.busy": "2023-09-01T22:22:34.951707Z",
     "iopub.status.idle": "2023-09-01T22:22:35.713707Z",
     "shell.execute_reply": "2023-09-01T22:22:35.712300Z",
     "shell.execute_reply.started": "2023-09-01T22:22:34.952339Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c055a5ec-7b49-4e7a-891f-e1dad66c3b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:22:35.715899Z",
     "iopub.status.busy": "2023-09-01T22:22:35.715418Z",
     "iopub.status.idle": "2023-09-01T22:22:37.960565Z",
     "shell.execute_reply": "2023-09-01T22:22:37.960054Z",
     "shell.execute_reply.started": "2023-09-01T22:22:35.715855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw NSI data\n",
    "nsi_filep = join(EXP_DIR_R, 'nsi.pqt')\n",
    "# Read and reset index\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)\n",
    "\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                             nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                           crs=\"EPSG:4326\")\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f2161a5f-2974-40d2-9962-ea5250559c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:22:37.961357Z",
     "iopub.status.busy": "2023-09-01T22:22:37.961192Z",
     "iopub.status.idle": "2023-09-01T22:22:38.367792Z",
     "shell.execute_reply": "2023-09-01T22:22:38.367242Z",
     "shell.execute_reply.started": "2023-09-01T22:22:37.961342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac68d7e6-d767-44ee-a615-e8c315370a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:22:38.368615Z",
     "iopub.status.busy": "2023-09-01T22:22:38.368445Z",
     "iopub.status.idle": "2023-09-01T22:23:51.902482Z",
     "shell.execute_reply": "2023-09-01T22:23:51.900999Z",
     "shell.execute_reply.started": "2023-09-01T22:22:38.368599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Write out to interim/exposure/\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, 'nsi_res.gpkg')\n",
    "nsi_res.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb195-f0b5-4995-8321-4a168478c05b",
   "metadata": {},
   "source": [
    "# Prepare depth-damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "23e58412-7c6d-4ffc-a7ff-81105e628708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:51.904839Z",
     "iopub.status.busy": "2023-09-01T22:23:51.904341Z",
     "iopub.status.idle": "2023-09-01T22:23:52.795256Z",
     "shell.execute_reply": "2023-09-01T22:23:52.793762Z",
     "shell.execute_reply.started": "2023-09-01T22:23:51.904790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw naccs data\n",
    "# vuln/physical is a directory w/ files that \n",
    "# is pre-supplied for the user of this codebase\n",
    "# The NACCS data is extracted from a pdf w/ manual entry\n",
    "# I entered it as a dataframe that mimics the way\n",
    "# Hazus enters data on DDFs so that it could potentially be \n",
    "# ingested into the Hazus database more easily\n",
    "naccs = pd.read_csv(join(VULN_DIR_R, 'physical', 'naccs_ddfs.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aa5b64b5-3620-4fd2-b5a9-073fb9106457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:52.797507Z",
     "iopub.status.busy": "2023-09-01T22:23:52.797030Z",
     "iopub.status.idle": "2023-09-01T22:23:52.875994Z",
     "shell.execute_reply": "2023-09-01T22:23:52.875160Z",
     "shell.execute_reply.started": "2023-09-01T22:23:52.797462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal of processing is to have the data in\n",
    "# tidy format and with non string values\n",
    "\n",
    "# I think we should stick to RES1. Change\n",
    "# NSI processing above as well\n",
    "# Need to change occ type codes for pile \n",
    "# foundation\n",
    "\n",
    "# Drop Description and Source columns\n",
    "# Melt on occupancy damage category\n",
    "# Each depth is associated with a percent damage\n",
    "dropcols = ['Description', 'Source']\n",
    "idvars = ['Occupancy', 'DamageCategory']\n",
    "naccs_melt = naccs.drop(columns=dropcols).melt(id_vars=idvars,\n",
    "                                               var_name='depth_str',\n",
    "                                               value_name='pct_dam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "naccs_melt['depth_str'] = naccs_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "naccs_melt['rel_dam'] = naccs_melt['pct_dam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "dropcols = ['depth_str', 'pct_dam']\n",
    "newcols = ['occtype', 'dam_cat', 'depth_ft', 'rel_dam']\n",
    "naccs_melt = naccs_melt.drop(columns=dropcols)\n",
    "naccs_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(VULN_DIR_I, 'physical')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'naccs_ddfs.csv')\n",
    "naccs_melt.to_csv(vuln_out_filep, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0063a40-6877-4690-8f29-fe533443ee7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:52.877437Z",
     "iopub.status.busy": "2023-09-01T22:23:52.877149Z",
     "iopub.status.idle": "2023-09-01T22:23:52.915528Z",
     "shell.execute_reply": "2023-09-01T22:23:52.914690Z",
     "shell.execute_reply.started": "2023-09-01T22:23:52.877411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Got HAZUS DDFs from here: \n",
    "# https://github.com/cran/hazus/tree/master/data\n",
    "# Downloaded the Hazus 5.1 technical manual and created a \n",
    "# Riverine IDs spreadsheet that tracks what the current version of \n",
    "# HAZUS recommends for riverine DDFs. Will cross reference that with \n",
    "# the downloaded DDFs from the cran/hazus/data/ repository. \n",
    "# I loaded the data in R (it’s .rda) and then converted to csv. \n",
    "\n",
    "# These are ddfs in the same form as the naccs data (I made the\n",
    "# naccs data conform to this as best as I could)\n",
    "hazus_ddfs = pd.read_csv(join(VULN_DIR_R, 'physical', 'haz_fl_dept.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df8fec62-1492-486a-860c-271c58830806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:52.917159Z",
     "iopub.status.busy": "2023-09-01T22:23:52.916753Z",
     "iopub.status.idle": "2023-09-01T22:23:52.980434Z",
     "shell.execute_reply": "2023-09-01T22:23:52.979772Z",
     "shell.execute_reply.started": "2023-09-01T22:23:52.917132Z"
    }
   },
   "outputs": [],
   "source": [
    "# For basements, use FIA (MOD.) which does one and two floors by\n",
    "# A and V zones\n",
    "# For no basements, use USACE - IWR\n",
    "# which does one and two floor, no flood zone specified\n",
    "# 106: FIA (MOD.) 1S WB A zone\n",
    "# 114: \"\" V zone\n",
    "# 108: FIA (MOD.) 1S WB A zone\n",
    "# 116: \"\" V zone\n",
    "\n",
    "# 129: USACE - IWR 1S NB\n",
    "# 130: USCAE - IWR 2S+ NB\n",
    "\n",
    "# Handling pile and pier foundations is important\n",
    "# for RES1, but this will not be an issue for this case-study\n",
    "# since there are no foundation types like this in the NSI\n",
    "# for Philly (is that true, though?)\n",
    "\n",
    "# Subset to DmgFnId in the codes above\n",
    "dmg_ids = [106, 108, 114, 116, 129, 130]\n",
    "hazus_res = hazus_ddfs[(hazus_ddfs['DmgFnId'].isin(dmg_ids)) &\n",
    "                       (hazus_ddfs['Occupancy'] == 'RES1')]\n",
    "\n",
    "# Make occtype column in the same form that the NSI has\n",
    "# e.g. RES1-1SNB\n",
    "# Add column for A or V zone\n",
    "# Note: outside SFHA basement homes will take A zone\n",
    "# What other option do we have? \n",
    "\n",
    "# Split Description by comma. \n",
    "# The split[0] element tells us stories (but description sometimes\n",
    "# says floors instead of story...)\n",
    "# Can get around this issue by looking at first word\n",
    "# The split[1] element\n",
    "# tells us w/ basement or no basement. Use this to create occtype\n",
    "desc = hazus_res['Description'].str.split(',')\n",
    "s_type = desc.str[0].str.split(' ').str[0]\n",
    "s_type = s_type.str.replace('one', '1').str.replace('two', '2')\n",
    "b_type = desc.str[1].str.strip()\n",
    "occtype = np.where(b_type == 'w/ basement',\n",
    "                   s_type + 'SWB',\n",
    "                   s_type + 'SNB')\n",
    "fz = desc.str[-1].str.replace('Structure', '').str.strip()\n",
    "\n",
    "# Need occtype, flood zone, depth_ft, and rel_dam columns\n",
    "# Follow steps from naccs processing to get depth_ft and rel_dam\n",
    "# First, drop unecessary columns\n",
    "# Don't need Source_Table, Occupy_Class, Cover_Class, empty columns\n",
    "# Description, Source, DmgFnId, Occupancy and first col (Unnamed: 0)\n",
    "# because index was written out\n",
    "# Don't need all na columns either (just for automobiles, apparently)\n",
    "hazus_res = hazus_res.loc[:,[col for col in hazus_res.columns if 'ft' in col]]\n",
    "hazus_res = hazus_res.dropna(axis=1, how='all')\n",
    "# Add the occtype and fld_zone columns\n",
    "hazus_res = hazus_res.assign(occtype=occtype,\n",
    "                             fld_zone=fz.str[0])\n",
    "\n",
    "# Then, occtype and fld_zone as index and melt rest of columns. Following \n",
    "# naccs processing\n",
    "idvars = ['occtype', 'fld_zone']\n",
    "hazus_melt = hazus_res.melt(id_vars=idvars,\n",
    "                            var_name='depth_str',\n",
    "                            value_name='pct_dam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "hazus_melt['depth_str'] = hazus_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = hazus_melt.loc[hazus_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = hazus_melt.loc[hazus_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "hazus_melt.loc[hazus_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "hazus_melt.loc[hazus_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "hazus_melt['rel_dam'] = hazus_melt['pct_dam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "dropcols = ['depth_str', 'pct_dam']\n",
    "newcols = ['occtype', 'fld_zone', 'depth_ft', 'rel_dam']\n",
    "hazus_melt = hazus_melt.drop(columns=dropcols)\n",
    "hazus_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(VULN_DIR_I, 'physical')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'hazus_ddfs.csv')\n",
    "hazus_melt.to_csv(vuln_out_filep, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b915f6-da76-4f7a-8d42-e37f4c13b1dc",
   "metadata": {},
   "source": [
    "# Process Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15901b73-fb6d-4c37-9fef-fbf941b1f9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:52.981497Z",
     "iopub.status.busy": "2023-09-01T22:23:52.981263Z",
     "iopub.status.idle": "2023-09-01T22:23:54.570018Z",
     "shell.execute_reply": "2023-09-01T22:23:54.569463Z",
     "shell.execute_reply.started": "2023-09-01T22:23:52.981476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the flood zones, do some processing on columns\n",
    "# The files are unzipped as shape files instead of gdb\n",
    "# We want S_FLD_HAZ_AR \n",
    "fld_haz_fp = join(HAZ_DIR_I, 'nfhl', 'S_FLD_HAZ_AR.shp')\n",
    "nfhl = gpd.read_file(fld_haz_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06340099-28e7-49ef-a999-6b44c48925ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:54.571832Z",
     "iopub.status.busy": "2023-09-01T22:23:54.571626Z",
     "iopub.status.idle": "2023-09-01T22:23:59.818058Z",
     "shell.execute_reply": "2023-09-01T22:23:59.816749Z",
     "shell.execute_reply.started": "2023-09-01T22:23:54.571815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl.loc[:,keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])\n",
    "\n",
    "# Write file\n",
    "fz_out_dir = join(HAZ_DIR_I, 'fld_zon')\n",
    "Path(fz_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "nfhl_f.to_file(join(fz_out_dir, 'floodzones.gpkg'),\n",
    "               driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74c21443-b419-445b-acb0-03a70a8beb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:59.819354Z",
     "iopub.status.busy": "2023-09-01T22:23:59.819114Z",
     "iopub.status.idle": "2023-09-01T22:23:59.912791Z",
     "shell.execute_reply": "2023-09-01T22:23:59.911757Z",
     "shell.execute_reply.started": "2023-09-01T22:23:59.819334Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is optional: delete the nfhl directory to reduce\n",
    "# the file storage burden\n",
    "# TODO: Make this a setting in a config file you can toggle as a user\n",
    "RM_NFHL = True\n",
    "if RM_NFHL:\n",
    "    # Get directory name\n",
    "    nfhl_dir = join(HAZ_DIR_I, 'nfhl')\n",
    "    \n",
    "    # Try to remove the tree; if it fails,\n",
    "    # throw an error using try...except.\n",
    "    try:\n",
    "        shutil.rmtree(nfhl_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6879df0-a805-4e9e-9ef3-9b61ba40a430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:59.913442Z",
     "iopub.status.busy": "2023-09-01T22:23:59.913293Z",
     "iopub.status.idle": "2023-09-01T22:24:00.238519Z",
     "shell.execute_reply": "2023-09-01T22:24:00.237326Z",
     "shell.execute_reply.started": "2023-09-01T22:23:59.913428Z"
    }
   },
   "outputs": [],
   "source": [
    "# No need to do anything for the depth rasters. Can \n",
    "# remove some extemperaneous files to reduce \n",
    "# size of project\n",
    "# This is optional processing - make that clear\n",
    "# In the interim/haz/dg directory, \n",
    "# only keep files that start with Depth_\n",
    "# Can loop through files in the directory and keep all files\n",
    "# that start with these characters\n",
    "dg_dir = join(HAZ_DIR_I, 'dg')\n",
    "for path in Path(dg_dir).iterdir():\n",
    "    if str(path.name[:5]) != 'Depth':\n",
    "        path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f7402-1c85-4d92-8f64-b75ae89fadfb",
   "metadata": {},
   "source": [
    "# Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "88399cad-21d1-4109-a1fb-dd3367006172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:24:00.239713Z",
     "iopub.status.busy": "2023-09-01T22:24:00.239539Z",
     "iopub.status.idle": "2023-09-01T22:24:00.257631Z",
     "shell.execute_reply": "2023-09-01T22:24:00.256581Z",
     "shell.execute_reply.started": "2023-09-01T22:24:00.239697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the reference files oriented to the county\n",
    "# Save as .gpkg\n",
    "# Clear ref/ directory when done\n",
    "\n",
    "# base file name for state files\n",
    "base_state_fp = 'tl_2022_42_'\n",
    "\n",
    "# base file name for us files\n",
    "base_us_fp = 'tl_2022_us_'\n",
    "\n",
    "# state based files\n",
    "state_ref_l = ['bg.shp', 'tabblock20.shp', 'tract.shp']\n",
    "state_ref_l = [base_state_fp + x for x in state_ref_l]\n",
    "\n",
    "# us based files\n",
    "us_ref_l = ['zcta520.shp']\n",
    "us_ref_l = [base_us_fp + x for x in us_ref_l]\n",
    "\n",
    "# merge list\n",
    "ref_l = state_ref_l + us_ref_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e452348b-1c76-43de-a076-bc5a5b0c8ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:24:00.258574Z",
     "iopub.status.busy": "2023-09-01T22:24:00.258407Z",
     "iopub.status.idle": "2023-09-01T22:26:12.829216Z",
     "shell.execute_reply": "2023-09-01T22:26:12.828492Z",
     "shell.execute_reply.started": "2023-09-01T22:24:00.258559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped reference data: bg\n",
      "Clipped reference data: tabblock20\n",
      "Clipped reference data: tract\n",
      "Clipped reference data: zcta520\n"
     ]
    }
   ],
   "source": [
    "# Read in county file\n",
    "counties = gpd.read_file(join(REF_DIR_I, base_us_fp + 'county.shp'))\n",
    "\n",
    "# Identify county from geoid column\n",
    "# If processing multiple counties in the future, \n",
    "# can change to .isin(FIPS) w/ FIPS as a list\n",
    "# Or, more generally can write a wrapper function\n",
    "# for handling string or list input\n",
    "counties_f = counties.loc[counties['GEOID'] == FIPS][['geometry']]\n",
    "counties_f['fips'] = FIPS\n",
    "\n",
    "# Use as reference for clipping other files\n",
    "# For each polygon in counties_f, which corresponds to a county,\n",
    "# you want to check temp_ref.within(polygon) and add these ref \n",
    "# polygons to a dataframe \n",
    "\n",
    "\n",
    "# Need a dict for these\n",
    "# Start with counties_f which we need to write out\n",
    "ref_clip_l = {'counties': counties_f}\n",
    "\n",
    "# Loop through other data, clip to county, save as .gpkg\n",
    "for ref in ref_l:\n",
    "    # Read in the ref file\n",
    "    temp_ref = gpd.read_file(join(REF_DIR_I, ref))\n",
    "    # Do a spatial join for ref in county(ies)\n",
    "    temp_ref_j = gpd.sjoin(temp_ref, counties_f, predicate='within')\n",
    "    # Add to ref clip list\n",
    "    # key/value pairs are refname.gpkg which can be obtained\n",
    "    # by splitting refname.shp on '.' and keeping first part of string\n",
    "    # then getting the last name after splitting on '_'\n",
    "    ref_name = ref.split('.')[0].split('_')[-1]\n",
    "    ref_clip_l[ref_name] = temp_ref_j\n",
    "    # Helpful log message\n",
    "    print('Clipped reference data: ' + ref_name)\n",
    "\n",
    "# Delete the shp files in interim/ref\n",
    "for path in Path(REF_DIR_I).iterdir():\n",
    "    path.unlink()\n",
    "\n",
    "# Save the gpkg files for each ref\n",
    "# I could probably write out the gpkg into \n",
    "# a different directory, or something else\n",
    "# that makes it obvious to just write the\n",
    "# file out when you loop through ref_l above\n",
    "# But reference files don't seem like\n",
    "# processed data -- interim seems right\n",
    "# An improvement might be having a temp directory\n",
    "# for storing unzipped files and then\n",
    "# it makes sense to clean these up\n",
    "# after processing and moving certain files\n",
    "# to interim. That seems neater and more\n",
    "# nominally correct\n",
    "for ref_name, ref in ref_clip_l.items():\n",
    "    ref_out_fp = join(REF_DIR_I, ref_name + '.gpkg')\n",
    "    # Wrote out ref data as gpkg\n",
    "    ref.to_file(ref_out_fp, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064112e-298d-4d81-999a-2f650eee3693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
