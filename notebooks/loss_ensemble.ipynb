{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff7bcd5-9ee5-4e88-9c02-cfcfdb0a9ca5",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2beaee75-dc09-4346-be50-107b04c87737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:30:25.680056Z",
     "iopub.status.busy": "2023-09-06T17:30:25.679517Z",
     "iopub.status.idle": "2023-09-06T17:30:26.187514Z",
     "shell.execute_reply": "2023-09-06T17:30:26.185955Z",
     "shell.execute_reply.started": "2023-09-06T17:30:25.680006Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1b395f-3b85-4b6a-aad7-c605058b80a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:30:26.190783Z",
     "iopub.status.busy": "2023-09-06T17:30:26.190305Z",
     "iopub.status.idle": "2023-09-06T17:30:29.578820Z",
     "shell.execute_reply": "2023-09-06T17:30:29.577627Z",
     "shell.execute_reply.started": "2023-09-06T17:30:26.190735Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005492e-de55-44cf-a670-5c24dc0ebbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:30:29.580047Z",
     "iopub.status.busy": "2023-09-06T17:30:29.579807Z",
     "iopub.status.idle": "2023-09-06T17:30:29.612802Z",
     "shell.execute_reply": "2023-09-06T17:30:29.611928Z",
     "shell.execute_reply.started": "2023-09-06T17:30:29.580030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath directories\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'results')\n",
    "\n",
    "# Directories for interim exposure, vulnerability (vuln) and \n",
    "# hazard\n",
    "EXP_DIR_I = join(FI, 'exposure')\n",
    "VULN_DIR_I = join(FI, 'vuln')\n",
    "HAZ_DIR_I = join(FI, 'haz')\n",
    "REF_DIR_I = join(FI, 'ref')\n",
    "\n",
    "# Ensure they exist\n",
    "Path(EXP_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare our results directory\n",
    "Path(FP).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reference fips\n",
    "FIPS = '42101'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027a649-e70b-46bc-9aec-510f266c6e53",
   "metadata": {},
   "source": [
    "# Prepare structure data for loss ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b705f3f-a6e3-4f01-a674-c710a02bc75b",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd4f300-023d-43fb-b3a0-2a61a202bfa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:41:42.487977Z",
     "iopub.status.busy": "2023-09-06T17:41:42.487680Z",
     "iopub.status.idle": "2023-09-06T17:42:27.709440Z",
     "shell.execute_reply": "2023-09-06T17:42:27.708091Z",
     "shell.execute_reply.started": "2023-09-06T17:41:42.487955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load nsi_res.gpkg, nsi_ref.pqt, and nsi_depths.pqt\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, 'nsi_res.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, 'nsi_depths.pqt'))\n",
    "# Also load the nsi_fz.pqt file because we need to know A or V fz\n",
    "# for using the hazus ddfs\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, 'nsi_fz.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "33632f6c-4789-46da-8d54-632ae3a67038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:30:21.533490Z",
     "iopub.status.busy": "2023-09-06T20:30:21.533255Z",
     "iopub.status.idle": "2023-09-06T20:30:24.098973Z",
     "shell.execute_reply": "2023-09-06T20:30:24.097732Z",
     "shell.execute_reply.started": "2023-09-06T20:30:21.533472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "keep_cols = ['fd_id', 'occtype', 'found_type', 'val_struct']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# We're also going to merge in fzs\n",
    "nsi_res = nsi_res.merge(nsi_fz[['fd_id', 'fld_zone']], on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "basements = structs.str[2:]\n",
    "stories = structs.str[:2]\n",
    "\n",
    "nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "nsi_res_f = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0e5c5c66-b57d-4185-919d-549b371ffe9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:30:24.100260Z",
     "iopub.status.busy": "2023-09-06T20:30:24.100098Z",
     "iopub.status.idle": "2023-09-06T20:30:24.118102Z",
     "shell.execute_reply": "2023-09-06T20:30:24.116719Z",
     "shell.execute_reply.started": "2023-09-06T20:30:24.100245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the fld_zone column processed for the way it needs\n",
    "# to be done for using hazus ddfs\n",
    "# Get the first character of the flood zone and only retain it\n",
    "# if it's a V zone. We are going to use A zone for A and outside\n",
    "# (if any) flood zone depth exposures\n",
    "ve_zone = np.where(full_df['fld_zone'].str[0] == 'V',\n",
    "                   'V',\n",
    "                   'A')\n",
    "full_df = full_df.assign(fz_ddf = ve_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cd26b9cb-28bd-42ae-a05f-d88b886b3539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:30:24.118794Z",
     "iopub.status.busy": "2023-09-06T20:30:24.118647Z",
     "iopub.status.idle": "2023-09-06T20:30:24.180054Z",
     "shell.execute_reply": "2023-09-06T20:30:24.178576Z",
     "shell.execute_reply.started": "2023-09-06T20:30:24.118780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, following the heuristic used in the Philadelphia\n",
    "# hazard mitigation plan for estimating flood losses,\n",
    "# we'll take the max of the Inl and Cst depths for the same RP\n",
    "# to get one value for a RP\n",
    "\n",
    "# First subset to depth columns, keep id in there\n",
    "depth_cols = full_df.columns[full_df.columns.str.contains('Inl|Cst')]\n",
    "depth_df = full_df[list(depth_cols) + ['fd_id']].melt(id_vars='fd_id',\n",
    "                                                      value_name='depth')\n",
    "# Get the two fld types and return period\n",
    "depth_df['fld_type'] = depth_df['variable'].str.split('_').str[0]\n",
    "depth_df['ret_per'] = depth_df['variable'].str.split('_').str[1]\n",
    "# This is giving us a data frame with the depths for flood types\n",
    "# in each return period side by side\n",
    "# We will take the max value of these to represent the depth\n",
    "# for that particular return period\n",
    "depth_df_f = depth_df.pivot(index=['fd_id','ret_per'], columns='fld_type',\n",
    "                            values='depth')\n",
    "depth_df_f['depth'] = np.max(depth_df_f[['Cst', 'Inl']], axis=1)\n",
    "\n",
    "# Now we will get the dataframe with fd_id and depths under different\n",
    "# return periods\n",
    "depths_merge = depth_df_f.reset_index().drop(columns=['Cst', 'Inl'])\n",
    "depths_merge = depths_merge.pivot(index='fd_id', columns='ret_per')\n",
    "\n",
    "# We will modify the column names to have depth_rp\n",
    "# for easier identification later\n",
    "depths_merge.columns = [x[0] + '_' + x[1] for x in depths_merge.columns]\n",
    "\n",
    "# And we will merge this back in to full_df\n",
    "# Drop the depth_cols\n",
    "full_df = full_df.drop(columns=depth_cols)\n",
    "# We reset index on depths_merge so we can merge on fd_id\n",
    "full_df = full_df.merge(depths_merge.reset_index(), on='fd_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600714a-3811-4eff-8282-e29409d0fd26",
   "metadata": {},
   "source": [
    "## Get parameters for structure uncertainty distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e138c1dd-2370-4b50-b27c-b176a3d989e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:43:03.324331Z",
     "iopub.status.busy": "2023-09-06T17:43:03.324099Z",
     "iopub.status.idle": "2023-09-06T17:43:03.388874Z",
     "shell.execute_reply": "2023-09-06T17:43:03.388048Z",
     "shell.execute_reply.started": "2023-09-06T17:43:03.324315Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are also going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from block level which matches\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census block again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "# In a conservative thrust, we can take the reported\n",
    "# coefficient of determination from Philadelphia Assesor's \n",
    "# methodology for estimating market values. This COD can be\n",
    "# multiplied by the estimated value from NSI for a presumably\n",
    "# conservative estimate of the standard deviation surrounding\n",
    "# structure value estimates to observed market values\n",
    "# We can also show in a representative example what would\n",
    "# happen to the loss estimate distribution\n",
    "# if the NSI COD is a factor of 2 larger. We still don't know\n",
    "# if this is a reasonable representation since we assume\n",
    "# there is no bias in the NSI structure valuation by\n",
    "# centering the noise distribution at their estimated value. \n",
    "# In reality, the Philly assessor office reports their estimates\n",
    "# are slightly biased which allows us to use a bias correction\n",
    "# factor if we used that data. Down the line, comparing\n",
    "# what the structure inventory distributions are using different\n",
    "# data sources could be very interesting, including accounting\n",
    "# for different # of RES1 buildings based on more detailed\n",
    "# and survye-based methods in the city assessor data\n",
    "# From the Nolte et al. (2023) large-scale parcel data good\n",
    "# practices data, we know that there are many issues in using parcel\n",
    "# data to fill in important data fields (even occupancy type)\n",
    "# It is not the panacea it appears framed as in the NSI technical\n",
    "# documentation\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Block group appears to have enough\n",
    "# This check is based on the subset of block groups (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "struct_tot = nsi_res[nsi_res['bg_id'].isin(full_df['bg_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b61e5f-2c2a-4868-86e9-5194b5d22510",
   "metadata": {},
   "source": [
    "### Number of stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39a3fc1-8f7a-4cb1-8ba9-a165cdc666e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:43:05.714119Z",
     "iopub.status.busy": "2023-09-06T17:43:05.713915Z",
     "iopub.status.idle": "2023-09-06T17:43:05.752614Z",
     "shell.execute_reply": "2023-09-06T17:43:05.750955Z",
     "shell.execute_reply.started": "2023-09-06T17:43:05.714102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the total number of structures w/ number of stories \n",
    "# in each block gruop\n",
    "stories_sum = struct_tot.groupby(['bg_id', 'stories']).size()\n",
    "# Then get the proportion\n",
    "stories_prop = stories_sum/struct_tot.groupby(['bg_id']).size()\n",
    "# Our parameters can be drawn from this table based on the bg_id\n",
    "# of a structure we are estimating losses for\n",
    "stories_param = stories_prop.reset_index().pivot(index='bg_id',\n",
    "                                                 columns='stories',\n",
    "                                                 values=0).fillna(0)\n",
    "# Since it's a binomial distribution, we only need to specify\n",
    "# one param. Arbitrarily choose 1S\n",
    "# Round the param to the hundredth place\n",
    "# Store in a dict\n",
    "stories_param = stories_param['1S'].round(2)\n",
    "STRY_DICT = dict(stories_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44281191-1c79-441a-beb6-876bd8b41f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T21:42:16.071199Z",
     "iopub.status.busy": "2023-09-05T21:42:16.070692Z",
     "iopub.status.idle": "2023-09-05T21:42:16.107589Z",
     "shell.execute_reply": "2023-09-05T21:42:16.105999Z",
     "shell.execute_reply.started": "2023-09-05T21:42:16.071153Z"
    }
   },
   "source": [
    "### Foundation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b31402a-fdb3-485f-aeee-25073ad9ca95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:59:05.114275Z",
     "iopub.status.busy": "2023-09-06T17:59:05.113719Z",
     "iopub.status.idle": "2023-09-06T17:59:05.168859Z",
     "shell.execute_reply": "2023-09-06T17:59:05.167383Z",
     "shell.execute_reply.started": "2023-09-06T17:59:05.114227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat procedure above\n",
    "found_sum = struct_tot.groupby(['bg_id', 'found_type']).size()\n",
    "found_prop = found_sum/struct_tot.groupby(['bg_id']).size()\n",
    "found_param = found_prop.reset_index().pivot(index='bg_id',\n",
    "                                             columns='found_type',\n",
    "                                             values=0).fillna(0)\n",
    "\n",
    "# We want a dictionary of bg_id to a list of B, C, S\n",
    "# for direct use in our multinomial distribution draw\n",
    "# Store params in a list (each row is bg_id and corresponds to\n",
    "# its own probabilities of each foundation type)\n",
    "params = found_param.values.round(2)\n",
    "# Then create our dictionary\n",
    "FND_DICT = dict(zip(found_param.index, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53855f2d-a13c-40e3-af60-8add535a8a71",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea76ce6d-3aba-478e-905c-b1c3ce690b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:43:09.253007Z",
     "iopub.status.busy": "2023-09-06T17:43:09.252491Z",
     "iopub.status.idle": "2023-09-06T17:43:09.290073Z",
     "shell.execute_reply": "2023-09-06T17:43:09.288251Z",
     "shell.execute_reply.started": "2023-09-06T17:43:09.252961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coefficient of determination for structure value\n",
    "# https://www.phila.gov/media/20220525080608/\n",
    "# tax-year-2023-mass-appraisal-valuation-methodology.pdf\n",
    "# There are three types of single family homes in Philadelphia\n",
    "# Their coefficient of dispersions are\n",
    "# Row: .134\n",
    "# Singles: .113\n",
    "# Twins: .107\n",
    "# Taking the weighted average of these (41080, 3584, 9188)\n",
    "# from the sample sizes of their methodology, we get\n",
    "# a coefficient of determination of .12799\n",
    "# So let's use .128\n",
    "CF_DET = .128\n",
    "\n",
    "# Triangular distributions for first-floor elevation conditioned\n",
    "# on foundation type\n",
    "# Can store these parameters in dictionaries\n",
    "\n",
    "# From this repo: https://github.com/HenryGeorgist/go-fathom/\n",
    "# blob/master/compute/foundationheights.go\n",
    "# from the wing et al. 2022 paper\n",
    "# distributions (min, most likely, max) conditioned on foundation type\n",
    "# Tri(0, .5, 1.5|Slab)\n",
    "# Tri(0, 1.5, 4|Crawl)\n",
    "# Tri(0, 1.5, 4|Basement)\n",
    "# Tri(6, 9, 12|Pier)\n",
    "# Tri (6, 9, 12|Pile)\n",
    "# For this case study, only need 'S', 'C', and 'B'\n",
    "\n",
    "FFE_DICT = {'S': [0, .5, 1.5],\n",
    "            'C': [0, 1.5, 4],\n",
    "            'B': [0, 1.5, 4]}\n",
    "\n",
    "# After you get the series of foundation types for\n",
    "# each house, you can generate random FFE values\n",
    "# from FFE_DICT[fnd_type] \n",
    "# using np.random.default_rng().triangular\n",
    "# Because this is conditional on fnd_type, I think\n",
    "# we can call .loc[fnd_type == ft] for each ft in fnd_type\n",
    "# and generate random draws that is the length of that\n",
    "# subset and get fd_id/ffe pairs as a dict (ffe_map). \n",
    "# After looping through the fnd_types, we can do .map(ffe_map)\n",
    "# We will have to merge dictionaries using x = x | y\n",
    "# https://stackoverflow.com/questions/38987/\n",
    "# how-do-i-merge-two-dictionaries-in-a-single-expression-in-python\n",
    "# which we can do after each loop where y is the newly\n",
    "# created loop for a specific fnd_type subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f997c7-d117-465d-9f79-438ca6cfefe2",
   "metadata": {},
   "source": [
    "# Load and prepare DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a026037-05f8-4a0b-a607-9a440ac2105c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:43:11.328818Z",
     "iopub.status.busy": "2023-09-06T17:43:11.328265Z",
     "iopub.status.idle": "2023-09-06T17:43:11.375904Z",
     "shell.execute_reply": "2023-09-06T17:43:11.374551Z",
     "shell.execute_reply.started": "2023-09-06T17:43:11.328771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need to load in DDFs and make sure they are in the correct form\n",
    "# for drawing pct_dam from a probabilistic distribution\n",
    "# Separate these versions from the point estimate based one\n",
    "\n",
    "# We need bld_type (i.e. 1SWB) dict to param combos\n",
    "# So for NACCS DDFs, we want 1SWB: [min, most likely, max] to\n",
    "# input to the triangular distribution\n",
    "# For Hazus, we want 1SWB_A (also V and w/ NB no fld zone): \n",
    "# [max(0, est-.3), min(1, est+.3)] to input to a uniform distribution\n",
    "\n",
    "# Each bld_type key will map to a dict of depth: params\n",
    "# pairs\n",
    "\n",
    "# Read files\n",
    "naccs = pd.read_csv(join(VULN_DIR_I, 'physical', 'naccs_ddfs.csv'))\n",
    "hazus = pd.read_csv(join(VULN_DIR_I, 'physical', 'hazus_ddfs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "46fbd17f-4084-4123-888e-47fb1bcad5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T21:25:31.821873Z",
     "iopub.status.busy": "2023-09-06T21:25:31.821323Z",
     "iopub.status.idle": "2023-09-06T21:25:31.911458Z",
     "shell.execute_reply": "2023-09-06T21:25:31.909668Z",
     "shell.execute_reply.started": "2023-09-06T21:25:31.821827Z"
    }
   },
   "outputs": [],
   "source": [
    "# NACCS Processing\n",
    "\n",
    "# Add bld_type (i.e. 1SWB) column\n",
    "bld_type = naccs['occtype'].str.split('-').str[1]\n",
    "naccs = naccs.assign(bld_type = bld_type)\n",
    "\n",
    "# Subset to RES1\n",
    "naccs_res = naccs.loc[naccs['occtype'].str.split('-').str[0] == 'RES1']\n",
    "\n",
    "# Want bld_type/depth_ft indices\n",
    "# and Min, ML, Max (in that order) columns\n",
    "# Pivot and reset index to get the data the way we want\n",
    "naccs_res = naccs_res.pivot(index=['bld_type', 'depth_ft'],\n",
    "                            columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "# Get array of the params\n",
    "p_cols = ['Min', 'ML', 'Max']\n",
    "tri_params = naccs_res[p_cols].values\n",
    "\n",
    "# Drop min, ml, max from df\n",
    "naccs_res_f = naccs_res.drop(columns=p_cols)\n",
    "\n",
    "# Store min/ml/max in a list\n",
    "# Need to do this to get our dict of dicts\n",
    "naccs_res_f =  naccs_res_f.assign(params=tri_params.tolist())\n",
    "\n",
    "# The way this data is stored requires a few assumptions\n",
    "# about loss estimation\n",
    "# First, any depths below that lowest depth have 0 loss\n",
    "# Second, any depths above the highest depth have the same\n",
    "# loss as the highest depth \n",
    "# To implement this, we will check depths (after drawing from their\n",
    "# distribution at each location) for whether they are inside\n",
    "# the range of the particular DDF which can be defined with \n",
    "# conastants. If below, loss is 0. If above, swap with\n",
    "# the upper bound\n",
    "# Third, we can round depths to the nearest value in the\n",
    "# dictionary to estimate their loss. There is no guidance in the\n",
    "# use of DDFs about interpolating between values given on the DDF\n",
    "# NFIP assessed damages data (recently released with the new v2 of\n",
    "# the NFIP claims) only provides depth in feet, rounded to the\n",
    "# nearest foot. So, any uncertainty surrounding the depth-damage\n",
    "# relationship for any foot should include some component of \n",
    "# measurement error in representing some non rounded depth value\n",
    "# to the rounded value and estimating a relationship\n",
    "# To implement this, we will round all depths to the nearest foot\n",
    "# before we check for whether they are inside the bounds for\n",
    "# estimating losses with a particular depth-damage function\n",
    "\n",
    "# We want all depths above max depths for the DDFs\n",
    "# to take the param values of the max depth DDF\n",
    "# First, we groupby bld type for naccs and get max depth for\n",
    "# each bld type\n",
    "# It turns out they are all 10 feet, but this may not always be the\n",
    "# case so it is helpful to have code that can handle when\n",
    "# ddfs have different max heights\n",
    "max_naccs_d = naccs_res_f.groupby(['bld_type'])['depth_ft'].idxmax()\n",
    "# Locate these rows in the dataframe for the ddfs\n",
    "max_d_params = naccs_res_f.iloc[max_naccs_d]\n",
    "# Can create a dict of bld_type to params\n",
    "# which will be called for any instance in loss estimation\n",
    "# where a depth value is not null, but the params value is\n",
    "# We will just use this dict to fill the param values\n",
    "# with those corresponding to the max depth for that same bld type\n",
    "max_d_dict = dict(zip(max_d_params['bld_type'], max_d_params['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "211b6331-185f-45d8-b4fa-6e8fd156b812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T17:43:13.076967Z",
     "iopub.status.busy": "2023-09-06T17:43:13.076458Z",
     "iopub.status.idle": "2023-09-06T17:43:13.128257Z",
     "shell.execute_reply": "2023-09-06T17:43:13.126777Z",
     "shell.execute_reply.started": "2023-09-06T17:43:13.076923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hazus processing\n",
    "# Similar to the above, but we are creating a new column\n",
    "# that combines bld_type and fld_zone to use as our key\n",
    "# for getting the depth/dam dict\n",
    "ddf_id = np.where(hazus['fld_zone'].notnull(),\n",
    "                  hazus['bld_type'] + '_' + hazus['fld_zone'],\n",
    "                  hazus['bld_type'])\n",
    "\n",
    "# After we get this new column, we are going to create two\n",
    "# new columns based on the +/- .3 (30% uncertainty) assumption\n",
    "# from Maggie's paper \n",
    "# (https://www.nature.com/articles/s41467-020-19188-9)\n",
    "# We will take the ddf_id, depth_ft, and these two columns\n",
    "# to do the same thing as before for the dict of dicts\n",
    "# We need to use max(0, ) and min(1, ) to make sure the +/- .3\n",
    "# doesn't lead to negative losses, greater than 100% losses\n",
    "dam_low = np.maximum(0, hazus['rel_dam'] - .3)\n",
    "dam_high = np.minimum(1, hazus['rel_dam'] + .3)\n",
    "\n",
    "hazus = hazus.assign(ddf_id=ddf_id,\n",
    "                     dam_low=dam_low,\n",
    "                     dam_high=dam_high)\n",
    "\n",
    "# get param cols\n",
    "p_cols = ['dam_low', 'dam_high']\n",
    "uni_params = np.round(hazus[p_cols].values, 2)\n",
    "\n",
    "# Get df of ddf_id, depth_ft\n",
    "hazus_f = hazus[['ddf_id', 'depth_ft']]\n",
    "\n",
    "# Add params in\n",
    "hazus_f = hazus_f.assign(params=uni_params.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a17642-37e8-4d4a-9724-d334bd0e72b5",
   "metadata": {},
   "source": [
    "# Run through SOWs and estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1fb8f3d3-d4b8-452c-9c15-809a7e5af986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:27:16.215059Z",
     "iopub.status.busy": "2023-09-06T22:27:16.214560Z",
     "iopub.status.idle": "2023-09-06T22:28:19.405910Z",
     "shell.execute_reply": "2023-09-06T22:28:19.405111Z",
     "shell.execute_reply.started": "2023-09-06T22:27:16.215016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated losses for each member of ensemble: 0.2% RP\n",
      "Estimated losses for each member of ensemble: 01% RP\n",
      "Estimated losses for each member of ensemble: 02% RP\n",
      "Estimated losses for each member of ensemble: 10% RP\n"
     ]
    }
   ],
   "source": [
    "# Here is the sketch of the computational approach for generating\n",
    "# the ensemble\n",
    "# rng refers to np.random.default_rng()\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Reminder of the dictionaries we have to help generate\n",
    "# our ensemble members efficiently\n",
    "# STRY_DICT\n",
    "# FND_DICT\n",
    "# FFE_DICT\n",
    "# hazus_dict\n",
    "# naccs_dict\n",
    "\n",
    "# 10,000 SOWs\n",
    "N_SOW = 10000\n",
    "# Need to create a dataframe w/ 10,000 rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# The way I usually do this is with\n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "             'tract_id', 'zcta_id', 'stories']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "\n",
    "# Draw from the structure value distribution for each property\n",
    "# rng(val_struct, val_struct*CF_DET) where these are array_like\n",
    "values = rng.normal(ens_df['val_struct'], ens_df['val_struct']*CF_DET)\n",
    "\n",
    "# Draw from the #stories distribution\n",
    "# We do this by mapping ens_df values with STRY_DICT\n",
    "# and passing this parameter to rng.binomial()\n",
    "# We also need to create an array of 1s with length\n",
    "# N_SOW * len(full_df) - i.e. len(ens_df)\n",
    "# full_df['bg_id'].map(STRY_DICT)\n",
    "bin_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "bin_p = ens_df['bg_id'].map(STRY_DICT).values\n",
    "# This gives us an array of 0s and 1s\n",
    "# Based on how STRY_DICT is defined, the probability of\n",
    "# success parameter corresponds to 1S, so we need to\n",
    "# swap out 1 with 1S and 0 with 2S\n",
    "stories = rng.binomial(bin_n, bin_p)\n",
    "stories = np.where(stories == 1,\n",
    "                   '1S',\n",
    "                   '2S')\n",
    "\n",
    "# Draw from the fnd_type distribution\n",
    "# We do the same thing as above but with\n",
    "# the FND_DICT. This is a multinomial distribution\n",
    "# and 0, 1, 2 correspond to B, C, S\n",
    "# We get an array returned of the form \n",
    "# [0, 0, 1] (if we have Slab foundation, for example)\n",
    "# so we need to transform this into the corresponding\n",
    "# foundation type array\n",
    "# Can do this with fnds[fnds[0] == 1] = 'B'\n",
    "# fnds[fnds[1]] == 1] = 'C' & fnds[fnds[2] == 1] = 'S'\n",
    "# One way to do the mapping is by treating each\n",
    "# row-array as a binary string and converting it\n",
    "# to an int\n",
    "# So you get [a, b, c] => a*2^2 + b*2^1 + c*2^0\n",
    "# This uniquely maps to 4, 2, and 1\n",
    "# So we can create a dict for 4: 'B', 2: 'C', and 1: 'S'\n",
    "# and make it a pd.Series() (I think this is useful because\n",
    "# pandas can combine this with the 1S and 2S string easily\n",
    "# into a series and we'll need to use that bld_type\n",
    "# for the other dicts we have)\n",
    "\n",
    "# This is our ens_df index aligned multinomial\n",
    "# probabilities array\n",
    "# np.stack makes sure the dtype is correct\n",
    "# Not sure why it is cast to object dtype if\n",
    "# I call .values, but this works...\n",
    "\n",
    "mult_p = np.stack(ens_df['bg_id'].map(FND_DICT))\n",
    "# This is our map of binary string/int\n",
    "# conversions to the foundation type\n",
    "bin_str_map = {4: 'B', 2: 'C', 1: 'S'}\n",
    "# We need our np.ones array \n",
    "mult_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# Draw from mult_p\n",
    "fnds = rng.multinomial(mult_n, mult_p)\n",
    "# Create a series of 4, 2, and 1 from the binary strings\n",
    "# This code accomplishes the conversion outlined in the\n",
    "# note above and comes from this stackoverflow post\n",
    "# https://stackoverflow.com/questions/41069825/\n",
    "# convert-binary-01-numpy-to-integer-or-binary-string\n",
    "fnds_ints = pd.Series(fnds.dot(2**np.arange(fnds.shape[1])[::-1]))\n",
    "# Replace these values with the fnd_type\n",
    "fnd_types = fnds_ints.map(bin_str_map)\n",
    "\n",
    "# We take fnd_types for two tasks now\n",
    "# First, if B, it's WB type home and we\n",
    "# combine this with stories to get the bld_type\n",
    "# This is naccs_ddf_type \n",
    "# We combine bld_type with fz_ddf to get hazus_ddf_type\n",
    "# For our case study, it turns out we will use the same hazus\n",
    "# ddf for the basement houses (_A) since no V zone houses\n",
    "# For no basement, hazus_ddf_type does not add the _fz\n",
    "\n",
    "# Let's get bld_type\n",
    "# Basement type from fnd_types\n",
    "base_types = np.where(fnd_types == 'B',\n",
    "                      'WB',\n",
    "                      'NB')\n",
    "\n",
    "# Combine 1S and this\n",
    "bld_types = pd.Series(stories) + pd.Series(base_types)\n",
    "\n",
    "# In theory, bld_type is naccs_ddf_type. No need to \n",
    "# take this storage up in practice... just refer to bld_type\n",
    "# when needed\n",
    "# For WB homes, hazus_ddf_type is bld_types + '_' + ens_df['fz_ddf']\n",
    "# For NB homes, it's bld_types\n",
    "# It makes practical sense to create a new series for this\n",
    "hazus_ddf_types = pd.Series(np.where(base_types == 'WB',\n",
    "                                     bld_types + '_' + ens_df['fz_ddf'],\n",
    "                                     bld_types))\n",
    "\n",
    "# Second, we are going to use the fnd_type to draw from the\n",
    "# FFE distribution\n",
    "# Need to use np.stack to get the array of floats\n",
    "tri_params = np.stack(fnd_types.map(FFE_DICT))\n",
    "\n",
    "# Can use [:] to access like a matrix and directly input to \n",
    "# rng.triangular\n",
    "# 0, 1, and 2 are column indices corresponding to left,\n",
    "# mode, and right\n",
    "# We round this to the nearest single decimal (i.e. 4.5 was\n",
    "# the max value provided for some of these foundation types)\n",
    "ffes = np.round(rng.triangular(tri_params[:,0],\n",
    "                               tri_params[:,1],\n",
    "                               tri_params[:,2]),\n",
    "                1)\n",
    "\n",
    "# Now we need to adjust the depths in each RP by the ffes\n",
    "# For each depth column in ens_df, do the adjustment\n",
    "# Round the resultant value to the nearest int\n",
    "depth_cols = [x for x in full_df.columns if 'depth' in x]\n",
    "# We can create a dict of ret_per to \n",
    "# (depth - ffe) pairs. The reason to do it\n",
    "# this way is that we can then loop through\n",
    "# all the return periods (keys of the dict)\n",
    "# w/o knowing how many return periods\n",
    "# a given flood model provides. While this is\n",
    "# not strictly needed for our case study, it \n",
    "# could be helpful in the future\n",
    "# We assign np.nan when ens_df[d_col] is 0 because\n",
    "# the flood model implies no depth exposure. Don't want to\n",
    "# get a depth - ffe value that has non-zero losses in the DDF!\n",
    "exp_d = {}\n",
    "for d_col in depth_cols:\n",
    "    ret_per = d_col.split('_')[1] # i.e. depth_0.2 -> 0.2\n",
    "    exp_depth = np.where(ens_df[d_col] > 0,\n",
    "                         np.round(ens_df[d_col] - ffes),\n",
    "                         np.nan)\n",
    "    exp_d[ret_per] = exp_depth\n",
    "\n",
    "# For calculating losses with naccs\n",
    "# check for lower and upper bound of that specific ddf\n",
    "# Use dictionary of RPs to losses\n",
    "naccs_loss = {}\n",
    "for ret_per, depths in exp_d.items():\n",
    "    # Combine building types and depths on index\n",
    "    bld_depths = pd.concat([bld_types, pd.Series(depths)], axis=1)\n",
    "    # Rename columns to correspond to each series\n",
    "    bld_depths.columns = ['bld_type', 'depth_ft']\n",
    "    # Merge bld_type/depths with the ddfs to get params linked up\n",
    "    loss_prep = bld_depths.merge(naccs_res_f, how='left')\n",
    "    # Helpful to have a mask for where there are no flood depths\n",
    "    loss_mask = loss_prep['depth_ft'].notnull()\n",
    "    # When depths are null, no flooding so no damages\n",
    "    loss_prep.loc[~loss_mask, 'rel_dam'] = 0\n",
    "    # There are some depths greater than the support from DDFs\n",
    "    # We are going to use the max_d_dict from preparing the DDFs to\n",
    "    # assign the params from the max depth for the same bld_type\n",
    "    missing_rows = ((loss_mask) &\n",
    "                    (loss_prep['params'].isnull()))\n",
    "    missing_params = loss_prep.loc[missing_rows]['bld_type'].map(max_d_dict)\n",
    "    # Replace the entries with missing params but positive depths\n",
    "    loss_prep.loc[missing_rows, 'params'] = missing_params\n",
    "    # Now we can estimate losses for all nonnull() depth_ft rows\n",
    "    # using the triangular distribution with the 'params' column\n",
    "    # Using the loss_mask, which gives us rows with flooding\n",
    "    # we will first get the ddf_params from 'parms' and\n",
    "    # use np.stack to access the columns for rng()\n",
    "    ddf_params = pd.DataFrame(np.stack(loss_prep.loc[loss_mask]['params']),\n",
    "                              columns=['left', 'mode', 'right'],\n",
    "                              index=loss_prep.loc[loss_mask].index)\n",
    "    # We separate left, mode, max as columns\n",
    "    # which makes it much easier to get the damage values we need\n",
    "    loss_df = pd.concat([loss_prep.loc[loss_mask], ddf_params], axis=1)\n",
    "    \n",
    "    # When there is no actual triangular distribution, because values are\n",
    "    # equal, just return one of the param values, \n",
    "    no_tri_mask = loss_df['left'] == loss_df['right']\n",
    "    loss_df.loc[no_tri_mask, 'rel_dam'] = loss_df.loc[no_tri_mask]['left']\n",
    "    \n",
    "    # Otherwise we draw\n",
    "    # from the triangular distribution\n",
    "    loss_df.loc[~no_tri_mask,\n",
    "                'rel_dam'] = rng.triangular(loss_df.loc[~no_tri_mask]['left'],\n",
    "                                            loss_df.loc[~no_tri_mask]['mode'],\n",
    "                                            loss_df.loc[~no_tri_mask]['right'])\n",
    "    \n",
    "    # Combine to get our losses series\n",
    "    fld_loss = loss_df['rel_dam']\n",
    "    no_fld = loss_prep[loss_prep['rel_dam'].notnull()]['rel_dam']\n",
    "    losses = pd.concat([fld_loss, no_fld], axis=0).sort_index()\n",
    "\n",
    "    # This is what we store in the dict\n",
    "    naccs_loss[ret_per] = losses\n",
    "\n",
    "    print('Estimated losses for each member of ensemble: ' + \n",
    "          ret_per + '% RP')\n",
    "\n",
    "# Same for hazus\n",
    "# If lower than lower bound, assign 0 damage\n",
    "# If higher than upper bound, assign the max depth value\n",
    "# Calculate losses for each and then generate a random\n",
    "# list of 0s and 1s. If 1, use the damage value from naccs\n",
    "# If 0, use hazus\n",
    "\n",
    "    \n",
    "# After this, we have our ensemble. The remaining steps are to\n",
    "# aggregate across the ensemble to calculate expected annual losses\n",
    "# I think we can do things like:\n",
    "# Aggregate total EAL for individual SOWs\n",
    "# Estimate descriptive statistics for EAL of properties across SOWs\n",
    "\n",
    "# Reminder of the dictionaries we have to help generate\n",
    "# our ensemble members efficiently\n",
    "# STRY_DICT\n",
    "# FND_DICT\n",
    "# FFE_DICT\n",
    "# hazus_dict\n",
    "# naccs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3d0f09d2-6900-464d-b16a-36d1fe3ba139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:29:37.235515Z",
     "iopub.status.busy": "2023-09-06T22:29:37.235239Z",
     "iopub.status.idle": "2023-09-06T22:29:38.418053Z",
     "shell.execute_reply": "2023-09-06T22:29:38.417009Z",
     "shell.execute_reply.started": "2023-09-06T22:29:37.235495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.2</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.624814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.898819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.751781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819995</th>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819996</th>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819997</th>\n",
       "      <td>0.240344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819998</th>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819999</th>\n",
       "      <td>0.243163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11820000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.2   01   02   10\n",
       "0         0.624814  0.0  0.0  0.0\n",
       "1         0.612170  0.0  0.0  0.0\n",
       "2         0.898819  0.0  0.0  0.0\n",
       "3         0.874765  0.0  0.0  0.0\n",
       "4         0.751781  0.0  0.0  0.0\n",
       "...            ...  ...  ...  ...\n",
       "11819995  0.161669  0.0  0.0  0.0\n",
       "11819996  0.000157  0.0  0.0  0.0\n",
       "11819997  0.240344  0.0  0.0  0.0\n",
       "11819998  0.049612  0.0  0.0  0.0\n",
       "11819999  0.243163  0.0  0.0  0.0\n",
       "\n",
       "[11820000 rows x 4 columns]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame.from_dict(naccs_loss)\n",
    "\n",
    "# Notes for finishing this up\n",
    "# Repeat the process from above for Hazus stuff\n",
    "# (need to do the max_d_depth dict thing in the prepare DDFs section)\n",
    "# Then we create the random series (50% chance) of 0/1\n",
    "# Get the index of 1s and on the pd.DataFrame.from_dict\n",
    "# use .iloc[] on that. Same for the other DDF source\n",
    "# Should be able to easily concat since index is preserved\n",
    "# Then we merge this with structure value, fnd_type,\n",
    "# ffe, num_stories & fd_id. Those are our ensemble members\n",
    "\n",
    "# From there we can examine results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbd412-5f51-475b-a582-d554bd1d8926",
   "metadata": {},
   "source": [
    "# Get losses from our NSI & Hazus DDF benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8bd3ce81-6a8a-4c47-893a-f98cb275938d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:44:00.776758Z",
     "iopub.status.busy": "2023-09-06T20:44:00.776211Z",
     "iopub.status.idle": "2023-09-06T20:44:00.819207Z",
     "shell.execute_reply": "2023-09-06T20:44:00.817374Z",
     "shell.execute_reply.started": "2023-09-06T20:44:00.776712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          -2.0\n",
       "1          -2.0\n",
       "2          -2.0\n",
       "3          -0.0\n",
       "4          -1.0\n",
       "           ... \n",
       "11819995   -2.0\n",
       "11819996   -2.0\n",
       "11819997   -3.0\n",
       "11819998   -2.0\n",
       "11819999   -2.0\n",
       "Name: depth_01, Length: 11820000, dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_d['01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a745fc-1068-4a0d-8282-212b201df08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
