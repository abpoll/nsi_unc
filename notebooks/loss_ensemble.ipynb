{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff7bcd5-9ee5-4e88-9c02-cfcfdb0a9ca5",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2beaee75-dc09-4346-be50-107b04c87737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:18:57.600548Z",
     "iopub.status.busy": "2023-09-05T19:18:57.599972Z",
     "iopub.status.idle": "2023-09-05T19:18:58.019682Z",
     "shell.execute_reply": "2023-09-05T19:18:58.018078Z",
     "shell.execute_reply.started": "2023-09-05T19:18:57.600482Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1b395f-3b85-4b6a-aad7-c605058b80a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:19:30.443308Z",
     "iopub.status.busy": "2023-09-05T19:19:30.442749Z",
     "iopub.status.idle": "2023-09-05T19:19:35.587230Z",
     "shell.execute_reply": "2023-09-05T19:19:35.586128Z",
     "shell.execute_reply.started": "2023-09-05T19:19:30.443235Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005492e-de55-44cf-a670-5c24dc0ebbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:55:07.435319Z",
     "iopub.status.busy": "2023-09-05T19:55:07.434960Z",
     "iopub.status.idle": "2023-09-05T19:55:07.841449Z",
     "shell.execute_reply": "2023-09-05T19:55:07.839823Z",
     "shell.execute_reply.started": "2023-09-05T19:55:07.435297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath directories\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "FP = join(ABS_DIR, 'data', 'results')\n",
    "\n",
    "# Directories for interim exposure, vulnerability (vuln) and \n",
    "# hazard\n",
    "EXP_DIR_I = join(FI, 'exposure')\n",
    "VULN_DIR_I = join(FI, 'vuln')\n",
    "HAZ_DIR_I = join(FI, 'haz')\n",
    "REF_DIR_I = join(FI, 'ref')\n",
    "\n",
    "# Ensure they exist\n",
    "Path(EXP_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare our results directory\n",
    "Path(FP).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reference fips\n",
    "FIPS = '42101'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027a649-e70b-46bc-9aec-510f266c6e53",
   "metadata": {},
   "source": [
    "# Prepare structure data for loss ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b705f3f-a6e3-4f01-a674-c710a02bc75b",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd4f300-023d-43fb-b3a0-2a61a202bfa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:57:26.120792Z",
     "iopub.status.busy": "2023-09-05T19:57:26.120555Z",
     "iopub.status.idle": "2023-09-05T19:58:12.825636Z",
     "shell.execute_reply": "2023-09-05T19:58:12.824728Z",
     "shell.execute_reply.started": "2023-09-05T19:57:26.120773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load nsi_res.gpkg, nsi_ref.pqt, and nsi_depths.pqt\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, 'nsi_res.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, 'nsi_depths.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33632f6c-4789-46da-8d54-632ae3a67038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T21:31:11.341622Z",
     "iopub.status.busy": "2023-09-05T21:31:11.341315Z",
     "iopub.status.idle": "2023-09-05T21:31:12.967137Z",
     "shell.execute_reply": "2023-09-05T21:31:12.966047Z",
     "shell.execute_reply.started": "2023-09-05T21:31:11.341597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "keep_cols = ['fd_id', 'occtype', 'found_type', 'val_struct']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "basements = structs.str[2:]\n",
    "stories = structs.str[:2]\n",
    "\n",
    "nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "nsi_res_f = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600714a-3811-4eff-8282-e29409d0fd26",
   "metadata": {},
   "source": [
    "## Get parameters for structure uncertainty distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138c1dd-2370-4b50-b27c-b176a3d989e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are also going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from block level which matches\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census block again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "# In a conservative thrust, we can take the reported\n",
    "# coefficient of determination from Philadelphia Assesor's \n",
    "# methodology for estimating market values. This COD can be\n",
    "# multiplied by the estimated value from NSI for a presumably\n",
    "# conservative estimate of the standard deviation surrounding\n",
    "# structure value estimates to observed market values\n",
    "# We can also show in a representative example what would\n",
    "# happen to the loss estimate distribution\n",
    "# if the NSI COD is a factor of 2 larger. We still don't know\n",
    "# if this is a reasonable representation since we assume\n",
    "# there is no bias in the NSI structure valuation by\n",
    "# centering the noise distribution at their estimated value. \n",
    "# In reality, the Philly assessor office reports their estimates\n",
    "# are slightly biased which allows us to use a bias correction\n",
    "# factor if we used that data. Down the line, comparing\n",
    "# what the structure inventory distributions are using different\n",
    "# data sources could be very interesting, including accounting\n",
    "# for different # of RES1 buildings based on more detailed\n",
    "# and survye-based methods in the city assessor data\n",
    "# From the Nolte et al. (2023) large-scale parcel data good\n",
    "# practices data, we know that there are many issues in using parcel\n",
    "# data to fill in important data fields (even occupancy type)\n",
    "# It is not the panacea it appears framed as in the NSI technical\n",
    "# documentation\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Block group appears to have enough\n",
    "# This check is based on the subset of block groups (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "struct_tot = nsi_res[nsi_res['bg_id'].isin(full_df['bg_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b61e5f-2c2a-4868-86e9-5194b5d22510",
   "metadata": {},
   "source": [
    "### Number of stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f39a3fc1-8f7a-4cb1-8ba9-a165cdc666e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T21:46:41.982800Z",
     "iopub.status.busy": "2023-09-05T21:46:41.982246Z",
     "iopub.status.idle": "2023-09-05T21:46:42.038425Z",
     "shell.execute_reply": "2023-09-05T21:46:42.036561Z",
     "shell.execute_reply.started": "2023-09-05T21:46:41.982753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the total number of structures w/ number of stories \n",
    "# in each block gruop\n",
    "stories_sum = struct_tot.groupby(['bg_id', 'stories']).size()\n",
    "# Then get the proportion\n",
    "stories_prop = stories_sum/struct_tot.groupby(['bg_id']).size()\n",
    "# Our parameters can be drawn from this table based on the bg_id\n",
    "# of a structure we are estimating losses for\n",
    "stories_param = stories_prop.reset_index().pivot(index='bg_id',\n",
    "                                                 columns='stories',\n",
    "                                                 values=0).fillna(0)\n",
    "# Since it's a binomial distribution, we only need to specify\n",
    "# one param. Arbitrarily choose 1S\n",
    "# Round the param to the hundredth place\n",
    "# We can access using .loc with the bg_id\n",
    "stories_param = stories_param['1S'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44281191-1c79-441a-beb6-876bd8b41f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T21:42:16.071199Z",
     "iopub.status.busy": "2023-09-05T21:42:16.070692Z",
     "iopub.status.idle": "2023-09-05T21:42:16.107589Z",
     "shell.execute_reply": "2023-09-05T21:42:16.105999Z",
     "shell.execute_reply.started": "2023-09-05T21:42:16.071153Z"
    }
   },
   "source": [
    "### Foundation type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fbb63-7480-475c-8cab-04522697af15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T21:46:14.808475Z",
     "iopub.status.busy": "2023-09-05T21:46:14.807907Z",
     "iopub.status.idle": "2023-09-05T21:46:15.718887Z",
     "shell.execute_reply": "2023-09-05T21:46:15.716937Z",
     "shell.execute_reply.started": "2023-09-05T21:46:14.808428Z"
    }
   },
   "source": [
    "# Repeat procedure above\n",
    "found_sum = struct_tot.groupby(['bg_id', 'found_type']).size()\n",
    "found_prop = found_sum/struct_tot.groupby(['bg_id']).size()\n",
    "found_param = found_prop.reset_index().pivot(index='bg_id',\n",
    "                                             columns='found_type',\n",
    "                                             values=0).fillna(0)\n",
    "\n",
    "# In this case, we end up with a dataframe that is\n",
    "# perfect for calling np.random.multinomial since the pvals\n",
    "# argument can take an np array\n",
    "# So we will just pass in bg_id to loc and call values like\n",
    "# np.random.multinomial(1, found_param.loc['421010054001'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53855f2d-a13c-40e3-af60-8add535a8a71",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76ce6d-3aba-478e-905c-b1c3ce690b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of determination for structure value\n",
    "\n",
    "# Triangular distributions for first-floor elevation conditioned\n",
    "# on foundation type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f997c7-d117-465d-9f79-438ca6cfefe2",
   "metadata": {},
   "source": [
    "# Load and prepare DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a026037-05f8-4a0b-a607-9a440ac2105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to load in DDFs and make sure they are in the correct form\n",
    "# for drawing pct_dam from a probabilistic distribution\n",
    "# Separate these versions from the point estimate based one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a17642-37e8-4d4a-9724-d334bd0e72b5",
   "metadata": {},
   "source": [
    "# Run through SOWs and estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d18658-6713-4746-8ab3-258fb9f2db76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31cbd412-5f51-475b-a582-d554bd1d8926",
   "metadata": {},
   "source": [
    "# Get losses from our NSI & Hazus DDF benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3ce81-6a8a-4c47-893a-f98cb275938d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
