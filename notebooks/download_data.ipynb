{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4d30d3-e9e9-415f-acad-a9c249dbb114",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737a88d6-6fd3-49e9-b7c6-b6dd97a369ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T13:53:24.623032Z",
     "iopub.status.busy": "2023-08-31T13:53:24.622790Z",
     "iopub.status.idle": "2023-08-31T13:53:24.632602Z",
     "shell.execute_reply": "2023-08-31T13:53:24.631072Z",
     "shell.execute_reply.started": "2023-08-31T13:53:24.623012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48360e3-1432-4ba2-b1e3-a642eefa9eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:12:03.443251Z",
     "iopub.status.busy": "2023-08-30T19:12:03.442736Z",
     "iopub.status.idle": "2023-08-30T19:12:06.500580Z",
     "shell.execute_reply": "2023-08-30T19:12:06.498943Z",
     "shell.execute_reply.started": "2023-08-30T19:12:03.443201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from pyproj import CRS\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1029c5f1-668d-441a-84b0-f2be2990445a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:12:06.503302Z",
     "iopub.status.busy": "2023-08-30T19:12:06.502590Z",
     "iopub.status.idle": "2023-08-30T19:12:06.549533Z",
     "shell.execute_reply": "2023-08-30T19:12:06.547771Z",
     "shell.execute_reply.started": "2023-08-30T19:12:06.503226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths (could be replaced by config files or user input)\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "\n",
    "# Directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exposure\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for FEMA NFHL and depth grids\n",
    "HAZ_DIR_R = join(FR, \"haz\")\n",
    "\n",
    "# Make sure directories exist\n",
    "Path(EXP_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_R).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a6b712-807b-48c2-8d4b-925d86198797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:59:55.149321Z",
     "iopub.status.busy": "2023-08-30T19:59:55.148762Z",
     "iopub.status.idle": "2023-08-30T19:59:55.183931Z",
     "shell.execute_reply": "2023-08-30T19:59:55.182092Z",
     "shell.execute_reply.started": "2023-08-30T19:59:55.149272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants (could be replaced by config files or user input)\n",
    "FIPS = '42101'\n",
    "\n",
    "# FEMA \"chunk\" size for API\n",
    "CHUNK_FEMA = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d529f58-fdc4-4786-b965-22132504edc2",
   "metadata": {},
   "source": [
    "# Exposure Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f3ce5-5f20-4318-8af4-ee680b2c7e9b",
   "metadata": {},
   "source": [
    "## National Structure Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9360278-4cc0-418b-b13a-c8b624b1c942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:40:35.034057Z",
     "iopub.status.busy": "2023-08-30T17:40:35.033507Z",
     "iopub.status.idle": "2023-08-30T17:41:07.081333Z",
     "shell.execute_reply": "2023-08-30T17:41:07.080107Z",
     "shell.execute_reply.started": "2023-08-30T17:40:35.034008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the URL\n",
    "# (Could be specified in a config file)\n",
    "url = \"https://nsi.sec.usace.army.mil/nsiapi/structures\"\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the NSI API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for NSI DFs\n",
    "nsi_df_list = []\n",
    "\n",
    "for fips in FIPS:\n",
    "    # GET Request\n",
    "    nsi_get = requests.get(url + '?fips=' + fips)\n",
    "    \n",
    "    # Temp data frame\n",
    "    temp = pd.json_normalize(nsi_get.json()['features'])\n",
    "    \n",
    "    # Add to list\n",
    "    nsi_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nsi = pd.concat(nsi_df_list, axis=0)\n",
    "\n",
    "# TODO: Provide helpful log information\n",
    "# Things like number of rows, whether the get request\n",
    "# was successful, file size, if it was written successfully\n",
    "\n",
    "# Write to file\n",
    "nsi.to_parquet(join(EXP_DIR_R, 'nsi.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982aa139-a7cf-414e-8e25-e1e9cc3b0f23",
   "metadata": {},
   "source": [
    "# Hazard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d73dad20-d1dc-45ce-bf58-7efdbe2bf7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:34:44.579083Z",
     "iopub.status.busy": "2023-08-30T19:34:44.578824Z",
     "iopub.status.idle": "2023-08-30T19:34:45.441020Z",
     "shell.execute_reply": "2023-08-30T19:34:45.439107Z",
     "shell.execute_reply.started": "2023-08-30T19:34:44.579061Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/\n",
    "# download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bc3fd-dd22-4d37-bf13-c35cd654c539",
   "metadata": {},
   "source": [
    "## National Flood Hazard Layer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae809fd2-b5ac-46f9-9ba0-561514d67a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:56:59.392508Z",
     "iopub.status.busy": "2023-08-30T17:56:59.391945Z",
     "iopub.status.idle": "2023-08-30T17:57:30.104510Z",
     "shell.execute_reply": "2023-08-30T17:57:30.102109Z",
     "shell.execute_reply.started": "2023-08-30T17:56:59.392456Z"
    }
   },
   "outputs": [],
   "source": [
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for the current county NFHL after\n",
    "# downloading & cancelling the download\n",
    "# https://map1.msc.fema.gov/data/\n",
    "# FRP/FRD_02040202_PA_GeoTIFFs_20160801\n",
    "# .zip?LOC=ccad78e48360e7a0a5cf6848dfa4db11\n",
    "\n",
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for GeoTIFFs for the Flood Risk Database\n",
    "# https://hazards.fema.gov/nfhlv2/output/County/420757_20230701.zip\n",
    "url = (\"https://hazards.fema.gov/nfhlv2/output/County/420757_20230701.zip\")\n",
    "\n",
    "# Destination file directory\n",
    "dst = Path(join(HAZ_DIR_R, 'nfhl'))\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "# Destination path\n",
    "dst_path = join(dst, 'nfhl.zip')\n",
    "\n",
    "# Download nfhl\n",
    "download_url(url, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7353a-1720-45e6-8be6-577a0b58a162",
   "metadata": {},
   "source": [
    "## Depth Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523f7fa5-830c-4e89-97b9-e43974e858a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:35:46.864472Z",
     "iopub.status.busy": "2023-08-30T19:35:46.864205Z",
     "iopub.status.idle": "2023-08-30T19:35:47.730339Z",
     "shell.execute_reply": "2023-08-30T19:35:47.728349Z",
     "shell.execute_reply.started": "2023-08-30T19:35:46.864449Z"
    }
   },
   "outputs": [],
   "source": [
    "# I went to FEMA Flood Map Service Center\n",
    "# I chose Philadelphia County from the drop down menus\n",
    "# I got the following link for GeoTIFFs for the Flood Risk Database\n",
    "# \"https://map1.msc.fema.gov/data/FRP/FRD_02040202_PA_GeoTIFFs_20160801\" +\n",
    "# \".zip?LOC=ccad78e48360e7a0a5cf6848dfa4db11\"\n",
    "# This takes a while to download because it's a large file\n",
    "# You can confirm the endpoint for this download by following the steps, \n",
    "# clicking download on the DL icon on the webpage, immediately\n",
    "# cancelling the download, and checking your browser's download\n",
    "# page to see what server the download happens from\n",
    "# I did these steps on Google Chrome 114.0.5735.133\n",
    "\n",
    "url = (\"https://map1.msc.fema.gov/data/FRP/\"\n",
    "       + \"FRD_02040202_PA_GeoTIFFs_20160801.zip\")\n",
    "\n",
    "# Destination file directory\n",
    "dst = Path(join(HAZ_DIR_R, 'dg'))\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "# Destination path\n",
    "dst_path = join(dst, 'dg.zip')\n",
    "\n",
    "# Download depth grids\n",
    "# In order to download in a reproducible way, you might have to\n",
    "# set verify=False with the latest requests version\n",
    "# Withoug this setting, I get SSL error\n",
    "# I do not feel comfortable with this, and directly\n",
    "# downloaded the file and uploaded to the data directory.\n",
    "\n",
    "# Therefore, I comment out the download_url line here\n",
    "\n",
    "# download_url(url, dst_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5508eff-1359-40f0-b96d-3dccf3fd16f8",
   "metadata": {},
   "source": [
    "# Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d02007ae-5888-46fc-aa54-e23187e37b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:47:00.435461Z",
     "iopub.status.busy": "2023-08-30T19:47:00.434876Z",
     "iopub.status.idle": "2023-08-30T19:47:01.244933Z",
     "shell.execute_reply": "2023-08-30T19:47:01.242925Z",
     "shell.execute_reply.started": "2023-08-30T19:47:00.435411Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/\n",
    "# download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705473a-9f5b-4aeb-94a7-8a2d83d997d2",
   "metadata": {},
   "source": [
    "## Social Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d23c410-f117-4e79-9a7d-27b4e0f3a3df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:47:30.849923Z",
     "iopub.status.busy": "2023-08-30T19:47:30.849380Z",
     "iopub.status.idle": "2023-08-30T19:47:32.955525Z",
     "shell.execute_reply": "2023-08-30T19:47:32.953607Z",
     "shell.execute_reply.started": "2023-08-30T19:47:30.849876Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI\n",
    "url = 'https://coast.noaa.gov/htdata/SocioEconomic/SoVI2010/SoVI_2010_PA.zip'\n",
    "save_path = join(VULN_DIR_R, 'social', 'noaa.zip')\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c96d57f-1849-465d-a591-23cac1f6b72d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:47:32.958181Z",
     "iopub.status.busy": "2023-08-30T19:47:32.957709Z",
     "iopub.status.idle": "2023-08-30T19:47:35.554035Z",
     "shell.execute_reply": "2023-08-30T19:47:35.552144Z",
     "shell.execute_reply.started": "2023-08-30T19:47:32.958133Z"
    }
   },
   "outputs": [],
   "source": [
    "# CEJST\n",
    "# Data from https://screeningtool.geoplatform.gov/en/downloads\n",
    "url = ('https://static-data-screeningtool.geoplatform.gov/data-versions/'\n",
    "       + '1.0/data/score/downloadable/1.0-communities.csv')\n",
    "\n",
    "save_path = join(VULN_DIR_R, 'social', 'cejst.csv')\n",
    "\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3929991-60b5-4c5f-a140-095e0ff709e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:47:35.555993Z",
     "iopub.status.busy": "2023-08-30T19:47:35.555535Z",
     "iopub.status.idle": "2023-08-30T19:47:35.917182Z",
     "shell.execute_reply": "2023-08-30T19:47:35.915338Z",
     "shell.execute_reply.started": "2023-08-30T19:47:35.555950Z"
    }
   },
   "outputs": [],
   "source": [
    "# FHA LMI\n",
    "# Data from https://www.hudexchange.info/programs/\n",
    "# acs-low-mod-summary-data/\n",
    "# acs-low-mod-summary-data-block-groups-places/\n",
    "\n",
    "url = ('https://www.hudexchange.info/sites/onecpd/assets/File/'\n",
    "       + 'ACS_2015_lowmod_blockgroup_all.xlsx')\n",
    "\n",
    "# Unfortunately xlsx file\n",
    "# But you can use openpyxl engine with pd.read_excel\n",
    "save_path = join(VULN_DIR_R, 'social', 'lmi.xlsx')\n",
    "\n",
    "# Make sure parent directory exists\n",
    "# TODO: There could be a useful helper function for this\n",
    "Path(save_path).parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Request and write\n",
    "download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2788e-f10f-4dd5-9358-0cf47db7bc5b",
   "metadata": {},
   "source": [
    "# Administrative Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6270fdb4-da07-436b-8b95-8e8a4f024b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T19:54:16.327120Z",
     "iopub.status.busy": "2023-08-30T19:54:16.326572Z",
     "iopub.status.idle": "2023-08-30T19:54:16.363934Z",
     "shell.execute_reply": "2023-08-30T19:54:16.362090Z",
     "shell.execute_reply.started": "2023-08-30T19:54:16.327070Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a script of helpful functions and add this\n",
    "# Helper function for downloading zip files\n",
    "# from https://stackoverflow.com/questions/9419162/\n",
    "# download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b376e7-ccde-409d-9710-6552b4157a76",
   "metadata": {},
   "source": [
    "## US Census TIGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d859ddbc-72ff-4968-a6f8-4e861276701a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T20:05:05.655630Z",
     "iopub.status.busy": "2023-08-30T20:05:05.655031Z",
     "iopub.status.idle": "2023-08-30T20:05:06.548835Z",
     "shell.execute_reply": "2023-08-30T20:05:06.546974Z",
     "shell.execute_reply.started": "2023-08-30T20:05:05.655576Z"
    }
   },
   "outputs": [],
   "source": [
    "# U.S. wide data\n",
    "# County boundaries and zip code tabulation areas\n",
    "# TODO: These could be prespecified in a config file\n",
    "COUNTY_URL = ('https://www2.census.gov/geo/tiger/TIGER2022/'\n",
    "              + 'COUNTY/tl_2022_us_county.zip')\n",
    "\n",
    "ZCTA_URL = ('https://www2.census.gov/geo/tiger/TIGER2022/'\n",
    "            + 'ZCTA520/tl_2022_us_zcta520.zip')\n",
    "\n",
    "# State-level data\n",
    "# Get state fips from county code\n",
    "# Get tract, block group, block URLs from state fips\n",
    "STATE_FIPS = FIPS[:2]\n",
    "\n",
    "# Filename conventions for tract, block group, block\n",
    "base_url = 'https://www2.census.gov/geo/tiger/TIGER2022/'\n",
    "TRACT_URL = base_url + '/TRACT/tl_2022_' + STATE_FIPS + '_tract.zip'\n",
    "BG_URL = base_url + '/BG/tl_2022_' + STATE_FIPS + '_bg.zip'\n",
    "BLOCK_URL = (base_url + '/TABBLOCK20/tl_2022_' + STATE_FIPS\n",
    "             + '_tabblock20.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544487d1-e731-4b70-a5cc-958a5231502b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T20:09:07.418649Z",
     "iopub.status.busy": "2023-08-30T20:09:07.418097Z",
     "iopub.status.idle": "2023-08-30T20:09:59.183876Z",
     "shell.execute_reply": "2023-08-30T20:09:59.182379Z",
     "shell.execute_reply.started": "2023-08-30T20:09:07.418599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and wrote file: county\n",
      "Downloaded and wrote file: zcta\n",
      "Downloaded and wrote file: tract\n",
      "Downloaded and wrote file: bg\n",
      "Downloaded and wrote file: block\n"
     ]
    }
   ],
   "source": [
    "# Loop through references\n",
    "ref_list = [COUNTY_URL, ZCTA_URL, TRACT_URL, BG_URL, BLOCK_URL]\n",
    "ref_names = ['county', 'zcta', 'tract', 'bg', 'block']\n",
    "for i, ref in enumerate(ref_list):\n",
    "    # Get save path from ref name and .zip\n",
    "    save_path = join(REF_DIR_R, ref_names[i] + '.zip')\n",
    "    # Request and write\n",
    "    download_url(ref_list[i], save_path)\n",
    "    # Helpful log message\n",
    "    print('Downloaded and wrote file: ' + ref_names[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
