{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fe333a-bfb2-480a-a2a5-b2d66f3e1932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:57:44.269052Z",
     "iopub.status.busy": "2024-01-27T19:57:44.268503Z",
     "iopub.status.idle": "2024-01-27T19:57:44.305506Z",
     "shell.execute_reply": "2024-01-27T19:57:44.304030Z",
     "shell.execute_reply.started": "2024-01-27T19:57:44.268999Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18875f29-add5-4d7d-a52d-b53e89765071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:57:44.613180Z",
     "iopub.status.busy": "2024-01-27T19:57:44.612632Z",
     "iopub.status.idle": "2024-01-27T19:57:46.791275Z",
     "shell.execute_reply": "2024-01-27T19:57:46.789967Z",
     "shell.execute_reply.started": "2024-01-27T19:57:44.613129Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951810f6-cf12-497f-9fbd-a1f56b4cb5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:57:46.793328Z",
     "iopub.status.busy": "2024-01-27T19:57:46.792784Z",
     "iopub.status.idle": "2024-01-27T19:57:46.814666Z",
     "shell.execute_reply": "2024-01-27T19:57:46.813520Z",
     "shell.execute_reply.started": "2024-01-27T19:57:46.793297Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're only using single county for v0.1\n",
    "fips = FIPS[0]\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day\n",
    "stateabbr = STATEABBR[0]\n",
    "nation = NATION[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedffb9-6f85-4e54-af2e-7fc5205c16b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:30:52.981614Z",
     "iopub.status.busy": "2024-01-27T18:30:52.981007Z",
     "iopub.status.idle": "2024-01-27T18:30:53.183507Z",
     "shell.execute_reply": "2024-01-27T18:30:53.181980Z",
     "shell.execute_reply.started": "2024-01-27T18:30:52.981561Z"
    }
   },
   "source": [
    "# Process - everything at county level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b89cdf0-fa8a-4daa-9bc5-a103b9b76faa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:32:05.538214Z",
     "iopub.status.busy": "2024-01-27T18:32:05.537605Z",
     "iopub.status.idle": "2024-01-27T18:32:06.103818Z",
     "shell.execute_reply": "2024-01-27T18:32:06.101926Z",
     "shell.execute_reply.started": "2024-01-27T18:32:05.538160Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates\n",
    "# TODO - future versions of UNSAFE will expand the damage functions\n",
    "# that are represented with uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5469ce-48e4-4005-9bc3-019dae7dc0ae",
   "metadata": {},
   "source": [
    "## NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7cd672-4e61-4c27-81cc-39409806c2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:43:21.234259Z",
     "iopub.status.busy": "2024-01-27T18:43:21.233658Z",
     "iopub.status.idle": "2024-01-27T18:43:36.382788Z",
     "shell.execute_reply": "2024-01-27T18:43:36.382030Z",
     "shell.execute_reply.started": "2024-01-27T18:43:21.234206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NSI\n",
    "nsi_filep = join(EXP_DIR_R, fips, 'nsi.json')\n",
    "with open(nsi_filep, 'r') as fp:\n",
    "    nsi_full = json.load(fp)\n",
    "\n",
    "# json normalize \n",
    "nsi_df = pd.json_normalize(nsi_full['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c69db25-94a3-461e-bfb7-0910e9a1d00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:44:50.385142Z",
     "iopub.status.busy": "2024-01-27T18:44:50.384528Z",
     "iopub.status.idle": "2024-01-27T18:44:50.891134Z",
     "shell.execute_reply": "2024-01-27T18:44:50.890036Z",
     "shell.execute_reply.started": "2024-01-27T18:44:50.385088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to gdf\n",
    "# This is useful for some spatial joins we need to perform\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_df['properties.x'],\n",
    "                              nsi_df['properties.y'])\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_df, geometry=geometry,\n",
    "                           crs=NSI_CRS)\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0eac82-2346-4144-8e54-f5fafe4d4be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:45:49.991053Z",
     "iopub.status.busy": "2024-01-27T18:45:49.990406Z",
     "iopub.status.idle": "2024-01-27T18:45:50.773242Z",
     "shell.execute_reply": "2024-01-27T18:45:50.772133Z",
     "shell.execute_reply.started": "2024-01-27T18:45:49.990998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 in UNSAFE v0.1\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]\n",
    "\n",
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips', 'bldgtype',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct', 'sqft',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "577b2daf-75a8-4706-8561-85364cc73862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:46:35.941805Z",
     "iopub.status.busy": "2024-01-27T18:46:35.941458Z",
     "iopub.status.idle": "2024-01-27T18:47:06.587984Z",
     "shell.execute_reply": "2024-01-27T18:47:06.587143Z",
     "shell.execute_reply.started": "2024-01-27T18:46:35.941774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out to interim/exposure/FIPS/\n",
    "# Single family homes -- sf\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, fips, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "res_out.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e7e27-e32c-409c-876c-d6d4ba777f11",
   "metadata": {},
   "source": [
    "## Depth Damage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77d8c535-f77a-4df8-8bfc-5b450c26650a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:01:44.882378Z",
     "iopub.status.busy": "2024-01-27T19:01:44.881779Z",
     "iopub.status.idle": "2024-01-27T19:01:45.445559Z",
     "shell.execute_reply": "2024-01-27T19:01:45.443720Z",
     "shell.execute_reply.started": "2024-01-27T19:01:44.882326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generally, we will process these DDFs the same way since they\n",
    "# are written in mostly the same format\n",
    "# However, there are a few preprocessing steps necessary for the hazus\n",
    "# ddfs. Also, there are some differences for NACCS vs. HAZUS\n",
    "# shallow uncertainty representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0ed20a9-428b-4218-b4b2-61c97f5c5760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:02:00.576210Z",
     "iopub.status.busy": "2024-01-27T19:02:00.575620Z",
     "iopub.status.idle": "2024-01-27T19:02:00.647888Z",
     "shell.execute_reply": "2024-01-27T19:02:00.646627Z",
     "shell.execute_reply.started": "2024-01-27T19:02:00.576155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read depth damage functions\n",
    "ddf_filedir = join(VULN_DIR_UZ, \"physical\", nation)\n",
    "naccs = pd.read_csv(join(ddf_filedir, \"naccs_ddfs.csv\"))\n",
    "hazus = pd.read_csv(join(ddf_filedir, \"haz_fl_dept.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22126e4-3e9b-4db8-971d-384df3ca8b12",
   "metadata": {},
   "source": [
    "### HAZUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c08ea59-4224-454c-ba31-8b4905e9c96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:02:46.425857Z",
     "iopub.status.busy": "2024-01-27T19:02:46.425257Z",
     "iopub.status.idle": "2024-01-27T19:02:47.029198Z",
     "shell.execute_reply": "2024-01-27T19:02:47.027663Z",
     "shell.execute_reply.started": "2024-01-27T19:02:46.425805Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, preprocessing for hazus ddfs\n",
    "# For basements, use FIA (MOD.) which does one and two floors by\n",
    "# A and V zones\n",
    "# For no basements, use USACE - IWR\n",
    "# which does one and two floor, no flood zone specified\n",
    "# 106: FIA (MOD.) 1S WB A zone\n",
    "# 114: \"\" V zone\n",
    "# 108: FIA (MOD.) 1S WB A zone\n",
    "# 116: \"\" V zone\n",
    "# 129: USACE - IWR 1S NB\n",
    "# 130: USCAE - IWR 2S+ NB\n",
    "# For elevating homes, we can use Pile foundation DDFs\n",
    "# from USACE - Wilmington\n",
    "# 178 - 1S Pile Foundation\n",
    "# 183 - 2S Pile Foundation\n",
    "# These are no basement homes, so to speak\n",
    "# The USACE New Orleans DDFs have some pier foundation\n",
    "# DDFs with fresh & salt water and long & short duration\n",
    "# but this does not appear to apply to out study area\n",
    "# Subset to DmgFnId in the codes above\n",
    "dmg_ids = [106, 108, 114, 116, 129, 130, 178, 183]\n",
    "hazus_res = hazus[(hazus['DmgFnId'].isin(dmg_ids)) & \n",
    "                  (hazus['Occupancy'] == 'RES1')]\n",
    "\n",
    "# Make occtype column in the same form that the NSI has\n",
    "# e.g. RES1-1SNB\n",
    "# Add column for A or V zone\n",
    "# Note: outside SFHA basement homes will take A zone\n",
    "# What other option do we have? \n",
    "\n",
    "# Split Description by comma. \n",
    "# The split[0] element tells us stories (but description sometimes\n",
    "# says floors instead of story...)\n",
    "# Can get around this issue by looking at first word\n",
    "# The split[1] element\n",
    "# tells us w/ basement or no basement. Use this to create occtype\n",
    "desc = hazus_res['Description'].str.split(',')\n",
    "s_type = desc.str[0].str.split(' ').str[0]\n",
    "s_type = s_type.str.replace('one', '1').str.replace('two', '2')\n",
    "b_type = desc.str[1].str.strip()\n",
    "# Below, we are just trying to get archetypes like\n",
    "# 1SNB, 2SWB, 1SPL -- for pile foundation\n",
    "occtype = np.where(b_type == 'w/ basement',\n",
    "                   s_type + 'SWB',\n",
    "                   s_type + 'SNB')\n",
    "occtype = np.where(b_type == 'Pile foundation',\n",
    "                   s_type + 'SPL',\n",
    "                   occtype)\n",
    "# Some of these HAZUS DDFs require us to keep track of the\n",
    "# flood zone they're in\n",
    "# I don't think this matters for our case study since\n",
    "# there are no high wave coastsal zones\n",
    "# This line is designed to work specifically \n",
    "# with the way the descriptions\n",
    "# are written out for the DDFs used in UNSAFE v0.1\n",
    "fz = desc.str[-1].str.lower().str.replace('structure', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83b4a09b-d99e-4b2e-82c0-af6b32f9d476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:02:59.387536Z",
     "iopub.status.busy": "2024-01-27T19:02:59.386937Z",
     "iopub.status.idle": "2024-01-27T19:02:59.459136Z",
     "shell.execute_reply": "2024-01-27T19:02:59.457600Z",
     "shell.execute_reply.started": "2024-01-27T19:02:59.387482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need occtype, flood zone, depth_ft, and rel_dam columns\n",
    "# Follow steps from naccs processing to get depth_ft and rel_dam\n",
    "# First, drop unecessary columns\n",
    "# Don't need Source_Table, Occupy_Class, Cover_Class, empty columns\n",
    "# Description, Source, DmgFnId, Occupancy and first col (Unnamed: 0)\n",
    "# because index was written out\n",
    "# Don't need all na columns either (just for automobiles, apparently)\n",
    "hazus_res = hazus_res.loc[:,[col for col in hazus_res.columns if 'ft' in col]]\n",
    "hazus_res = hazus_res.dropna(axis=1, how='all')\n",
    "# Add the occtype and fld_zone columns\n",
    "hazus_res = hazus_res.assign(occtype=occtype,\n",
    "                             fld_zone=fz.str[0])\n",
    "\n",
    "# Then, occtype and fld_zone as index and melt rest of columns. \n",
    "idvars = ['occtype', 'fld_zone']\n",
    "\n",
    "# Get a tidy ddf back\n",
    "hazus_melt = tidy_ddfs(hazus_res, idvars)\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "# Since we just have the building types, call this\n",
    "# bld_type instead of occtype\n",
    "dropcols = ['depth_str', 'pct_dam', 'occtype', 'fld_zone']\n",
    "\n",
    "# We create an \"id\" col for the ddfs\n",
    "# Our key for HAZUS is bld_type & fld_zone\n",
    "ddf_id = np.where(hazus_melt['fld_zone'].notnull(),\n",
    "                  hazus_melt['occtype'] + '_' + hazus_melt['fld_zone'],\n",
    "                  hazus_melt['occtype'])\n",
    "\n",
    "# Add this to our dataframe so that we can drop bld_type & fld_zone\n",
    "# Easier to have the flood zone as a capital letter\n",
    "# It's lower case because of earlier code to do\n",
    "# some processing\n",
    "hazus_melt = hazus_melt.assign(ddf_id=pd.Series(ddf_id).str.upper())\n",
    "# Drop columns\n",
    "hazus = hazus_melt.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d56abe41-5c9c-4fb1-b646-0a2b5975424c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:19:41.724154Z",
     "iopub.status.busy": "2024-01-27T19:19:41.723650Z",
     "iopub.status.idle": "2024-01-27T19:19:42.097488Z",
     "shell.execute_reply": "2024-01-27T19:19:42.096162Z",
     "shell.execute_reply.started": "2024-01-27T19:19:41.724112Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need to interpolate between the values of the DDF that we\n",
    "# are given. Generally speaking, this introduces artificial spread\n",
    "# in the relative damage distribution since the interpolation is\n",
    "# actually a combo of measurement & modeling uncertainty that\n",
    "# the DDF bounds yield. But, linear interpolation between DDF points\n",
    "# is so common that UNSAFE will not depart from that before a paper\n",
    "# makes the rigorous case that the approach is not needed once\n",
    "# you use DDFs w/ uncertainty bounds\n",
    "\n",
    "# To do this interpolation, we loop through each ddf_id, \n",
    "# and then we will just sample 10 points and create nan rows\n",
    "# (besides ddf_id). Then we interpolate, store in a list\n",
    "# and concat at the end\n",
    "df_int_list = []\n",
    "for ddf_id, df in hazus.groupby('ddf_id'):\n",
    "    # This creates the duplicate rows\n",
    "    ddf_int = df.loc[np.repeat(df.index, 10)].reset_index(drop=True)\n",
    "    # Now we have to make them nulls by finding\n",
    "    # the \"original\" indexed rows\n",
    "    ddf_int.loc[ddf_int.index % 10 != 0, ['depth_ft', 'rel_dam']] = np.nan\n",
    "    # Now we interpolate (just on floats)\n",
    "    ddf_int_floats = ddf_int[['depth_ft', 'rel_dam']].interpolate()\n",
    "    # And add on the ddf_id column back\n",
    "    ddf_int_floats['ddf_id'] = ddf_id\n",
    "    # Drop duplicate rows (this happens for the max depth values)\n",
    "    ddf_int = ddf_int_floats.drop_duplicates()\n",
    "    # And append\n",
    "    df_int_list.append(ddf_int)\n",
    "hazus_ddfs = pd.concat(df_int_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "213cd04c-459e-49eb-a2cb-d39d0f63a6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:19:43.701419Z",
     "iopub.status.busy": "2024-01-27T19:19:43.701004Z",
     "iopub.status.idle": "2024-01-27T19:19:43.758270Z",
     "shell.execute_reply": "2024-01-27T19:19:43.756430Z",
     "shell.execute_reply.started": "2024-01-27T19:19:43.701381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we're going to process this tidy dataframe into a dictionary\n",
    "# for easier ensemble generation\n",
    "\n",
    "# After we get this new column, we are going to create two\n",
    "# new columns based on the +/- .3*pt_estimate (30% uncertainty) \n",
    "# assumption from Maggie's paper \n",
    "# (https://www.nature.com/articles/s41467-020-19188-9)\n",
    "# We will take the ddf_id, depth_ft, and these two columns\n",
    "# to do the same thing as before for the dict of dicts\n",
    "# We need to use max(0, ) and min(1, ) to make sure the +/- .3\n",
    "# doesn't lead to negative losses, greater than 100% losses\n",
    "# Since Maggie's paper, though, there have been studies\n",
    "# suggesting that the damage distribution at each depth\n",
    "# follows more of a long upper tailed Beta distribution. \n",
    "# While we don't have parameters for this, we can at least \n",
    "# represent a wider upper tail. So, -.3 and +.5 can better\n",
    "# represent this\n",
    "# A key assumption is that\n",
    "# we can round depths to the nearest value in the\n",
    "# dictionary to estimate their loss. There is no guidance in the\n",
    "# use of DDFs about interpolating between values given on the DDF\n",
    "# NFIP assessed damages data (recently released with the new v2 of\n",
    "# the NFIP claims) only provides depth in feet, rounded to the\n",
    "# nearest foot. This is the data the curves we use are trained on\n",
    "# or data like this (see NACCS report) \n",
    "# So, any uncertainty surrounding the depth-damage\n",
    "# relationship for any foot should include some component of \n",
    "# measurement error in representing some non rounded depth value\n",
    "# to the rounded value and estimating a relationship\n",
    "# To implement this, we will round all depths to the nearest foot\n",
    "# before we check for whether they are inside the bounds for\n",
    "# estimating losses with a particular depth-damage function\n",
    "# Because of this, rounding the damage parameters to the nearest\n",
    "# hundredth is a much lower order concern\n",
    "dam_low = np.maximum(0,\n",
    "                     hazus_ddfs['rel_dam'] - .3*hazus_ddfs['rel_dam']).round(2)\n",
    "dam_high = np.minimum(1,\n",
    "                      hazus_ddfs['rel_dam'] + .3*hazus_ddfs['rel_dam']).round(2)\n",
    "\n",
    "# Add these columns into our dataframe\n",
    "hazus_ddfs = hazus_ddfs.assign(low=dam_low,\n",
    "                               high=dam_high)\n",
    "\n",
    "# For reasons that will become more obvious later,\n",
    "# it is really helpful to store our params as a list\n",
    "# Get param cols\n",
    "uni_params = ['low', 'high']\n",
    "\n",
    "# Get df of ddf_id, depth_ft, rel_dam\n",
    "hazus_f = hazus_ddfs[['ddf_id', 'depth_ft', 'rel_dam']]\n",
    "# Now store params as a list\n",
    "hazus_f = hazus_f.assign(params=hazus_ddfs[uni_params].values.tolist())\n",
    "\n",
    "# We are going to write out hazus_f \n",
    "# In generating the ensemble for losses\n",
    "# we are going to merge this dataframe\n",
    "# with our structure ensemble - merged with\n",
    "# depths. So, on haz_depth & depth_ft from hazus_f\n",
    "# plus the structure archetype, we can get\n",
    "# the rel_dam parameters. We will draw from this\n",
    "# and get the rel_dam realization for this\n",
    "# state of the world\n",
    "# But, the way this data is stored requires a few assumptions\n",
    "# about loss estimation\n",
    "# First, any depths below that lowest depth have 0 loss\n",
    "# Second, any depths above the highest depth have the same\n",
    "# loss as the highest depth \n",
    "# To implement this, we will check depths (after drawing from their\n",
    "# distribution at each location) for whether they are inside\n",
    "# the range of the particular DDF which can be defined with \n",
    "# conastants. If below, loss is 0. If above, swap with\n",
    "# the upper bound\n",
    "# This is why it's very helpful to have the params stored as \n",
    "# a list, because now we can get unique key/value pairs\n",
    "# for the ddf_id and the params\n",
    "# We need two dicts for HAZUS\n",
    "# One is with the params list\n",
    "# One is just ddf_id to rel_dam (for benchmark loss calculations\n",
    "# when uncertainty is not considered)\n",
    "\n",
    "# We can call our helper function to get our dictionaries\n",
    "HAZUS_MAX_DICT = ddf_max_depth_dict(hazus_f.reset_index(drop=True),\n",
    "                                    'params')\n",
    "HAZUS_MAX_NOUNC_DICT = ddf_max_depth_dict(hazus, 'rel_dam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b549e7d-1a82-4ce1-8f6d-39d2d2562584",
   "metadata": {},
   "source": [
    "### NACCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f86252-c1ee-4f3f-a438-71ecb4bbe875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:18:13.621021Z",
     "iopub.status.busy": "2024-01-27T19:18:13.620411Z",
     "iopub.status.idle": "2024-01-27T19:18:14.231026Z",
     "shell.execute_reply": "2024-01-27T19:18:14.229356Z",
     "shell.execute_reply.started": "2024-01-27T19:18:13.620967Z"
    }
   },
   "outputs": [],
   "source": [
    "# For NACCS, we have the RES 1 DDFs\n",
    "# NACCS need some preprocessing as well\n",
    "# First, subset to the relevant Occupancy types\n",
    "# We want to end up with ddf ids 1swb, etc.\n",
    "# don't need to keep the RES1- part for this case study\n",
    "naccs['res_type'] = naccs['Occupancy'].str.split('-').str[0]\n",
    "naccs['bld_type'] = naccs['Occupancy'].str.split('-').str[1]\n",
    "occ_types = ['1SWB', '2SWB', '1SNB', '2SNB']\n",
    "naccs_res = naccs.loc[(naccs['bld_type'].isin(occ_types)) &\n",
    "                      (naccs['res_type'] == 'RES1')]\n",
    "\n",
    "# Next, drop columns we don't need\n",
    "drop_cols = ['Description', 'Source', 'Occupancy', 'res_type']\n",
    "naccs_res = naccs_res.drop(columns=drop_cols)\n",
    "\n",
    "# Rename DamageCategory\n",
    "naccs_res = naccs_res.rename(columns={'DamageCategory': 'dam_cat',\n",
    "                                      'bld_type': 'ddf_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18514802-2d3e-4233-b606-d27d087adfad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:20:06.047582Z",
     "iopub.status.busy": "2024-01-27T19:20:06.046980Z",
     "iopub.status.idle": "2024-01-27T19:20:06.158918Z",
     "shell.execute_reply": "2024-01-27T19:20:06.157320Z",
     "shell.execute_reply.started": "2024-01-27T19:20:06.047529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the melted dataframe\n",
    "idvars = ['ddf_id', 'dam_cat']\n",
    "naccs_melt = tidy_ddfs(naccs_res, idvars)\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = ['depth_str', 'pct_dam']\n",
    "naccs_f = naccs_melt.drop(columns=drop_cols)\n",
    "\n",
    "# We want to pivot the dataframe so that Min/ML/Max are our columns\n",
    "naccs_piv = naccs_f.pivot(index=['ddf_id', 'depth_ft'],\n",
    "                          columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "\n",
    "# We do the interpolation again\n",
    "df_int_list = []\n",
    "for ddf_id, df in naccs_piv.groupby('ddf_id'):\n",
    "    # This creates the duplicate rows\n",
    "    ddf_int = df.loc[np.repeat(df.index, 10)].reset_index(drop=True)\n",
    "    # Now we have to make them nulls by finding\n",
    "    # the \"original\" indexed rows\n",
    "    float_cols = ['depth_ft', 'ML', 'Max', 'Min']\n",
    "    ddf_int.loc[ddf_int.index % 10 != 0,\n",
    "                float_cols] = np.nan\n",
    "    # Now we interpolate (again, on floats)\n",
    "    ddf_int_floats = ddf_int[float_cols].interpolate().round(2)\n",
    "    # Add in our ddf id col\n",
    "    ddf_int_floats['ddf_id'] = ddf_id\n",
    "    # Drop duplicate rows (this happens for the max depth values)\n",
    "    ddf_int = ddf_int_floats.drop_duplicates()\n",
    "    # And append\n",
    "    df_int_list.append(ddf_int)\n",
    "naccs_ddfs = pd.concat(df_int_list, axis=0)\n",
    "\n",
    "# We want to obtain our 'params' column\n",
    "# same as above\n",
    "p_cols = ['Min', 'ML', 'Max']\n",
    "tri_params = naccs_ddfs[p_cols].values\n",
    "# Drop the p_cols\n",
    "naccs_out = naccs_ddfs.drop(columns=p_cols)\n",
    "naccs_out = naccs_out.assign(params=tri_params.tolist())\n",
    "\n",
    "# Get out dict of max depths\n",
    "NACCS_MAX_DICT = ddf_max_depth_dict(naccs_out.reset_index(drop=True),\n",
    "                                    'params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89df6f-aa3f-44eb-976a-2f89fc583eaf",
   "metadata": {},
   "source": [
    "### Save processed ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76f5cefa-5177-446e-b5fe-1233dc085407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:20:31.806084Z",
     "iopub.status.busy": "2024-01-27T19:20:31.805610Z",
     "iopub.status.idle": "2024-01-27T19:20:31.987507Z",
     "shell.execute_reply": "2024-01-27T19:20:31.986449Z",
     "shell.execute_reply.started": "2024-01-27T19:20:31.806043Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need one hazus file with params for \n",
    "# uncertainty and one w/ just rel_dam\n",
    "hazus_unc = hazus_f[['ddf_id', 'depth_ft', 'params']]\n",
    "hazus_nounc = hazus_f[['ddf_id', 'depth_ft', 'rel_dam']]\n",
    "\n",
    "# Main directory\n",
    "ddf_out_dir = join(VULN_DIR_I, 'physical')\n",
    "# Main ddf files\n",
    "hazus_out_filep = join(ddf_out_dir, 'hazus_ddfs.pqt')\n",
    "hazus_nounc_out_filep = join(ddf_out_dir, 'hazus_ddfs_nounc.pqt')\n",
    "naccs_out_filep = join(ddf_out_dir, 'naccs_ddfs.pqt')\n",
    "# Dictionaries - save as .json for simplicity\n",
    "naccs_max_filep = join(ddf_out_dir, 'naccs.json')\n",
    "hazus_max_filep = join(ddf_out_dir, 'hazus.json')\n",
    "hazus_max_nounc_filep = join(ddf_out_dir, 'hazus_nounc.json')\n",
    "\n",
    "# Only need to call this for one of the files\n",
    "# since they share the same parent directory\n",
    "prepare_saving(hazus_out_filep)\n",
    "\n",
    "# Save as parquet files since\n",
    "# these will directly read in the\n",
    "# DDF params as a list, not as a string\n",
    "hazus_unc.to_parquet(hazus_out_filep)\n",
    "hazus_nounc.to_parquet(hazus_nounc_out_filep)\n",
    "naccs_out.to_parquet(naccs_out_filep)\n",
    "\n",
    "# Save the json files\n",
    "with open(naccs_max_filep, 'w') as fp:\n",
    "    json.dump(NACCS_MAX_DICT, fp)\n",
    "\n",
    "with open(hazus_max_filep, 'w') as fp:\n",
    "    json.dump(HAZUS_MAX_DICT, fp)\n",
    "\n",
    "with open(hazus_max_nounc_filep, 'w') as fp:\n",
    "    json.dump(HAZUS_MAX_NOUNC_DICT, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11615b54-bc09-4943-8b33-3f2092f4bffe",
   "metadata": {},
   "source": [
    "## NFHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49eae204-bad7-400f-91ad-d4fcd0aa7067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:24:33.410351Z",
     "iopub.status.busy": "2024-01-27T19:24:33.410101Z",
     "iopub.status.idle": "2024-01-27T19:24:36.121050Z",
     "shell.execute_reply": "2024-01-27T19:24:36.119943Z",
     "shell.execute_reply.started": "2024-01-27T19:24:33.410333Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want S_FLD_HAZ_AR \n",
    "fld_haz_fp = join(UNZIP_DIR, 'external', 'pol',\n",
    "                  fips, 'S_FLD_HAZ_AR.shp')\n",
    "nfhl = gpd.read_file(fld_haz_fp)\n",
    "\n",
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl.loc[:,keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '0.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])\n",
    "\n",
    "# Write file\n",
    "nfhl_out_filep = join(POL_DIR_I, fips, 'fld_zones.gpkg')\n",
    "prepare_saving(nfhl_out_filep)\n",
    "nfhl_f.to_file(nfhl_out_filep,\n",
    "               driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0f73-1a0c-41a2-ba66-0b54bbe322f5",
   "metadata": {},
   "source": [
    "## Reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b865d9-d498-41df-a1ee-90ed98f5d415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:33:06.755690Z",
     "iopub.status.busy": "2024-01-27T19:33:06.755461Z",
     "iopub.status.idle": "2024-01-27T19:33:15.264005Z",
     "shell.execute_reply": "2024-01-27T19:33:15.263351Z",
     "shell.execute_reply.started": "2024-01-27T19:33:06.755674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get our county from the unzipped county file\n",
    "county_filep = join(REF_DIR_UZ, nation, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "# Return our county as our clipping polygon\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == fips]\n",
    "\n",
    "# TODO - it could be useful to use different clipping files for\n",
    "# different case studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26da0905-cac7-44da-a8ca-6b24b88aa770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:33:15.264887Z",
     "iopub.status.busy": "2024-01-27T19:33:15.264746Z",
     "iopub.status.idle": "2024-01-27T19:35:22.373268Z",
     "shell.execute_reply": "2024-01-27T19:35:22.372611Z",
     "shell.execute_reply.started": "2024-01-27T19:33:15.264874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ref: block\n",
      "Saved Ref: bg\n",
      "Saved Ref: tract\n",
      "Saved Ref: county\n",
      "Saved Ref: zcta\n"
     ]
    }
   ],
   "source": [
    "# For each .shp file in our unzipped ref directory\n",
    "# we are going to reproject & clip, then write out\n",
    "for path in Path(REF_DIR_UZ).rglob('*.shp'):\n",
    "    # Read in the file\n",
    "    ref_shp = gpd.read_file(path)\n",
    "    \n",
    "    # Process the filename to figure out what \n",
    "    # reference data this is\n",
    "    # the files are written out in the form of\n",
    "    # tl_2022_34_tract.shp, for example\n",
    "    # so we split the string on '_', take the\n",
    "    # last element of the array, and ignore\n",
    "    # the last 4 characters\n",
    "    ref_name = path.name.split('_')[-1][:-4]\n",
    "    # Replace the ref name with our ref_name dict values\n",
    "    ref_name_out = REF_NAMES_DICT[ref_name]\n",
    "\n",
    "    # Reproject and clip our reference shapefile\n",
    "    ref_reproj = ref_shp.to_crs(clip_gdf.crs)\n",
    "    ref_clipped = gpd.clip(ref_reproj, clip_gdf)\n",
    "    \n",
    "    # Write file\n",
    "    ref_out_filep = join(REF_DIR_I, fips, ref_name_out + \".gpkg\")\n",
    "    prepare_saving(ref_out_filep)\n",
    "    ref_clipped.to_file(ref_out_filep,\n",
    "                        driver='GPKG')\n",
    "\n",
    "    # Helpful message to track progress\n",
    "    print(\"Saved Ref: \" + ref_name_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0552c4-b890-4740-8aa6-03e62276ecfa",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463bc514-4ee1-4adb-aa28-ca7adda4199e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:49:29.771161Z",
     "iopub.status.busy": "2024-01-27T19:49:29.770963Z",
     "iopub.status.idle": "2024-01-27T19:49:30.477438Z",
     "shell.execute_reply": "2024-01-27T19:49:30.476378Z",
     "shell.execute_reply.started": "2024-01-27T19:49:29.771146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load relevant spatial data (tract, block group)\n",
    "tract_filep = join(REF_DIR_I, fips, 'tract.gpkg')\n",
    "bg_filep = join(REF_DIR_I, fips, 'bg.gpkg')\n",
    "tract_geo = gpd.read_file(tract_filep)\n",
    "bg_geo = gpd.read_file(bg_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7e9eaf-5568-4793-82c3-7d301e06e4ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:40:49.717682Z",
     "iopub.status.busy": "2024-01-27T19:40:49.717436Z",
     "iopub.status.idle": "2024-01-27T19:40:50.865529Z",
     "shell.execute_reply": "2024-01-27T19:40:50.864444Z",
     "shell.execute_reply.started": "2024-01-27T19:40:49.717664Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286047/3885091292.py:3: DtypeWarning: Columns (18,26,70,72,85,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n"
     ]
    }
   ],
   "source": [
    "# CEJST data\n",
    "ce_filep = join(VULN_DIR_R, 'social', nation, 'cejst.csv')\n",
    "cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n",
    "\n",
    "# Columns to keep\n",
    "# Identified as disadvantaged\n",
    "# Census tract 2010 ID\n",
    "keep_cols = ['Census tract 2010 ID', 'Identified as disadvantaged']\n",
    "cejst_sub = cejst[keep_cols]\n",
    "# Rename columns\n",
    "cejst_sub.columns = ['GEOID', 'disadvantaged']\n",
    "\n",
    "# Merge with tract_geo\n",
    "cejst_f = tract_geo[['GEOID', 'geometry']].merge(cejst_sub,\n",
    "                                                 on='GEOID',\n",
    "                                                 how='inner')\n",
    "\n",
    "# Retain only the disadvantaged \n",
    "cejst_f = cejst_f[cejst_f['disadvantaged'] == True].drop(columns='disadvantaged')\n",
    "\n",
    "# Write file\n",
    "cejst_out_filep = join(VULN_DIR_I, 'social', fips, 'cejst.gpkg')\n",
    "prepare_saving(cejst_out_filep)\n",
    "cejst_f.to_file(cejst_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a49029-2d7e-4405-865c-982617491eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:42:30.730267Z",
     "iopub.status.busy": "2024-01-27T19:42:30.729661Z",
     "iopub.status.idle": "2024-01-27T19:42:36.633068Z",
     "shell.execute_reply": "2024-01-27T19:42:36.632122Z",
     "shell.execute_reply.started": "2024-01-27T19:42:30.730215Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI data\n",
    "sovi_suffix = 'SoVI2010_' + stateabbr\n",
    "sovi_filename = 'SoVI0610_' + stateabbr + '.shp'\n",
    "sovi_filep = join(VULN_DIR_UZ, 'social', stateabbr,\n",
    "                  sovi_suffix, sovi_filename)\n",
    "sovi = gpd.read_file(sovi_filep)\n",
    "\n",
    "# Subset columns\n",
    "keep_cols = ['GEOID10', 'SOVI0610' + stateabbr]\n",
    "sovi_high = sovi[keep_cols]\n",
    "\n",
    "# Rename GEOID10 to GEOID\n",
    "sovi_high = sovi_high.rename(columns={'GEOID10': 'GEOID'})\n",
    "\n",
    "# Subset to tracts in our study area (using the tract_geo geometries)\n",
    "sovi_f = tract_geo[['GEOID', 'geometry']].merge(sovi_high,\n",
    "                                                on='GEOID',\n",
    "                                                how='inner')\n",
    "\n",
    "# Write out file\n",
    "sovi_out_filep = join(VULN_DIR_I, 'social', fips, 'sovi.gpkg')\n",
    "sovi_f.to_file(sovi_out_filep, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762ec324-468a-425a-b6c5-41bc2dbcb5df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:49:32.870052Z",
     "iopub.status.busy": "2024-01-27T19:49:32.869831Z",
     "iopub.status.idle": "2024-01-27T19:49:56.960621Z",
     "shell.execute_reply": "2024-01-27T19:49:56.959867Z",
     "shell.execute_reply.started": "2024-01-27T19:49:32.870035Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMI data\n",
    "# Read data\n",
    "lmi_filename = 'ACS_2015_lowmod_blockgroup_all.xlsx'\n",
    "lmi_filep = join(VULN_DIR_R, 'social', nation, lmi_filename)\n",
    "lmi = pd.read_excel(lmi_filep, engine='openpyxl')\n",
    "# Get GEOID for merge (last 12 characters is the bg id)\n",
    "lmi['GEOID'] = lmi['GEOID'].str[-12:]\n",
    "\n",
    "# Retain GEOID and Lowmod_pct\n",
    "keep_cols = ['GEOID', 'Lowmod_pct']\n",
    "lmi_f = bg_geo[['GEOID', 'geometry']].merge(lmi[keep_cols],\n",
    "                                            on='GEOID',\n",
    "                                            how='inner')\n",
    "\n",
    "# Write file\n",
    "lmi_out_filep = join(VULN_DIR_I, 'social', fips, 'lmi.gpkg')\n",
    "lmi_f.to_file(lmi_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7288e-0edf-46f4-a7c3-c1344a287b99",
   "metadata": {},
   "source": [
    "# Link everything to NSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e1eac-454f-436e-aad9-3daa716aa701",
   "metadata": {},
   "source": [
    "## Flood zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81cff71-40ed-41de-bb55-96006cf4976e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:50:51.446398Z",
     "iopub.status.busy": "2024-01-27T19:50:51.445798Z",
     "iopub.status.idle": "2024-01-27T19:51:38.960061Z",
     "shell.execute_reply": "2024-01-27T19:51:38.959365Z",
     "shell.execute_reply.started": "2024-01-27T19:50:51.446343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just for jupyter notebooks\n",
    "# Scripts don't need to reload the data since it runs all at once\n",
    "# Jupyter is more for development, and might only run\n",
    "# some sections at a time\n",
    "# Using same names as above for consistency\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, fips, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)\n",
    "\n",
    "nfhl_out_filep = join(POL_DIR_I, fips, 'fld_zones.gpkg')\n",
    "nfhl_clip_out = gpd.read_file(nfhl_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f922d2-ef21-464e-835c-1b8a9a639ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:51:38.961037Z",
     "iopub.status.busy": "2024-01-27T19:51:38.960893Z",
     "iopub.status.idle": "2024-01-27T19:52:03.090752Z",
     "shell.execute_reply": "2024-01-27T19:52:03.089597Z",
     "shell.execute_reply.started": "2024-01-27T19:51:38.961024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi to flood zone crs\n",
    "nsi_fz = nsi_clip_out.to_crs(nfhl_clip_out.crs)\n",
    "\n",
    "# Spatial join, retaining flood zone cols\n",
    "# Only need the id and geom from nsi for this\n",
    "fz_m = gpd.sjoin(nsi_fz[['fd_id', 'geometry']],\n",
    "                 nfhl_clip_out,\n",
    "                 predicate='within')\n",
    "\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently\n",
    "\n",
    "# Write out fd_id/fld_ar_id/fld_zone/static_bfe\n",
    "keep_cols = ['fd_id', 'fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "fz_m_out = fz_m[keep_cols]\n",
    "\n",
    "nsi_fz_filep = join(EXP_DIR_I, fips, 'nsi_fz.pqt')\n",
    "prepare_saving(nsi_fz_filep)\n",
    "fz_m_out.to_parquet(nsi_fz_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b00e30-d2c2-4edd-8a14-9e7550630325",
   "metadata": {},
   "source": [
    "## Reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76d953f-e02e-4be5-a666-85f431e53cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:58:01.945172Z",
     "iopub.status.busy": "2024-01-27T19:58:01.944921Z",
     "iopub.status.idle": "2024-01-27T19:58:47.635357Z",
     "shell.execute_reply": "2024-01-27T19:58:47.634651Z",
     "shell.execute_reply.started": "2024-01-27T19:58:01.945154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, fips, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7d59ac-21f6-49c8-9155-0decec6de58a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:58:47.636341Z",
     "iopub.status.busy": "2024-01-27T19:58:47.636195Z",
     "iopub.status.idle": "2024-01-27T19:59:32.108271Z",
     "shell.execute_reply": "2024-01-27T19:59:32.107525Z",
     "shell.execute_reply.started": "2024-01-27T19:58:47.636328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked reference to NSI: tract_id\n",
      "Linked reference to NSI: block_id\n",
      "Linked reference to NSI: bg_id\n",
      "Linked reference to NSI: zcta_id\n"
     ]
    }
   ],
   "source": [
    "# For zcta, tract, bg, and block\n",
    "# we want to do spatial joins to link\n",
    "# up fd_id in the NSI with the ref\n",
    "# We will use config data to do this\n",
    "# since other references may be brought in \n",
    "# down the line\n",
    "# We are going to store fd_id/ref_id links in a dataframe\n",
    "ref_df_list = []\n",
    "for ref_name, ref_id in REF_ID_NAMES_DICT.items():\n",
    "    if ref_name != 'county':\n",
    "        ref_filep = join(REF_DIR_I, fips, ref_name + \".gpkg\")\n",
    "    \n",
    "        # Load in the ref file\n",
    "        ref_geo = gpd.read_file(ref_filep)\n",
    "    \n",
    "        # Limit the geodataframe to our ref id and 'geometry' column\n",
    "        keep_col = [ref_id, 'geometry']\n",
    "        ref_geo_sub = ref_geo[keep_col]\n",
    "    \n",
    "        # Limit the NSI to our fd_id and geometry column\n",
    "        keep_col_nsi = ['fd_id', 'geometry']\n",
    "        nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "    \n",
    "        # Reproj nsi_sub to the reference crs\n",
    "        nsi_reproj = nsi_sub.to_crs(ref_geo.crs)\n",
    "    \n",
    "        # Do a spatial join\n",
    "        nsi_ref = gpd.sjoin(nsi_reproj, ref_geo_sub, predicate='within')\n",
    "    \n",
    "        # Set index to fd_id and just keep the ref_id\n",
    "        # Rename that column to our ref_name + '_id'\n",
    "        # Append this to our ref_df_list\n",
    "        nsi_ref_f = nsi_ref.set_index('fd_id')[[ref_id]]\n",
    "        nsi_ref_f = nsi_ref_f.rename(columns={ref_id: ref_name + '_id'})\n",
    "        ref_df_list.append(nsi_ref_f)\n",
    "    \n",
    "        # Helpful message\n",
    "        print('Linked reference to NSI: ' + ref_name + '_id')\n",
    "\n",
    "# Can concat and write\n",
    "nsi_refs = pd.concat(ref_df_list, axis=1).reset_index()\n",
    "ref_filep = join(EXP_DIR_I, fips, 'nsi_ref.pqt')\n",
    "prepare_saving(ref_filep)\n",
    "nsi_refs.to_parquet(ref_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9fc7b-5fe5-4dd9-b8ce-cae114598749",
   "metadata": {},
   "source": [
    "## Social Vulnerability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b31bda-ecff-4961-87fe-746bd767a6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:59:37.868880Z",
     "iopub.status.busy": "2024-01-27T19:59:37.868601Z",
     "iopub.status.idle": "2024-01-27T20:00:23.885532Z",
     "shell.execute_reply": "2024-01-27T20:00:23.884212Z",
     "shell.execute_reply.started": "2024-01-27T19:59:37.868858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "# Read in single family home gpkg\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, fips, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e95109d-ab46-4945-b46b-a3e37c7e2c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:20:09.082111Z",
     "iopub.status.busy": "2024-01-27T20:20:09.081434Z",
     "iopub.status.idle": "2024-01-27T20:20:15.416167Z",
     "shell.execute_reply": "2024-01-27T20:20:15.415503Z",
     "shell.execute_reply.started": "2024-01-27T20:20:09.082057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked vulnerability to NSI: lmi\n",
      "Linked vulnerability to NSI: sovi\n",
      "Linked vulnerability to NSI: cejst\n"
     ]
    }
   ],
   "source": [
    "# Read in processed sovi data\n",
    "# Loop through the community boundary data\n",
    "# Get links to the single family home data\n",
    "# Store in single dataframe\n",
    "# Write out\n",
    "\n",
    "sovi_dir = join(VULN_DIR_I, 'social', fips)\n",
    "filenames = ['lmi', 'sovi', 'cejst']\n",
    "\n",
    "sovi_df_list = []\n",
    "for fn in filenames:\n",
    "    # Read in each gpkg\n",
    "    fp = join(sovi_dir, fn + '.gpkg')\n",
    "    sovi_geo = gpd.read_file(fp)\n",
    "\n",
    "    # Subset sovi_geo based on thresholds\n",
    "    # For cejst and ovb this is already done\n",
    "    # For lmi and ovb need to do the filter as follows\n",
    "    if fn == 'lmi':\n",
    "        # See https://www.hudoig.gov/reports-publications/\n",
    "        # report/cdbg-dr-program-generally-\n",
    "        # met-low-and-moderate-income-requirements\n",
    "        # The statutory hreshold is 50%, so retain those\n",
    "        sovi_sub = sovi_geo[sovi_geo['Lowmod_pct'] > .5]\n",
    "    elif fn == 'sovi':\n",
    "        # Subset to threshhold for FMA (from 2022 NOFO)\n",
    "        sovi_sub = sovi_geo[sovi_geo['SOVI0610' + stateabbr] > .75]\n",
    "    else:\n",
    "        sovi_sub = sovi_geo\n",
    "\n",
    "    # Only need the geometry for sovi_sub\n",
    "    sovi_sub = sovi_sub[['geometry']]\n",
    "    \n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(sovi_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_sovi = gpd.sjoin(nsi_reproj, sovi_sub, predicate='within')\n",
    "\n",
    "    # Add indicator column\n",
    "    nsi_sovi[fn] = 1\n",
    "\n",
    "    # Append this to our sovi_df_list\n",
    "    sovi_df_list.append(nsi_sovi[['fd_id', fn]].set_index('fd_id'))\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked vulnerability to NSI: ' + fn)\n",
    "\n",
    "sovi_df_f = pd.concat(sovi_df_list, axis=1).fillna(0).astype(bool)\n",
    "sovi_out_filepath = join(sovi_dir, 'c_indicators.pqt')\n",
    "sovi_df_f.to_parquet(sovi_out_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSAFE",
   "language": "python",
   "name": "unsafe01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
