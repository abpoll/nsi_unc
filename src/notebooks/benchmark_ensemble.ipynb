{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6323f43-139c-4035-aa0e-5b8047e35ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T21:14:14.503285Z",
     "iopub.status.busy": "2024-01-27T21:14:14.502691Z",
     "iopub.status.idle": "2024-01-27T21:14:14.825662Z",
     "shell.execute_reply": "2024-01-27T21:14:14.824191Z",
     "shell.execute_reply.started": "2024-01-27T21:14:14.503226Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cde8c422-923e-4640-8196-af7c02795072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:27:27.550737Z",
     "iopub.status.busy": "2024-01-27T22:27:27.550060Z",
     "iopub.status.idle": "2024-01-27T22:27:28.136613Z",
     "shell.execute_reply": "2024-01-27T22:27:28.134737Z",
     "shell.execute_reply.started": "2024-01-27T22:27:27.550681Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9592e8a0-6f5e-4357-b3aa-c7dae061d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:27:28.139886Z",
     "iopub.status.busy": "2024-01-27T22:27:28.139315Z",
     "iopub.status.idle": "2024-01-27T22:27:28.181873Z",
     "shell.execute_reply": "2024-01-27T22:27:28.180039Z",
     "shell.execute_reply.started": "2024-01-27T22:27:28.139835Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're only using single county for v0.1\n",
    "fips = FIPS[0]\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day\n",
    "stateabbr = STATEABBR[0]\n",
    "nation = NATION[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb0791-4442-4b8c-bb8a-d3b37c6b0de8",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69781b7e-93bc-4b40-93e9-0c57c3a76114",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a7341949-210b-45ad-b08e-19778495cc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:27:28.184349Z",
     "iopub.status.busy": "2024-01-27T22:27:28.183824Z",
     "iopub.status.idle": "2024-01-27T22:28:15.502983Z",
     "shell.execute_reply": "2024-01-27T22:28:15.501614Z",
     "shell.execute_reply.started": "2024-01-27T22:27:28.184299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the single family homes,\n",
    "# the fd_id/reference file\n",
    "# the fd_id/depths file\n",
    "# the fd_id flood zone file\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, fips, 'nsi_sf.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, fips, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, fips, 'nsi_depths.pqt'))\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, fips, 'nsi_fz.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb178e21-542b-40eb-89b1-102535a53e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:15.505335Z",
     "iopub.status.busy": "2024-01-27T22:28:15.504821Z",
     "iopub.status.idle": "2024-01-27T22:28:15.541863Z",
     "shell.execute_reply": "2024-01-27T22:28:15.540782Z",
     "shell.execute_reply.started": "2024-01-27T22:28:15.505286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter to properties with > 0 \n",
    "depths_df = nsi_depths[nsi_depths.iloc[:,1:].sum(axis=1) > 0].set_index('fd_id')\n",
    "# Rename columns with \"depth\" key\n",
    "depths_df.columns = ['depth_' + x for x in depths_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1b09f89-42f0-47d5-9375-913ac1fc5a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:15.546291Z",
     "iopub.status.busy": "2024-01-27T22:28:15.545605Z",
     "iopub.status.idle": "2024-01-27T22:28:16.547112Z",
     "shell.execute_reply": "2024-01-27T22:28:16.546444Z",
     "shell.execute_reply.started": "2024-01-27T22:28:15.546241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "# We do need sqft for elevation cost or floodproof estimates\n",
    "\n",
    "# Normally we would only keep the below, but I'm commenting those out\n",
    "# because we also want to keep found_ht\n",
    "# keep_cols = ['fd_id', 'occtype', 'val_struct']\n",
    "keep_cols = ['fd_id', 'occtype', 'val_struct', 'bldgtype',\n",
    "             'found_type', 'found_ht', 'sqft']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# We're also going to merge in fzs\n",
    "nsi_res = nsi_res.merge(nsi_fz[['fd_id', 'fld_zone']], on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "basements = structs.str[2:]\n",
    "stories = structs.str[:2]\n",
    "\n",
    "nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "nsi_res_f = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])].set_index('fd_id')\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - note that there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data downloaded from the census webpage\n",
    "# You retain more structures if you use the block data we\n",
    "# downloaded in UNSAFE\n",
    "full_df = (nsi_res_f.join(depths_df)).reset_index()\n",
    "\n",
    "# ^ This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0eb8cf62-336a-4181-9ac5-2aa49ef6b4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.547889Z",
     "iopub.status.busy": "2024-01-27T22:28:16.547754Z",
     "iopub.status.idle": "2024-01-27T22:28:16.559254Z",
     "shell.execute_reply": "2024-01-27T22:28:16.558667Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.547877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the fld_zone column processed for the way it needs\n",
    "# to be done for using hazus ddfs\n",
    "# Get the first character of the flood zone and only retain it\n",
    "# if it's a V zone. We are going to use A zone for A and outside\n",
    "# (if any) flood zone depth exposures\n",
    "ve_zone = np.where(full_df['fld_zone'].str[0] == 'V',\n",
    "                   'V',\n",
    "                   'A')\n",
    "full_df = full_df.assign(fz_ddf = ve_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3090d2f-592d-45a3-856a-7acac192fcda",
   "metadata": {},
   "source": [
    "## Get parameters for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb27f06a-ded0-4fb8-9d85-6d9dc4f41c5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.559834Z",
     "iopub.status.busy": "2024-01-27T22:28:16.559714Z",
     "iopub.status.idle": "2024-01-27T22:28:16.604985Z",
     "shell.execute_reply": "2024-01-27T22:28:16.604387Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.559822Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are  going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from ref level which matches \n",
    "# (to the best of our ability)\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census ref again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Tract appears to have enough\n",
    "# This check is based on the subset of tracts (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "# TODO - STRUCT_REF should either be a config, \n",
    "# or something identified on-the-fly based on \n",
    "# whether we have enough observations at block or blockgroup\n",
    "# before \"rolling back\" to coarser resolution to get\n",
    "# proportions\n",
    "\n",
    "STRUCT_REF = 'tract_id'\n",
    "# Note that this approach assumes there is no correlation of\n",
    "# the foundation type proportions and number of stories\n",
    "# with features that vary within a census tract. Might not be\n",
    "# the right thing to do. That's a TODO\n",
    "struct_tot = nsi_res[nsi_res[STRUCT_REF].isin(full_df[STRUCT_REF])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b507d2d-2271-46a3-a301-fd3822123959",
   "metadata": {},
   "source": [
    "### Number of stories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12c3b6c6-24b8-44e4-9a96-05c8986f6233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.605581Z",
     "iopub.status.busy": "2024-01-27T22:28:16.605456Z",
     "iopub.status.idle": "2024-01-27T22:28:16.667226Z",
     "shell.execute_reply": "2024-01-27T22:28:16.666563Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.605569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the total number of structures w/ number of stories \n",
    "# in each block gruop\n",
    "stories_sum = struct_tot.groupby([STRUCT_REF, 'stories']).size()\n",
    "# Then get the proportion\n",
    "stories_prop = stories_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# Our parameters can be drawn from this table based on the bg_id\n",
    "# of a structure we are estimating losses for\n",
    "stories_param = stories_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "                                                 columns='stories',\n",
    "                                                 values=0).fillna(0)\n",
    "# Since it's a binomial distribution, we only need to specify\n",
    "# one param. Arbitrarily choose 1S\n",
    "# Round the param to the hundredth place\n",
    "# Store in a dict\n",
    "stories_param = stories_param['1S'].round(2)\n",
    "STRY_DICT = dict(stories_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66a8a4-7c70-40e9-bb04-7cb7bd0c5077",
   "metadata": {},
   "source": [
    "### Foundation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9c3a3014-9c93-408c-b72e-347d646dcb69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.668065Z",
     "iopub.status.busy": "2024-01-27T22:28:16.667912Z",
     "iopub.status.idle": "2024-01-27T22:28:16.690057Z",
     "shell.execute_reply": "2024-01-27T22:28:16.689424Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.668051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat procedure above\n",
    "found_sum = struct_tot.groupby([STRUCT_REF, 'found_type']).size()\n",
    "found_prop = found_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "found_param = found_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "                                             columns='found_type',\n",
    "                                             values=0).fillna(0)\n",
    "\n",
    "# We want a dictionary of bg_id to a list of B, C, S\n",
    "# for direct use in our multinomial distribution draw\n",
    "# Store params in a list (each row is bg_id and corresponds to\n",
    "# its own probabilities of each foundation type)\n",
    "params = found_param.values.round(2)\n",
    "# Then create our dictionary\n",
    "FND_DICT = dict(zip(found_param.index, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd877aa8-d075-4e5e-9cf9-bef71be397ae",
   "metadata": {},
   "source": [
    "## Load depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c62f13a-15ed-47bb-b64b-aedff3731b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.690762Z",
     "iopub.status.busy": "2024-01-27T22:28:16.690612Z",
     "iopub.status.idle": "2024-01-27T22:28:16.732194Z",
     "shell.execute_reply": "2024-01-27T22:28:16.731589Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.690747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a2ea4-7c02-4449-ad9e-a3d593f1d973",
   "metadata": {},
   "source": [
    "# Generate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8803e1fa-473d-44c0-873f-e5c927188280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.732870Z",
     "iopub.status.busy": "2024-01-27T22:28:16.732731Z",
     "iopub.status.idle": "2024-01-27T22:28:16.754589Z",
     "shell.execute_reply": "2024-01-27T22:28:16.753993Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.732856Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need a randon number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "379fb9ac-9fd3-4538-a641-41dafe6f5f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:16.755212Z",
     "iopub.status.busy": "2024-01-27T22:28:16.755075Z",
     "iopub.status.idle": "2024-01-27T22:28:18.200603Z",
     "shell.execute_reply": "2024-01-27T22:28:18.199960Z",
     "shell.execute_reply.started": "2024-01-27T22:28:16.755199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index for Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Need to create a dataframe w/ N_SOW rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "\n",
    "# We drop these for the ensemble because we don't need them\n",
    "drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "             'stories']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "print('Created Index for Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d16003-893f-456c-ac57-cfc11dbbb836",
   "metadata": {},
   "source": [
    "## Sample structure characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3227aec8-99fc-4b2b-8236-a6f92e8f4a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:18.201493Z",
     "iopub.status.busy": "2024-01-27T22:28:18.201361Z",
     "iopub.status.idle": "2024-01-27T22:28:34.207449Z",
     "shell.execute_reply": "2024-01-27T22:28:34.206664Z",
     "shell.execute_reply.started": "2024-01-27T22:28:18.201481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw values\n",
      "Draw stories\n",
      "Draw foundation type\n",
      "Generated Structure Characteristics\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "# Draw from the structure value distribution for each property\n",
    "# normal(val_struct, val_struct*CF_DET) where these are array_like\n",
    "# I also want to treat this as truncated\n",
    "# on the lower end since there is a risk of drawing impossibly\n",
    "# low numbers (like negative) with this approach\n",
    "# https://github.com/kieranrcampbell/blog-notebooks/blob/master/\n",
    "# Fast%20vectorized%20sampling%20from%20truncated%\n",
    "# 20normal%20distributions%20in%20python.ipynb\n",
    "# outlines an approach to use numpy to do a truncated sample\n",
    "# TODO move this to a util file\n",
    "def truncnorm_rvs_recursive(x, sigma, lower_clip):\n",
    "    rng = np.random.default_rng()\n",
    "    q = rng.normal(x, sigma)\n",
    "    if np.any(q < lower_clip):\n",
    "        # Adjustment to the code provided to index the sigma vector\n",
    "        q[q < lower_clip] = truncnorm_rvs_recursive(x[q < lower_clip],\n",
    "                                                    sigma[q < lower_clip],\n",
    "                                                    lower_clip)\n",
    "\n",
    "    return q\n",
    "# Using 20000 as an artificial, arbitrary lower bound on value\n",
    "values = truncnorm_rvs_recursive(ens_df['val_struct'],\n",
    "                                 ens_df['val_struct']*COEF_VARIATION,\n",
    "                                 20000)\n",
    "\n",
    "print('Draw values')\n",
    "\n",
    "# Draw from the #stories distribution\n",
    "# We do this by mapping ens_df values with STRY_DICT\n",
    "# and passing this parameter to rng.binomial()\n",
    "# We also need to create an array of 1s with length\n",
    "# N_SOW * len(full_df) - i.e. len(ens_df)\n",
    "# full_df['bg_id'].map(STRY_DICT)\n",
    "bin_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "bin_p = ens_df[STRUCT_REF].map(STRY_DICT).values\n",
    "# This gives us an array of 0s and 1s\n",
    "# Based on how STRY_DICT is defined, the probability of\n",
    "# success parameter corresponds to 1S, so we need to\n",
    "# swap out 1 with 1S and 0 with 2S\n",
    "stories = rng.binomial(bin_n, bin_p)\n",
    "stories = np.where(stories == 1,\n",
    "                   '1S',\n",
    "                   '2S')\n",
    "\n",
    "print('Draw stories')\n",
    "\n",
    "# Draw from the fnd_type distribution\n",
    "# We do the same thing as above but with\n",
    "# the FND_DICT. This is a multinomial distribution\n",
    "# and 0, 1, 2 correspond to B, C, S\n",
    "# We get an array returned of the form \n",
    "# [0, 0, 1] (if we have Slab foundation, for example)\n",
    "# so we need to transform this into the corresponding\n",
    "# foundation type array\n",
    "# Can do this with fnds[fnds[0] == 1] = 'B'\n",
    "# fnds[fnds[1]] == 1] = 'C' & fnds[fnds[2] == 1] = 'S'\n",
    "# One way to do the mapping is by treating each\n",
    "# row-array as a binary string and converting it\n",
    "# to an int\n",
    "# So you get [a, b, c] => a*2^2 + b*2^1 + c*2^0\n",
    "# This uniquely maps to 4, 2, and 1\n",
    "# So we can create a dict for 4: 'B', 2: 'C', and 1: 'S'\n",
    "# and make it a pd.Series() (I think this is useful because\n",
    "# pandas can combine this with the 1S and 2S string easily\n",
    "# into a series and we'll need to use that bld_type\n",
    "# for the other dicts we have)\n",
    "\n",
    "# This is our ens_df index aligned multinomial\n",
    "# probabilities array\n",
    "# np.stack makes sure the dtype is correct\n",
    "# Not sure why it is cast to object dtype if\n",
    "# I call .values, but this works...\n",
    "\n",
    "mult_p = np.stack(ens_df[STRUCT_REF].map(FND_DICT))\n",
    "# This is our map of binary string/int\n",
    "# conversions to the foundation type\n",
    "bin_str_map = {4: 'B', 2: 'C', 1: 'S'}\n",
    "# We need our np.ones array \n",
    "mult_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# Draw from mult_p\n",
    "fnds = rng.multinomial(mult_n, mult_p)\n",
    "# Create a series of 4, 2, and 1 from the binary strings\n",
    "# This code accomplishes the conversion outlined in the\n",
    "# note above and comes from this stackoverflow post\n",
    "# https://stackoverflow.com/questions/41069825/\n",
    "# convert-binary-01-numpy-to-integer-or-binary-string\n",
    "fnds_ints = pd.Series(fnds.dot(2**np.arange(fnds.shape[1])[::-1]))\n",
    "# Replace these values with the fnd_type\n",
    "fnd_types = fnds_ints.map(bin_str_map)\n",
    "\n",
    "print('Draw foundation type')\n",
    "\n",
    "# We take fnd_types for two tasks now\n",
    "# First, if B, it's WB type home and we\n",
    "# combine this with stories to get the bld_type\n",
    "# This is naccs_ddf_type \n",
    "# We combine bld_type with fz_ddf to get hazus_ddf_type\n",
    "# For our case study, it turns out we will use the same hazus\n",
    "# ddf for the basement houses (_A) since no V zone houses\n",
    "# For no basement, hazus_ddf_type does not add the _fz\n",
    "\n",
    "# Let's get bld_type\n",
    "# Basement type from fnd_types\n",
    "base_types = np.where(fnd_types == 'B',\n",
    "                      'WB',\n",
    "                      'NB')\n",
    "\n",
    "# Combine 1S and this\n",
    "bld_types = pd.Series(stories) + pd.Series(base_types)\n",
    "\n",
    "# In theory, bld_type is naccs_ddf_type. No need to \n",
    "# take this storage up in practice... just refer to bld_type\n",
    "# when needed\n",
    "# For WB homes, hazus_ddf_type is bld_types + '_' + ens_df['fz_ddf']\n",
    "# For NB homes, it's bld_types\n",
    "# It makes practical sense to create a new series for this\n",
    "hazus_ddf_types = np.where(base_types == 'WB',\n",
    "                           bld_types + '_' + ens_df['fz_ddf'],\n",
    "                           bld_types)\n",
    "\n",
    "\n",
    "# We are going to use the fnd_type to draw from the\n",
    "# FFE distribution\n",
    "# Need to use np.stack to get the array of floats\n",
    "tri_params = np.stack(fnd_types.map(FFE_DICT))\n",
    "\n",
    "# Can use [:] to access like a matrix and directly input to \n",
    "# rng.triangular\n",
    "# 0, 1, and 2 are column indices corresponding to left,\n",
    "# mode, and right\n",
    "# We round this to the nearest foot\n",
    "ffes = np.round(rng.triangular(tri_params[:,0],\n",
    "                               tri_params[:,1],\n",
    "                               tri_params[:,2]))\n",
    "\n",
    "print('Generated Structure Characteristics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12fc13-54ba-4ada-beab-efe44c4f40b6",
   "metadata": {},
   "source": [
    "## Estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5fb8e1af-a0ba-46a1-aad0-9acc68c04262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:34.209329Z",
     "iopub.status.busy": "2024-01-27T22:28:34.209177Z",
     "iopub.status.idle": "2024-01-27T22:28:34.672067Z",
     "shell.execute_reply": "2024-01-27T22:28:34.671385Z",
     "shell.execute_reply.started": "2024-01-27T22:28:34.209315Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we adjust depths by FFE\n",
    "# And will store this in a df\n",
    "depth_cols = [x for x in full_df.columns if 'depth' in x]\n",
    "depth_ffe_df = ens_df[depth_cols].subtract(ffes, axis=0).round(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7f54db18-f133-4fa6-97ea-a9988f735b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:28:34.672893Z",
     "iopub.status.busy": "2024-01-27T22:28:34.672748Z",
     "iopub.status.idle": "2024-01-27T22:29:47.726707Z",
     "shell.execute_reply": "2024-01-27T22:29:47.725922Z",
     "shell.execute_reply.started": "2024-01-27T22:28:34.672880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 50\n",
      "Estimate Losses for NACCS & Hazus, RP: 10\n",
      "Randomly assigned NACCS or HAZUS Loss\n",
      "Obtained Full Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Now, we are going to loop through each return period\n",
    "# and estimate losses for NACCS and HAZUS using our helper\n",
    "# functions for each of these\n",
    "\n",
    "for d_col in depth_ffe_df.columns:\n",
    "    rp = d_col.split('_')[-1]\n",
    "    naccs_loss[rp] = est_naccs_loss(pd.Series(bld_types),\n",
    "                                    depth_ffe_df[d_col],\n",
    "                                    naccs_ddfs,\n",
    "                                    NACCS_MAX_DICT)\n",
    "    hazus_loss[rp] = est_hazus_loss(pd.Series(hazus_ddf_types),\n",
    "                                    depth_ffe_df[d_col],\n",
    "                                    hazus_ddfs,\n",
    "                                    HAZUS_MAX_DICT)\n",
    "\n",
    "    print('Estimate Losses for NACCS & Hazus, RP: ' + rp)\n",
    "\n",
    "# Then, we convert these to dataframes\n",
    "hazus_df = pd.DataFrame.from_dict(hazus_loss)\n",
    "naccs_df = pd.DataFrame.from_dict(naccs_loss)\n",
    "\n",
    "# And we use a binomial rv to determine if we use the hazus or naccs\n",
    "# loss estimate for a particular SOW (across RPs right now)\n",
    "# You would do this within the loop above if you wanted\n",
    "# to change the damage function across RPs in a particular SOW\n",
    "# I think it makes more sense to say that in a particular SOW,\n",
    "# the flood-frequency relationship has a particular form\n",
    "# (like, in a bootstrapped sense), and the way this is\n",
    "# implemented now is consistent with that\n",
    "# Binomial\n",
    "random_loss = rng.binomial(1, .5, size=len(ens_df))\n",
    "\n",
    "# Get indices to take from each df\n",
    "hazus_ind = (random_loss == 0)\n",
    "naccs_ind = (random_loss == 1)\n",
    "\n",
    "# Concat subsetted dataframes\n",
    "losses_df = pd.concat([hazus_df.loc[hazus_ind],\n",
    "                       naccs_df.loc[naccs_ind]], axis=0).sort_index()\n",
    "# Rename columns to make it more clear what this is\n",
    "losses_df.columns = ['rel_dam_' + x for x in losses_df.columns]\n",
    "\n",
    "# Add a column indicating NACCS or Hazus ddf\n",
    "losses_df.loc[hazus_ind, 'ddf'] = 'HAZUS'\n",
    "losses_df.loc[naccs_ind, 'ddf'] = 'NACCS'\n",
    "\n",
    "print('Randomly assigned NACCS or HAZUS Loss')\n",
    "\n",
    "# Add clearer column names to the ffe dataframe\n",
    "depth_ffe_df.columns = ['ffe_' + x for x in depth_ffe_df.columns]\n",
    "\n",
    "# Concat for our full ensemble\n",
    "ens_df = pd.concat([ens_df, losses_df, depth_ffe_df,\n",
    "                    pd.Series(stories, name='stories'),\n",
    "                    pd.Series(fnd_types, name='fnd_type'),\n",
    "                    pd.Series(ffes, name='ffe'),\n",
    "                    pd.Series(values, name='val_s')],\n",
    "                   axis=1)\n",
    "\n",
    "\n",
    "# Get relative damage columns\n",
    "rel_cols = [x for x in ens_df.columns if 'rel_dam' in x]\n",
    "# For each relative damage column, scale by val_s, the structure\n",
    "# value realization\n",
    "for col in rel_cols:\n",
    "    rp = col.split('_')[-1]\n",
    "    ens_df['loss_' + rp] = ens_df[col].values*ens_df['val_s'].values\n",
    "\n",
    "print('Obtained Full Ensemble')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "127ec8ac-2366-4239-8437-dd25c29349a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:29:47.727652Z",
     "iopub.status.busy": "2024-01-27T22:29:47.727501Z",
     "iopub.status.idle": "2024-01-27T22:29:51.750317Z",
     "shell.execute_reply": "2024-01-27T22:29:51.749635Z",
     "shell.execute_reply.started": "2024-01-27T22:29:47.727638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated EAL\n",
      "Generated loss ensemble\n"
     ]
    }
   ],
   "source": [
    "# Now we calculate EAL\n",
    "# We will use trapezoidal approximation for this\n",
    "# Using trapezoid method and adding bin of lowest probability\n",
    "# events to obtain expected annual \n",
    "\n",
    "# We want a list of return periods sorted from most frequent\n",
    "# to least frequent\n",
    "# We can do this by removing 'depth_' from our depth_ffe_df columns\n",
    "# and then sorting that list in ascending order\n",
    "# From this, we can define a probability equivalent list\n",
    "# and then a list indexing loss columns as well\n",
    "rp_list = sorted([int(x.split('_')[-1]) for x in depth_ffe_df.columns])\n",
    "p_rp_list = [round(1/int(x), 4) for x in rp_list]\n",
    "loss_list = ['loss_' + str(x) for x in rp_list]\n",
    "\n",
    "# Then we create an empty series\n",
    "eal = pd.Series(index=ens_df.index).fillna(0)\n",
    "\n",
    "# We loop through our loss list and apply the \n",
    "# trapezoidal approximation\n",
    "# We need these to be sorted from most frequent\n",
    "# to least frequent\n",
    "for i in range(len(loss_list) - 1):\n",
    "    loss1 = ens_df[loss_list[i]]\n",
    "    loss2 = ens_df[loss_list[i+1]]\n",
    "    rp1 = p_rp_list[i]\n",
    "    rp2 = p_rp_list[i+1]\n",
    "    eal += (loss1 + loss2)*(rp1-rp2)/2\n",
    "final = eal + ens_df[loss_list[-1]]*p_rp_list[-1]\n",
    "\n",
    "# This is the final trapezoid to add in\n",
    "final_eal = eal + ens_df[loss_list[-1]]*p_rp_list[-1]\n",
    "print('Calculated EAL')\n",
    "# Add eal columns to our dataframe\n",
    "ens_df = pd.concat([ens_df, pd.Series(final_eal, name='eal')],\n",
    "                   axis=1)\n",
    "\n",
    "# Let's also get the SOW index - start at 0\n",
    "sow_ind = np.arange(len(ens_df))%N_SOW\n",
    "ens_df = pd.concat([ens_df, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "print('Generated loss ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19c3b262-a0a4-4fe7-9c6d-fe7afc1de113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:33:03.363423Z",
     "iopub.status.busy": "2024-01-27T22:33:03.362821Z",
     "iopub.status.idle": "2024-01-27T22:33:13.057221Z",
     "shell.execute_reply": "2024-01-27T22:33:13.056266Z",
     "shell.execute_reply.started": "2024-01-27T22:33:03.363371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out our ensemble dfs\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "prepare_saving(ens_out_filep)\n",
    "ens_df.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384495-c9cc-4979-8c4f-450456f6e7c0",
   "metadata": {},
   "source": [
    "# Generate losses without uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8bd6e2fb-07df-4c4d-9422-3d59956d7085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T22:29:51.762325Z",
     "iopub.status.busy": "2024-01-27T22:29:51.762204Z",
     "iopub.status.idle": "2024-01-27T22:29:51.956844Z",
     "shell.execute_reply": "2024-01-27T22:29:51.956205Z",
     "shell.execute_reply.started": "2024-01-27T22:29:51.762313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate Losses for Hazus Default, RP: 500\n",
      "Estimate Losses for Hazus Default, RP: 100\n",
      "Estimate Losses for Hazus Default, RP: 50\n",
      "Estimate Losses for Hazus Default, RP: 10\n"
     ]
    }
   ],
   "source": [
    "# We are taking the full_df dataframe values as-is\n",
    "# to get eal estimates for our benchmark\n",
    "# From occtype, get 1SNB, etc. and combine with fz_ddf\n",
    "# for our ddf_id column\n",
    "b_types = full_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "hazus_ddf_types = pd.Series(np.where(b_types.str[-2:] == 'WB',\n",
    "                                     b_types + '_' + full_df['fz_ddf'],\n",
    "                                     b_types))\n",
    "\n",
    "# We need to get the depth columns translated into ffe adjusted\n",
    "# exposure depths\n",
    "# We can subset on depths_df to get a depth dataframe\n",
    "# and then we can subtract it by the found_ht column\n",
    "# for our adjustment\n",
    "# Merge to get indices aligned\n",
    "depth_cols = [x for x in full_df.columns if 'depth' in x]\n",
    "depth_ffe_df = full_df[depth_cols].subtract(full_df['found_ht'], axis=0).round(1)\n",
    "\n",
    "# Loop through return periods for losses\n",
    "hazus_def_loss = pd.DataFrame()\n",
    "for d_col in depth_ffe_df.columns:\n",
    "    rp = d_col.split('_')[-1]\n",
    "    hazus_def_loss[rp] = est_hazus_loss_nounc(hazus_ddf_types,\n",
    "                                              depth_ffe_df[d_col],\n",
    "                                              hazus_nounc,\n",
    "                                              HAZUS_MAX_NOUNC_DICT)\n",
    "\n",
    "    print('Estimate Losses for Hazus Default, RP: ' + rp)\n",
    "\n",
    "# Same processing as before for column names\n",
    "hazus_def_loss.columns = ['rel_dam_' + x for x in hazus_def_loss.columns]\n",
    "# Adjust depth_ffe_df columns\n",
    "depth_ffe_df.columns = ['ffe_' + x for x in depth_ffe_df.columns]\n",
    "\n",
    "# Join dataframes\n",
    "hazus_def = pd.concat([full_df, hazus_def_loss, depth_ffe_df],\n",
    "                       axis=1)\n",
    "\n",
    "# Get losses from rel_dam columns\n",
    "for col in hazus_def_loss.columns:\n",
    "    rp = col.split('_')[-1]\n",
    "    hazus_def['loss_' + rp] = hazus_def[col]*hazus_def['val_struct']\n",
    "\n",
    "# We want a list of return periods sorted from most frequent\n",
    "# to least frequent\n",
    "# We can do this by removing 'depth_' from our depth_ffe_df columns\n",
    "# and then sorting that list in ascending order\n",
    "# From this, we can define a probability equivalent list\n",
    "# and then a list indexing loss columns as well\n",
    "rp_list = sorted([int(x.split('_')[-1]) for x in depth_ffe_df.columns])\n",
    "p_rp_list = [round(1/int(x), 4) for x in rp_list]\n",
    "loss_list = ['loss_' + str(x) for x in rp_list]\n",
    "\n",
    "# Then we create an empty series\n",
    "eal = pd.Series(index=full_df.index).fillna(0)\n",
    "\n",
    "# We loop through our loss list and apply the \n",
    "# trapezoidal approximation\n",
    "for i in range(len(loss_list) - 1):\n",
    "    loss1 = hazus_def[loss_list[i]]\n",
    "    loss2 = hazus_def[loss_list[i+1]]\n",
    "    rp1 = p_rp_list[i]\n",
    "    rp2 = p_rp_list[i+1]\n",
    "    # We add each approximation\n",
    "    eal += (loss1 + loss2)*(rp1-rp2)/2\n",
    "# This is the final trapezoid to add in\n",
    "final_eal = eal + hazus_def[loss_list[-1]]*p_rp_list[-1]\n",
    "\n",
    "# Add eal column to our dataframe\n",
    "hazus_def = pd.concat([hazus_def, pd.Series(final_eal, name='eal')], axis=1)\n",
    "\n",
    "# Write out the default damages df\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "prepare_saving(hazus_def_out_filep)\n",
    "hazus_def.to_parquet(hazus_def_out_filep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSAFE",
   "language": "python",
   "name": "unsafe01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
