{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77317845-f6de-4685-9632-c0a45b09e356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:38:19.166818Z",
     "iopub.status.busy": "2024-01-27T20:38:19.166291Z",
     "iopub.status.idle": "2024-01-27T20:38:19.208839Z",
     "shell.execute_reply": "2024-01-27T20:38:19.207403Z",
     "shell.execute_reply.started": "2024-01-27T20:38:19.166765Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825aa305-25d8-4e1a-95e2-0cf0633c840e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:38:19.212225Z",
     "iopub.status.busy": "2024-01-27T20:38:19.211693Z",
     "iopub.status.idle": "2024-01-27T20:38:20.798511Z",
     "shell.execute_reply": "2024-01-27T20:38:20.796971Z",
     "shell.execute_reply.started": "2024-01-27T20:38:19.212174Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfe07f5-ef82-4fb0-9054-e239ea8b66ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:38:20.799967Z",
     "iopub.status.busy": "2024-01-27T20:38:20.799617Z",
     "iopub.status.idle": "2024-01-27T20:38:20.818408Z",
     "shell.execute_reply": "2024-01-27T20:38:20.817183Z",
     "shell.execute_reply.started": "2024-01-27T20:38:20.799942Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're only using single county for v0.1\n",
    "fips = FIPS[0]\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day\n",
    "stateabbr = STATEABBR[0]\n",
    "nation = NATION[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66af98-957f-46f7-b38f-62732a81f005",
   "metadata": {},
   "source": [
    "# Link NSI with depth grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9608b241-dde5-4845-ac0e-e27b2e2b02e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:38:20.823054Z",
     "iopub.status.busy": "2024-01-27T20:38:20.822830Z",
     "iopub.status.idle": "2024-01-27T20:39:07.245407Z",
     "shell.execute_reply": "2024-01-27T20:39:07.244895Z",
     "shell.execute_reply.started": "2024-01-27T20:38:20.823033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproject other files to the hazard CRS because\n",
    "# this is the data we want to maintain spatial accuracy with the most\n",
    "# I might want to clip this to the GC clip boundary since it can\n",
    "# potentially speed up some code for doing \n",
    "# point in raster, etc. \n",
    "# For my first pass linking up, I also want to include\n",
    "# the 5th and 95th percentile grids and just use\n",
    "# a heuristic approach for estimating the standard deviation\n",
    "# for a normal distribution\n",
    "# Get this standard deviation parameter and then use the median\n",
    "# value as the mean \n",
    "# That's all we get from the link NSI with hazard step...\n",
    "# Then in the ensemble merge step, we sample from\n",
    "# the spatially varying distribution across all RPs\n",
    "\n",
    "# To start, let's reproject the NSI to the HAZ_CRS\n",
    "# Then prepare the coordinates for point in raster checks\n",
    "nsi = gpd.read_file(join(EXP_DIR_I, fips, 'nsi_sf.gpkg'))\n",
    "nsi_reproj = nsi.to_crs(HAZ_CRS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97cfceb5-c2cf-4df7-b88f-7887374bb6fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:39:07.246226Z",
     "iopub.status.busy": "2024-01-27T20:39:07.246076Z",
     "iopub.status.idle": "2024-01-27T20:39:07.592009Z",
     "shell.execute_reply": "2024-01-27T20:39:07.591667Z",
     "shell.execute_reply.started": "2024-01-27T20:39:07.246211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store NSI coordinates in list\n"
     ]
    }
   ],
   "source": [
    "# For each depth grid, we will sample from the grid\n",
    "# by way of a list of coordinates from the reprojected\n",
    "# nsi geodataframe (this is the fastest way I know to do it)\n",
    "coords = zip(nsi_reproj['geometry'].x, nsi_reproj['geometry'].y)\n",
    "coord_list = [(x, y) for x, y in coords]\n",
    "print('Store NSI coordinates in list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fc49a2-d40d-4e11-80f7-27dc900b1b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:39:07.592660Z",
     "iopub.status.busy": "2024-01-27T20:39:07.592517Z",
     "iopub.status.idle": "2024-01-27T20:40:14.003848Z",
     "shell.execute_reply": "2024-01-27T20:40:14.002724Z",
     "shell.execute_reply.started": "2024-01-27T20:39:07.592647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 0_2 RP depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 01 RP depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 02 RP depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 10 RP depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll store series of fd_id/depth pairs for each rp_pctile\n",
    "# in a list and concat this into a df after iterating\n",
    "depth_list = []\n",
    "\n",
    "# Dictionary to store the depth grids\n",
    "dg_dict = {}\n",
    "\n",
    "# Loop through RPs\n",
    "for rp in RET_PERS:\n",
    "    dg = read_dg(rp)\n",
    "    print('Read in ' + rp + ' RP depth grid')\n",
    "\n",
    "    # Sample from the depth grid based on structure locations\n",
    "    # I did some ground truthing in qgis\n",
    "    # It appears that the sampled values align correctly\n",
    "    sampled_depths = [x[0] for x in dg.sample(coord_list)]\n",
    "    print('Sampled depths from grid')\n",
    "\n",
    "    # Store the series \n",
    "    depths = pd.Series(sampled_depths,\n",
    "                       index=nsi_reproj['fd_id'],\n",
    "                       name=rp)\n",
    "    # Add the series to the list of series\n",
    "    depth_list.append(depths)\n",
    "    print('Added depths to list\\n')\n",
    "\n",
    "# Concat to dataframe\n",
    "depth_df = pd.concat(depth_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958441a6-a52d-46dc-9eef-c0e9d3be7fbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:40:14.004752Z",
     "iopub.status.busy": "2024-01-27T20:40:14.004622Z",
     "iopub.status.idle": "2024-01-27T20:40:14.252040Z",
     "shell.execute_reply": "2024-01-27T20:40:14.251313Z",
     "shell.execute_reply.started": "2024-01-27T20:40:14.004740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace nodata values with 0\n",
    "depth_df[depth_df == dg.nodata] = 0\n",
    "\n",
    "# Retain only structures with some flood exposure\n",
    "depth_df_f = depth_df[depth_df.sum(axis=1) > 0]\n",
    "\n",
    "# Multiply by MTR_TO_FT to convert to feet\n",
    "depth_df_f = depth_df_f*MTR_TO_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a0f8950-2aac-42a4-ab93-1c379e8eea0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:44:40.492445Z",
     "iopub.status.busy": "2024-01-27T20:44:40.491874Z",
     "iopub.status.idle": "2024-01-27T20:44:40.525488Z",
     "shell.execute_reply": "2024-01-27T20:44:40.523750Z",
     "shell.execute_reply.started": "2024-01-27T20:44:40.492395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's rejigger the columns so that they reflect the \n",
    "# return period, not the annual exceedance probability\n",
    "ncol = [str(round(100/float(x.replace('_', '.')))) for x in depth_df_f.columns]\n",
    "depth_df_f.columns = ncol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02758042-38cc-4828-ad5c-320e01319efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:44:58.279751Z",
     "iopub.status.busy": "2024-01-27T20:44:58.279156Z",
     "iopub.status.idle": "2024-01-27T20:44:58.384372Z",
     "shell.execute_reply": "2024-01-27T20:44:58.382871Z",
     "shell.execute_reply.started": "2024-01-27T20:44:58.279699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out dataframe that links fd_id to depths\n",
    "# with columns corresponding to ret_per (i.e. 500, 100, 50, 10)\n",
    "nsi_depths_out = join(EXP_DIR_I, fips, 'nsi_depths.pqt')\n",
    "# Round to nearest hundredth foot\n",
    "# Depth-damage functions don't have nearly the precision\n",
    "# to make use of inches differences, but some precision\n",
    "# is needed for subtracting first floor elevation before rounding\n",
    "depth_df_f.round(2).reset_index().to_parquet(nsi_depths_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSAFE",
   "language": "python",
   "name": "unsafe01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
