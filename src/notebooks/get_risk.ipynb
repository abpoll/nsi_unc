{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed264f1c-60c1-49bd-af7b-ca42fd88e194",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedcdf6b-f5df-4b72-b285-3313df884f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T17:38:23.495434Z",
     "iopub.status.busy": "2024-03-22T17:38:23.494846Z",
     "iopub.status.idle": "2024-03-22T17:38:23.567270Z",
     "shell.execute_reply": "2024-03-22T17:38:23.565873Z",
     "shell.execute_reply.started": "2024-03-22T17:38:23.495377Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cbc7fd-af6d-4800-86c4-4ea008bf85d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T18:10:16.905777Z",
     "iopub.status.busy": "2024-03-22T18:10:16.905548Z",
     "iopub.status.idle": "2024-03-22T18:10:16.924432Z",
     "shell.execute_reply": "2024-03-22T18:10:16.923064Z",
     "shell.execute_reply.started": "2024-03-22T18:10:16.905759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from util.download import *\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.unzip import *\n",
    "from util.exp import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58884488-133d-4dd2-9d44-6fba2ac1526a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T17:38:26.248906Z",
     "iopub.status.busy": "2024-03-22T17:38:26.248145Z",
     "iopub.status.idle": "2024-03-22T17:38:26.279842Z",
     "shell.execute_reply": "2024-03-22T17:38:26.278328Z",
     "shell.execute_reply.started": "2024-03-22T17:38:26.248859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name the fips, statefips, stateabbr, and nation that\n",
    "# we are using for this analysis\n",
    "# We pass these in as a list even though the framework currently\n",
    "# processes a single county so that it can facilitate that\n",
    "# expansion in the future\n",
    "# TODO - could make sense to define these in the future\n",
    "# in json or other formats instead of as input in code\n",
    "fips_args = {\n",
    "    'FIPS': ['42101'], \n",
    "    'STATEFIPS': ['42'],\n",
    "    'STATEABBR': ['PA'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bf35-5d86-45c8-9869-617e4709f46a",
   "metadata": {},
   "source": [
    "# Download (and unzip) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d1cd00d-1c06-43ac-ade6-dc949e99d8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T19:57:38.327182Z",
     "iopub.status.busy": "2024-03-20T19:57:38.326805Z",
     "iopub.status.idle": "2024-03-20T19:59:13.554175Z",
     "shell.execute_reply": "2024-03-20T19:59:13.552560Z",
     "shell.execute_reply.started": "2024-03-20T19:57:38.327150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded from: https://nsi.sec.usace.army.mil/nsiapi/structures?fips=42101\n",
      "Downloaded from: https://www2.census.gov/geo/tiger/TIGER2022/TRACT/tl_2022_42_tract.zip\n",
      "Downloaded from: https://www2.census.gov/geo/tiger/TIGER2022/BG/tl_2022_42_bg.zip\n",
      "Downloaded from: https://www2.census.gov/geo/tiger/TIGER2022/TABBLOCK20/tl_2022_42_tabblock20.zip\n",
      "Downloaded from: https://static-data-screeningtool.geoplatform.gov/data-versions/1.0/data/score/downloadable/1.0-communities.csv\n",
      "Downloaded from: https://svi.cdc.gov/Documents/Data/2020/csv/states/SVI_2020_US.csv\n",
      "Downloaded from: https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\n",
      "Downloaded from: https://www2.census.gov/geo/tiger/TIGER2022/ZCTA520/tl_2022_us_zcta520.zip\n"
     ]
    }
   ],
   "source": [
    "# The util.const library provides us with\n",
    "# convenient functions for quickly downloading data from the sources\n",
    "# we specified in the config.yaml\n",
    "\n",
    "# URL_WILDCARDS has entries like {FIPS} which we want to replace\n",
    "# with the county code that is in a URL for downloading. \n",
    "# We create a dictionary of these mappings from our fips_args\n",
    "# dictionary. This is what we need to use the download_raw()\n",
    "# function \n",
    "\n",
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "download_raw(DOWNLOAD, wcard_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd95446c-adaa-4252-b94c-4adbe1e75a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T20:05:39.106242Z",
     "iopub.status.busy": "2024-03-20T20:05:39.105640Z",
     "iopub.status.idle": "2024-03-20T20:06:16.076386Z",
     "shell.execute_reply": "2024-03-20T20:06:16.075240Z",
     "shell.execute_reply.started": "2024-03-20T20:05:39.106187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped: noaa\n",
      "Unzipped: 10027236\n",
      "Unzipped: county\n",
      "Unzipped: zcta\n",
      "Unzipped: block\n",
      "Unzipped: bg\n",
      "Unzipped: tract\n",
      "Unzipped: dg\n",
      "Unzipped: nfhl\n"
     ]
    }
   ],
   "source": [
    "# We call unzip_raw from util.unzip\n",
    "unzip_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b9de-7fea-4474-ae90-47127e366661",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d3cf-adf4-4b5e-80e7-2db7873641cb",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014c5353-2508-4708-b3bf-b3f624fe9350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T17:38:30.523759Z",
     "iopub.status.busy": "2024-03-22T17:38:30.523227Z",
     "iopub.status.idle": "2024-03-22T17:39:24.231580Z",
     "shell.execute_reply": "2024-03-22T17:39:24.230739Z",
     "shell.execute_reply.started": "2024-03-22T17:38:30.523714Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this case study, we want single family houses from the\n",
    "# national structure inventory. We will call functions from exp.py\n",
    "# that takes the raw nsi data and converts it to a gdf\n",
    "# Then we will grab our properties of interest using the RES1\n",
    "# code for the 'occtype' variable. In addition, this case study\n",
    "# will look at properties <= 2 stories because these are\n",
    "# the properties we can represent structural uncertainty in\n",
    "# depth-damage relationships for\n",
    "\n",
    "nsi_gdf = get_nsi_geo(FIPS)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "nsi_sub = get_struct_subset(nsi_gdf,\n",
    "                            filter=sub_string,\n",
    "                            occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_sub.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c70cd6-09e4-4f32-a4ff-3a4851e30ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:26:57.263500Z",
     "iopub.status.busy": "2024-03-20T21:26:57.262895Z",
     "iopub.status.idle": "2024-03-20T21:27:38.485053Z",
     "shell.execute_reply": "2024-03-20T21:27:38.484223Z",
     "shell.execute_reply.started": "2024-03-20T21:26:57.263447Z"
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e55b1fc-9f4c-47b2-8daa-72613b672b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:38:30.142874Z",
     "iopub.status.busy": "2024-03-20T21:38:30.142248Z",
     "iopub.status.idle": "2024-03-20T21:40:42.280717Z",
     "shell.execute_reply": "2024-03-20T21:40:42.279693Z",
     "shell.execute_reply.started": "2024-03-20T21:38:30.142823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ref: block\n",
      "Saved Ref: bg\n",
      "Saved Ref: tract\n",
      "Saved Ref: county\n",
      "Saved Ref: zcta\n"
     ]
    }
   ],
   "source": [
    "# We are going to clip reference data to a clip file that\n",
    "# represents our study boundaries. In this case, it's the county\n",
    "# of Philadelphia, so we will prepare that as our clip file\n",
    "county_filep = join(REF_DIR_UZ, NATION, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == FIPS]\n",
    "\n",
    "# clip_ref_files will go through all unzipped ref files,\n",
    "# clip them in the clip file geometry, and write them\n",
    "clip_ref_files(clip_gdf, FIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eaafa-7a8f-469b-a64b-1737d3af4b04",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c538bf-ee9f-485c-a7f3-df85862107ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:58:21.783827Z",
     "iopub.status.busy": "2024-03-20T21:58:21.783468Z",
     "iopub.status.idle": "2024-03-20T21:58:22.055721Z",
     "shell.execute_reply": "2024-03-20T21:58:22.054002Z",
     "shell.execute_reply.started": "2024-03-20T21:58:21.783798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NACCS DDFs Processed\n",
      "HAZUS DDFs processed\n"
     ]
    }
   ],
   "source": [
    "# For NACCS DDFs, we are just going to call process_naccs\n",
    "# For HAZUS DDFs, we are going to call process_hazus but also\n",
    "# specify how to define the uncertainty around these point estimate\n",
    "# DDFs\n",
    "# In general, the functions could be expanded to allow the user to\n",
    "# specify which building types to consider, but right now\n",
    "# that is baked-in to the implementation in unsafe \n",
    "# Both of these functions will write out all the data you need\n",
    "# for estimating losses later on\n",
    "# We break it out into two scripts because not all analyses\n",
    "# will want to represent deep uncertainty in DDFs and will\n",
    "# only call one of the process functions\n",
    "\n",
    "process_naccs()\n",
    "\n",
    "# .3 was used in Zarekarizi et al. 2020\n",
    "# https://www.nature.com/articles/s41467-020-19188-9\n",
    "# and we are going to use that for this case study\n",
    "UNIF_UNC = .3\n",
    "process_hazus(unif_unc=UNIF_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb7e13-17c6-4877-b7af-45ea26eaf385",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ad1b1b6-c7fb-4639-9d79-0f61f21bbaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T22:15:32.820905Z",
     "iopub.status.busy": "2024-03-20T22:15:32.820307Z",
     "iopub.status.idle": "2024-03-20T22:16:05.394001Z",
     "shell.execute_reply": "2024-03-20T22:16:05.393339Z",
     "shell.execute_reply.started": "2024-03-20T22:15:32.820857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jumbo/keller-lab/projects/icom/nsi_unc/src/util/exp.py:125: DtypeWarning: Columns (18,26,70,72,85,131) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed cejst\n",
      "Processed CDC SVI\n",
      "Processed low-mod income\n"
     ]
    }
   ],
   "source": [
    "# Process national social vulnerability data\n",
    "# Tell the function which datasets we want processed\n",
    "# In this case study, we will use cejst, svi, and lmi\n",
    "# LMI was downloaded manually (not from the download code above)\n",
    "# because of the way it is hosted at its url\n",
    "\n",
    "sovi_list = ['cejst', 'svi', 'lmi']\n",
    "process_national_sovi(sovi_list, FIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3425-ba65-489d-8edf-f9c0ae862a67",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ee5dc31-081a-4c17-b5b5-004dce20ea20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T22:04:14.848620Z",
     "iopub.status.busy": "2024-03-20T22:04:14.848023Z",
     "iopub.status.idle": "2024-03-20T22:04:18.815849Z",
     "shell.execute_reply": "2024-03-20T22:04:18.815174Z",
     "shell.execute_reply.started": "2024-03-20T22:04:14.848570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote NFHL for county\n"
     ]
    }
   ],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "process_nfhl(FIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2f93-c107-436b-ac1e-3648b162b64e",
   "metadata": {},
   "source": [
    "## Link flood zones and references to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd28ca6-90bb-431a-bab6-af1953ce8e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T17:56:19.145062Z",
     "iopub.status.busy": "2024-03-22T17:56:19.144484Z",
     "iopub.status.idle": "2024-03-22T17:57:17.016054Z",
     "shell.execute_reply": "2024-03-22T17:57:17.015277Z",
     "shell.execute_reply.started": "2024-03-22T17:56:19.145011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote out: fz\n",
      "Linked reference to NSI: tract_id\n",
      "Linked reference to NSI: block_id\n",
      "Linked reference to NSI: bg_id\n",
      "Linked reference to NSI: zcta_id\n"
     ]
    }
   ],
   "source": [
    "# Link flood zones\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently for other\n",
    "# case studies\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "get_spatial_var(nsi_sub,\n",
    "                nfhl,\n",
    "                'fz',\n",
    "                FIPS,\n",
    "                keep_cols)\n",
    "\n",
    "# Link references\n",
    "# This will do spatial joins for structures within\n",
    "# all the reference spatial files (besides county)\n",
    "# and output a file of fd_id (these are unique strucutre ids)\n",
    "# linked to all of the reference ids\n",
    "get_ref_ids(nsi_sub, FIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a394-ad51-4bde-9589-07f9259d1adb",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2250956b-b979-4122-8695-dd9a5dde870f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T18:21:16.748100Z",
     "iopub.status.busy": "2024-03-22T18:21:16.747913Z",
     "iopub.status.idle": "2024-03-22T18:22:16.792205Z",
     "shell.execute_reply": "2024-03-22T18:22:16.790181Z",
     "shell.execute_reply.started": "2024-03-22T18:21:16.748086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store NSI coordinates in list\n",
      "Read in 0_2 depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 01 depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 02 depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Read in 10 depth grid\n",
      "Sampled depths from grid\n",
      "Added depths to list\n",
      "\n",
      "Wrote depth dataframe\n"
     ]
    }
   ],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "depth_df = get_inundations(nsi_sub, FIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a614-1687-4963-a28a-ba00d423485c",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915863-55ed-4e8d-9a70-6b673105e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate fro mpreparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "full_df = get_base_df(FIPS)\n",
    "\n",
    "# Generate SOWs based on this dataframe. The function gives\n",
    "# users the option to specify what to treat as uncertain. It could\n",
    "# be improved to give the user more customization on the \"how\" part\n",
    "# We pass in tract_id to specify in this case study that\n",
    "# we will draw from basement and stories distributions defined\n",
    "# at the tract level\n",
    "# We specify hazus & naccs for the ddfs we want losses estimated\n",
    "# under\n",
    "# We specify val, stories, basement, and ffe as the features\n",
    "# we want to represent with uncertainty\n",
    "generate_ensemble(full_df,\n",
    "                  'tract_id',\n",
    "                  ['hazus', 'naccs'],\n",
    "                  ['val', 'stories', 'basement', 'ffe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969aa-0b7c-4078-9e6d-37b2d5b8cbb0",
   "metadata": {},
   "source": [
    "# Estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11d23-3c34-472a-a36f-6f06d94daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We estimate losses for the full ensemble. For now, when deep \n",
    "# uncertainty is specified in the DDF (i.e. you \n",
    "# want to get damages with HAZUS and NACCS) they are estimated on\n",
    "# the same SOWs and that's returned. No synthesis of \n",
    "# deep unceratinties in UNSAFE yet. \n",
    "\n",
    "# We also want benchmark estimates without uncertainty \n",
    "# which we can do with the full_df specified above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSAFE",
   "language": "python",
   "name": "unsafe01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
