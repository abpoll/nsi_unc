{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed264f1c-60c1-49bd-af7b-ca42fd88e194",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedcdf6b-f5df-4b72-b285-3313df884f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cbc7fd-af6d-4800-86c4-4ea008bf85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd\n",
    "\n",
    "from unsafe.download import *\n",
    "from unsafe.files import *\n",
    "from unsafe.const import *\n",
    "from unsafe.unzip import *\n",
    "from unsafe.exp import *\n",
    "from unsafe.ddfs import *\n",
    "from unsafe.ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58884488-133d-4dd2-9d44-6fba2ac1526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the fips, statefips, stateabbr, and nation that\n",
    "# we are using for this analysis\n",
    "# We pass these in as a list even though the framework currently\n",
    "# processes a single county so that it can facilitate that\n",
    "# expansion in the future\n",
    "# TODO - could make sense to define these in the future\n",
    "# in json or other formats instead of as input in code\n",
    "fips_args = {\n",
    "    'FIPS': ['42101'], \n",
    "    'STATEFIPS': ['42'],\n",
    "    'STATEABBR': ['PA'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad060e58-0489-4bf4-8e24-6728dd15c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass in a config file that sets up\n",
    "# constants and the structure for downlading data\n",
    "# For the directory structure of our case study, \n",
    "# we use the following \n",
    "ABS_DIR = Path().absolute().parents[0]\n",
    "\n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# Wildcards for urls\n",
    "URL_WILDCARDS = CONFIG['url_wildcards']\n",
    "\n",
    "# Get the file extensions for api endpoints\n",
    "API_EXT = CONFIG['api_ext']\n",
    "\n",
    "# Get the CRS constants\n",
    "NSI_CRS = CONFIG['nsi_crs']\n",
    "\n",
    "# Dictionary of ref_names\n",
    "REF_NAMES_DICT = CONFIG['ref_names']\n",
    "\n",
    "# Dictionary of ref_id_names\n",
    "REF_ID_NAMES_DICT = CONFIG['ref_id_names']\n",
    "\n",
    "# Coefficient of variation\n",
    "# for structure values\n",
    "COEF_VARIATION = CONFIG['coef_var']\n",
    "\n",
    "# First floor elevation dictionary\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Number of states of the world\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# Get hazard model variables\n",
    "# Get Return Period list\n",
    "RET_PERS = CONFIG['RPs']\n",
    "HAZ_FILEN = CONFIG['haz_filename']\n",
    "# Get CRS for depth grids\n",
    "HAZ_CRS = CONFIG['haz_crs']\n",
    "\n",
    "# Get the files we need downloaded\n",
    "# These are specified in the \"download\" key \n",
    "# in the config file\n",
    "# We transpose because one of the utils\n",
    "# needs to return a list of the output files\n",
    "# TODO the logic here works for one county, but \n",
    "# will need rethinking for generalizability\n",
    "# I think the way to do it would be to break it\n",
    "# into DOWNLOAD_FIPS, DOWNLOAD_STATE, etc.\n",
    "# and these are accessed differently\n",
    "DOWNLOAD = pd.json_normalize(CONFIG['download'], sep='_').T\n",
    "\n",
    "# We can also specify the filepath to the\n",
    "# raw data directory\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# Figures directory\n",
    "FIG_DIR = join(ABS_DIR, \"figures\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bf35-5d86-45c8-9869-617e4709f46a",
   "metadata": {},
   "source": [
    "# Download (and unzip) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cd00d-1c06-43ac-ade6-dc949e99d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The util.const library provides us with\n",
    "# convenient functions for quickly downloading data from the sources\n",
    "# we specified in the config.yaml\n",
    "\n",
    "# URL_WILDCARDS has entries like {FIPS} which we want to replace\n",
    "# with the county code that is in a URL for downloading. \n",
    "# We create a dictionary of these mappings from our fips_args\n",
    "# dictionary. This is what we need to use the download_raw()\n",
    "# function \n",
    "\n",
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "download_raw(DOWNLOAD, wcard_dict,\n",
    "             FR, API_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95446c-adaa-4252-b94c-4adbe1e75a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call unzip_raw from util.unzip\n",
    "# This will unzip files we downloaded, but also \n",
    "# .zip files that we uploaded to raw/external/\n",
    "unzip_raw(FR, UNZIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b9de-7fea-4474-ae90-47127e366661",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d3cf-adf4-4b5e-80e7-2db7873641cb",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c5353-2508-4708-b3bf-b3f624fe9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case study, we want single family houses from the\n",
    "# national structure inventory. We will call functions from exp.py\n",
    "# that takes the raw nsi data and converts it to a gdf\n",
    "# Then we will grab our properties of interest using the RES1\n",
    "# code for the 'occtype' variable. In addition, this case study\n",
    "# will look at properties <= 2 stories because these are\n",
    "# the properties we can represent structural uncertainty in\n",
    "# depth-damage relationships for\n",
    "\n",
    "nsi_gdf = get_nsi_geo(FIPS, NSI_CRS, EXP_DIR_R)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "nsi_sub = get_struct_subset(nsi_gdf,\n",
    "                            filter=sub_string,\n",
    "                            occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_sub.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c70cd6-09e4-4f32-a4ff-3a4851e30ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:26:57.263500Z",
     "iopub.status.busy": "2024-03-20T21:26:57.262895Z",
     "iopub.status.idle": "2024-03-20T21:27:38.485053Z",
     "shell.execute_reply": "2024-03-20T21:27:38.484223Z",
     "shell.execute_reply.started": "2024-03-20T21:26:57.263447Z"
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b1fc-9f4c-47b2-8daa-72613b672b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to clip reference data to a clip file that\n",
    "# represents our study boundaries. In this case, it's the county\n",
    "# of Philadelphia, so we will prepare that as our clip file\n",
    "county_filep = join(REF_DIR_UZ, NATION, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == FIPS]\n",
    "\n",
    "# clip_ref_files will go through all unzipped ref files,\n",
    "# clip them in the clip file geometry, and write them\n",
    "clip_ref_files(clip_gdf, FIPS,\n",
    "               REF_DIR_UZ, REF_DIR_I, REF_NAMES_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eaafa-7a8f-469b-a64b-1737d3af4b04",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c538bf-ee9f-485c-a7f3-df85862107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NACCS DDFs, we are just going to call process_naccs\n",
    "# For HAZUS DDFs, we are going to call process_hazus but also\n",
    "# specify how to define the uncertainty around these point estimate\n",
    "# DDFs\n",
    "# In general, the functions could be expanded to allow the user to\n",
    "# specify which building types to consider, but right now\n",
    "# that is baked-in to the implementation in unsafe \n",
    "# Both of these functions will write out all the data you need\n",
    "# for estimating losses later on\n",
    "# We break it out into two scripts because not all analyses\n",
    "# will want to represent deep uncertainty in DDFs and will\n",
    "# only call one of the process functions\n",
    "\n",
    "process_naccs(VULN_DIR_UZ, VULN_DIR_I)\n",
    "\n",
    "# .3 was used in Zarekarizi et al. 2020\n",
    "# https://www.nature.com/articles/s41467-020-19188-9\n",
    "# and we are going to use that for this case study\n",
    "UNIF_UNC = .3\n",
    "process_hazus(VULN_DIR_UZ, VULN_DIR_I, unif_unc=UNIF_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb7e13-17c6-4877-b7af-45ea26eaf385",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b1b6-c7fb-4639-9d79-0f61f21bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national social vulnerability data\n",
    "# Tell the function which datasets we want processed\n",
    "# In this case study, we will use cejst and svi\n",
    "# which are available nationally\n",
    "\n",
    "sovi_list = ['cejst', 'svi']\n",
    "process_national_sovi(sovi_list, FIPS,\n",
    "                      VULN_DIR_R, REF_DIR_I, VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3425-ba65-489d-8edf-f9c0ae862a67",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5dc31-081a-4c17-b5b5-004dce20ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "process_nfhl(FIPS,\n",
    "             POL_DIR_UZ,\n",
    "             POL_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2f93-c107-436b-ac1e-3648b162b64e",
   "metadata": {},
   "source": [
    "## Link flood zones and references to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28ca6-90bb-431a-bab6-af1953ce8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link flood zones\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently for other\n",
    "# case studies\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "get_spatial_var(nsi_sub,\n",
    "                nfhl,\n",
    "                'fz',\n",
    "                FIPS,\n",
    "                EXP_DIR_I,\n",
    "                keep_cols)\n",
    "\n",
    "# Link references\n",
    "# This will do spatial joins for structures within\n",
    "# all the reference spatial files (besides county)\n",
    "# and output a file of fd_id (these are unique strucutre ids)\n",
    "# linked to all of the reference ids\n",
    "get_ref_ids(nsi_sub, FIPS,\n",
    "            REF_ID_NAMES_DICT, REF_DIR_I, EXP_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a394-ad51-4bde-9589-07f9259d1adb",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250956b-b979-4122-8695-dd9a5dde870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "depth_df = get_inundations(nsi_sub, FIPS,\n",
    "                           HAZ_CRS, RET_PERS, EXP_DIR_I,\n",
    "                           HAZ_DIR_UZ, HAZ_FILEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a614-1687-4963-a28a-ba00d423485c",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915863-55ed-4e8d-9a70-6b673105e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate fro mpreparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "\n",
    "base_df = get_base_df(FIPS, EXP_DIR_I)\n",
    "\n",
    "# Generate SOWs based on this dataframe. The function gives\n",
    "# users the option to specify what to treat as uncertain. It could\n",
    "# be improved to give the user more customization on the \"how\" part\n",
    "# We pass in tract_id to specify in this case study that\n",
    "# we will draw from basement and stories distributions defined\n",
    "# at the tract level\n",
    "# We specify hazus & naccs for the ddfs we want losses estimated\n",
    "# under\n",
    "# We specify val, stories, and basement as the features\n",
    "# we want to represent with uncertainty\n",
    "# If you generate an ensemble, you are at least considering\n",
    "# ffe uncertainty from the FFE_DICT\n",
    "# We estimate losses for the full ensemble. For now, when deep \n",
    "# uncertainty is specified in the DDF (i.e. you \n",
    "# want to get damages with HAZUS and NACCS) they are estimated on\n",
    "# the same SOWs and that's returned. No synthesis of \n",
    "# deep unceratinties in UNSAFE yet. \n",
    "ens_df_f = generate_ensemble(nsi_sub,\n",
    "                             base_df,\n",
    "                             ['hazus', 'naccs'],\n",
    "                             ['val_struct', 'stories', 'basement'],\n",
    "                             N_SOW,\n",
    "                             FFE_DICT,\n",
    "                             COEF_VARIATION,\n",
    "                             VULN_DIR_I)\n",
    "\n",
    "# Save dataframes\n",
    "base_out_filep = join(FO, 'base_df.pqt')\n",
    "prepare_saving(base_out_filep)\n",
    "base_df.to_parquet(base_out_filep)\n",
    "\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "ens_df_f.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969aa-0b7c-4078-9e6d-37b2d5b8cbb0",
   "metadata": {},
   "source": [
    "# Estimate benchmark losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11d23-3c34-472a-a36f-6f06d94daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want benchmark estimates without uncertainty \n",
    "# which we can do with the full_df specified above\n",
    "nounc_df = benchmark_loss(base_df, VULN_DIR_I)\n",
    "\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "prepare_saving(hazus_def_out_filep)\n",
    "nounc_df.to_parquet(hazus_def_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cfd06-8977-466d-9191-39eab0af37dd",
   "metadata": {},
   "source": [
    "# Quick comparison of estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fda306-01c2-403f-92a1-d1961c76314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "temp = ens_df_f.groupby(['sow_ind'])[['naccs_eal', 'hazus_eal']].sum()\n",
    "temp['naccs_eal'].hist(bins=50, color='blue', alpha=.5, label='NACCS')\n",
    "temp['hazus_eal'].hist(bins=50, color='orange', alpha=.5, label='HAZUS')\n",
    "ax.axvline(nounc_df['eal'].sum(), color='red', label='No Uncertainty')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0918c-5feb-41c9-80fb-103f3e027c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSAFE",
   "language": "python",
   "name": "unsafe01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
