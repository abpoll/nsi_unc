{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d781b0",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "This notebook serves to demonstrate the following workflow to help introduce new users to the UNSAFE framework, and to allow them to test that the code does what it says it does. \n",
    "\n",
    "1) Configure the working directory structure and workflow parameters;\n",
    "2) If you want to make use of the downloading functionality in UNSAFE, download and unzip data;\n",
    "3) Subset the full structure inventory to single-family structures and convert the data to a GeoDataFrame;\n",
    "4) Specify a spatial extent of your study area (i.e. county shapefile or a study boundary) and process reference files through clipping (e.g. census tract data);\n",
    "5) Process expert DDFs for use in ensembles;\n",
    "6) Process social vulnerability data by linking it with corresponding reference data (e.g. linking Climate and Economic Justice Screening Tool with census tracts);\n",
    "7) Prepare the National Flood Hazard Layer data for identifying structure location in and outside of the federal floodplain;\n",
    "8) Link structures to all vector spatial data;\n",
    "9) Link structures to inundation data, provided as raster(s);\n",
    "10) Prepare the structure inventory for loss estimation (this base inventory can be used for estimating losses without uncertainty);\n",
    "11) Generate an ensemble of plausible structure realizations based on several parameters users can specify;\n",
    "12) Estimate expected annual losses for each ensemble member for a set of design events. Each ensemble member has a unique draw from the DDF distribution. Users can also estimate expected annual losses without uncertainty in exposure and vulnerability. \n",
    "\n",
    "\n",
    "We also provide code for producing some visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed264f1c-60c1-49bd-af7b-ca42fd88e194",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdf6b-f5df-4b72-b285-3313df884f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are options we like setting for working with jupyter notebooks. Totally optional\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c741b",
   "metadata": {},
   "source": [
    "We load packages (mostly from UNSAFE) to help us get through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc7fd-af6d-4800-86c4-4ea008bf85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd\n",
    "\n",
    "from unsafe.download import *\n",
    "from unsafe.files import *\n",
    "from unsafe.const import *\n",
    "from unsafe.unzip import *\n",
    "from unsafe.exp import *\n",
    "from unsafe.ddfs import *\n",
    "from unsafe.ensemble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bd764",
   "metadata": {},
   "source": [
    "Next, we configure spatial units like county fips. Right now, the county is the unit of analysis for UNSAFE. The county fips code is used to download data from the national structure inventory API, and for organzing the directory structure. This facilitates distributed county-level analyses (though we have not tested this yet, and there are likely some things to work out before then). We also keep track of the state fips code (the first two digits of the county code) for downloading data and organizing the file directory. The state abbrevation for a county can be helpful (though it's not currently used). We also have the \"Nation\" indicator, since damage functions have been produced in other countries. Keeping track of nation is probably not a feature that will remain in UNSAFE in future versions. In future versions, we plan to have state fips and state abbrevation inferred from the user-supplied FIPS code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58884488-133d-4dd2-9d44-6fba2ac1526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the fips, statefips, stateabbr, and nation that\n",
    "# we are using for this analysis\n",
    "# We pass these in as a list even though the framework currently\n",
    "# processes a single county so that it can facilitate that\n",
    "# expansion in the future\n",
    "# TODO - could make sense to define these in the future\n",
    "# in json or other formats instead of as input in code\n",
    "fips_args = {\n",
    "    'FIPS': ['42101'], \n",
    "    'STATEFIPS': ['42'],\n",
    "    'STATEABBR': ['PA'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e042cfc",
   "metadata": {},
   "source": [
    "Most of the configuration for UNSAFE is about setting up the file directory structure. Users identify the absolute directory to the root of their project directory and store this in ABS_DIR. This is where we recommend storing a config.yaml file in a `congig/` directory. Below, we show all the file directory configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad060e58-0489-4bf4-8e24-6728dd15c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass in a config file that sets up\n",
    "# constants and the structure for downlading data\n",
    "# For the directory structure of our case study, \n",
    "# we use the following, but you may\n",
    "# have to specify a different Path\n",
    "ABS_DIR = Path().absolute().parents[0]\n",
    "\n",
    "\n",
    "# We are updating the config filename because for this example\n",
    "# we are using different hazard data. The HAZ_FILEN parameter\n",
    "# will change values. Everything else is the same. \n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config_partial.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# We can also specify the filepath to the\n",
    "# raw data directory\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ef389",
   "metadata": {},
   "source": [
    "Next, we provide detailed comments on the other things in the config.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f936e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files we need downloaded\n",
    "# These are specified in the \"download\" key \n",
    "# in the config file\n",
    "# We transpose because one of the utils\n",
    "# needs to return a list of the output files\n",
    "DOWNLOAD = pd.json_normalize(CONFIG['download'], sep='_').T\n",
    "\n",
    "# Wildcards for urls. For example, {FIPS} is an element in this\n",
    "# list because in the urls supplied in the download dictionary, \n",
    "# this is what we are going to replace with the FIPS code\n",
    "# for our analysis. We don't assume that the wildcard is the\n",
    "# only text in between brackets in a url (it happens to be the\n",
    "# case for this case study) so we think it's useful to have\n",
    "# this list pre-configured. \n",
    "URL_WILDCARDS = CONFIG['url_wildcards']\n",
    "\n",
    "# Get the file extensions for api endpoints\n",
    "# In our case study, this is only for downloading from the NSI\n",
    "API_EXT = CONFIG['api_ext']\n",
    "\n",
    "# The data from the NSI is in .json format\n",
    "# and we couldn't find the coordinate reference system\n",
    "# through GET requests (we may have missed how). However,\n",
    "# we were able to find metadata online that indicates \n",
    "# the CRS. \n",
    "NSI_CRS = CONFIG['nsi_crs']\n",
    "\n",
    "# Dictionary of ref_names\n",
    "# When we download tracts, block groups, etc. \n",
    "# from the TIGER endpoints, they can sometimes have a lot of\n",
    "# characters. We find it helpful to standardize the\n",
    "# names (i.e. block instead of tabblock20), but this\n",
    "# can be customized to your preferences. \n",
    "REF_NAMES_DICT = CONFIG['ref_names']\n",
    "\n",
    "# Dictionary of ref_id_names\n",
    "# We will run the same processes on all the reference data\n",
    "# like reprojecting and clipping to our study boundaries.\n",
    "# We also will merge tabular data that is designed at\n",
    "# certain administrative boundaries with attribute\n",
    "# joins. This dictionary converts the names like GEOID\n",
    "# to tract_id. \n",
    "REF_ID_NAMES_DICT = CONFIG['ref_id_names']\n",
    "\n",
    "# Coefficient of variation\n",
    "# for structure values\n",
    "# This is what we scale the structure value by\n",
    "# to get the standard deviation we draw from\n",
    "COEF_VARIATION = CONFIG['coef_var']\n",
    "\n",
    "# First floor elevation dictionary\n",
    "# This maps foundation types to the triangular distributions\n",
    "# for first-floor elevation\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Number of states of the world\n",
    "# This is the number of ensemble members\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# The hazard configuration will vary on a case study by\n",
    "# case study basis. There may be different configuration\n",
    "# parameters that you need to specify depending on how\n",
    "# the hazard data you're using is structured. We hope\n",
    "# to provide techniques to systematically accommodate \n",
    "# certain inundation model outputs as we gain more experience \n",
    "# coupling UNSAFE with different models\n",
    "# The below structure can work on any depth grids from the FEMA\n",
    "# Flood Risk Database. We specify this case study\n",
    "# to focus on the riverine flooding products and for the\n",
    "# 500, 100, 50, and 10 year return periods. \n",
    "\n",
    "# Get hazard model variables\n",
    "# Get Return Period list\n",
    "RET_PERS = CONFIG['RPs']\n",
    "HAZ_FILEN = CONFIG['haz_filename']\n",
    "# Get CRS for depth grids\n",
    "HAZ_CRS = CONFIG['haz_crs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bf35-5d86-45c8-9869-617e4709f46a",
   "metadata": {},
   "source": [
    "# Download (and unzip) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59c4b5",
   "metadata": {},
   "source": [
    "We added download functionality to UNSAFE because it can  help make an analysis more reproducible. Instead of manually downloading hazard, exposure, and vulnerability data from different sources, much of (and sometimes all of) the data needed for a standard risk assessment can be specified using URL or API endpoints. \n",
    "\n",
    "We download data with `download_raw(files, wcard_dict, fr, api_ext)` and then unzip data -- both downloaded and user uploaded -- using `unzip_raw(fr, unzip_dir)`. In this partial data example, some of the data is manually uploaded in the `philadelphia_frd/data/raw/external/` directory. When you download UNSAFE, it comes with `external/haz/dg_clip.zip` and `external/vuln/ddfs.zip`. The first directory corresponds to the hazard data we will use for the case study, which originally comes from the Federal Emergency Managemeng Agency (FEMA) Philadelphia Flood Risk Database. You can manually download the data we used by going to [the FEMA Flood Map Service Center: Search All Products](https://msc.fema.gov/portal/advanceSearch#searchresultsanchor) and searching for \"FRD_02040202_PA_GeoTIFFs\" in the Product ID search box. This data is over 2 GB. You can use this data to follow the `full_data_example.ipynb` notebook. We clipped this raw data to a small spatial extent to allow for the partial example shown here. The second directory, `external/vuln/ddfs.zip`, corresponds to the Depth-Damage Functions, downloaded from [here](https://zenodo.org/records/10027236). This Zenodo repository is well-documented and includes a detailed explanation of where the records are originally from. We manually download these and store them in the data directory for the case study because the URL endpoints for most FEMA Flood Map Service Center products are difficult to identify, and the process for accessing the Zenodo API is somewhat complex. \n",
    "\n",
    "The `files` object (specified in CONFIG['DOWNLOAD']) should be structured in a nested dictionary in the following format: SPATIAL_UNIT -> ENDPOINT_TYPE -> COMPONENT -> FILENAME: ENDPOINT. As an example, to download the structures for a county from the NSI, you would specify `FIPS: api: exp: nsi: \"https://nsi.sec.usace.army.mil/nsiapi/structures?fips={FIPS}\"`. The `wcard_dict` is specified in our configuration step above, and will convert {FIPS} to our county code. \"api\" tells the function that we are downloading data from an api endpoint (we could also specify url), and exp tells the function that this is an exposure dataset and should be organized in the working directory accordingly. Other \"COMPONENT\"s include vuln for vulnerability, haz for hazard, pol for policy (e.g. National Flood Hazard Layer), and ref for reference (e.g. censust tract boundaries). We clarify when vulnerability refers to social or physical vulnerability with an additional nested dictionary, like COMPONENT -> SUBCOMPONENT -> FILENAME: ENDPOINT. \n",
    "\n",
    "Some endpoints need to be manually specified (e.g. the URL for the National Flood Hazard Layer) because we do not know how to uniquely identify the corresponding data (and the FEMA Help Desk did not respond to our emails asking for guidance). We believe it is a more reproducible practice to specify the download links when possible, as opposed to manually downloading files and uploading them to the working directory. However, users do not have to use the download functionality in UNSAFE. Users could treat all data as external to the UNSAFE workflow, and would configure the working environment accordingly. We recommend putting this data in `ABS_DIR/data/raw/external/` with subdirectories for a specific COMPONENT, but you can modify the filepaths defined above if you'd like to put this data somewhere else. \n",
    "\n",
    "`fr` corresponds to the relative filepath to the raw data directory. UNSAFE currently gives users the options to customize the directory structure in the configuration step, but enforces the nested directory structure within that which separates data by SPATIAL_UNIT and COMPONENT. We find that the raw, interim, and results structure provided in this tutorial is helpful for many types of risk assessments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cd00d-1c06-43ac-ade6-dc949e99d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unsafe.download library provides us with\n",
    "# convenient functions for quickly downloading data from the sources\n",
    "# we specified in the config.yaml\n",
    "\n",
    "# URL_WILDCARDS has entries like {FIPS} which we want to replace\n",
    "# with the county code that is in a URL for downloading. \n",
    "# We create a dictionary of these mappings from our fips_args\n",
    "# dictionary. This is what we need to use the download_raw()\n",
    "# function \n",
    "\n",
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "download_raw(DOWNLOAD, wcard_dict,\n",
    "             FR, API_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95446c-adaa-4252-b94c-4adbe1e75a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call unzip_raw from util.unzip\n",
    "# This will unzip files we downloaded, but also \n",
    "# .zip files that we uploaded to raw/external/\n",
    "# or wherever you put these files. We use glob.glob\n",
    "# to search for .zip directories in raw/, so as\n",
    "# long as you put external data somewhere in raw/\n",
    "# this should work. That said, we have only\n",
    "# tested this function when the directory\n",
    "# is structured as we do in this example.\n",
    "unzip_raw(FR, UNZIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71388923",
   "metadata": {},
   "source": [
    "Both of the functions we just called do a lot of work under the hood for us, calling helper functions in `unsafe.download` and `unsafe.unzip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b9de-7fea-4474-ae90-47127e366661",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc920086",
   "metadata": {},
   "source": [
    "This section comprises steps 3 through 9 mentioned at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d3cf-adf4-4b5e-80e7-2db7873641cb",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d61d1c",
   "metadata": {},
   "source": [
    "Now that we have downloaded the NSI records for the county, we are going to do some preprocessing for generating our ensemble.\n",
    "\n",
    "First, we will call `get_nsi_geo()` to use the spatial coordinates from the NSI and prepare a GeoDataFrame. We need the data in this format for linking other attributes needed for loss estimation with the NSI, like flood depths. \n",
    "\n",
    "Next, we subset the records from the NSI using SQL-like querying on our pandas DataFrame. \n",
    "\n",
    "These steps give us a GeoDataFrame of single family residences for our ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c5353-2508-4708-b3bf-b3f624fe9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case study, we want single family houses from the\n",
    "# national structure inventory. We will call functions from exp.py\n",
    "# that takes the raw nsi data and converts it to a gdf\n",
    "# Then we will grab our properties of interest using the RES1\n",
    "# code for the 'occtype' variable. In addition, this case study\n",
    "# will look at properties <= 2 stories because these are\n",
    "# the properties we can represent structural uncertainty in\n",
    "# depth-damage relationships for\n",
    "\n",
    "# This function asks us to pass in the FIPS code,\n",
    "# the coordinate reference system for the structure inventory\n",
    "# and the subdirectory where we put the raw structure inventory\n",
    "# data. We defined all of these arguments in our configuration\n",
    "# steps above. \n",
    "nsi_gdf = get_nsi_geo(FIPS, NSI_CRS, EXP_DIR_R)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "# We specify the subset step outside of the UNSAFE framework \n",
    "# but right now UNSAFE is only able to estimate losses\n",
    "# for these structures. \n",
    "# Down the line, UNSAFE will handle all occupancy types\n",
    "# and building features that HAZUS does, so the subset\n",
    "# step will only be called in analyses that are focused\n",
    "# on certain segments of the building stock. \n",
    "nsi_sub = get_struct_subset(nsi_gdf,\n",
    "                            filter=sub_string,\n",
    "                            occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses that we just\n",
    "# subset. You could write out the nsi_gdf GeoDataFrame\n",
    "# if you'd like to. \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_sub.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c70cd6-09e4-4f32-a4ff-3a4851e30ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:26:57.263500Z",
     "iopub.status.busy": "2024-03-20T21:26:57.262895Z",
     "iopub.status.idle": "2024-03-20T21:27:38.485053Z",
     "shell.execute_reply": "2024-03-20T21:27:38.484223Z",
     "shell.execute_reply.started": "2024-03-20T21:26:57.263447Z"
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451c49d",
   "metadata": {},
   "source": [
    "We process geospatial reference data (like census tracts and block groups) for three reasons:\n",
    "1) We will use census tracts to define the unit of aggregation for specifying multinomial distributions for foundation type and number of stories; and\n",
    "2) Data on social vulnerability, like the Climate and Economic Justice Screening Tool, are available as tabular data so we will link these to the corresponding geospatial reference dataset; and\n",
    "3) It can be useful to estimate losses at the property-level and then aggregate these estimates to different spatial scales. This is also a common use-case. \n",
    "\n",
    "We first define a clip_gdf GeoDataFrame reference to define our study's spatial extent. Here, we use the whole county, but users could upload a specific spatial boundary if they'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b1fc-9f4c-47b2-8daa-72613b672b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to clip reference data to a clip file that\n",
    "# represents our study boundaries. In this case, it's the county\n",
    "# of Philadelphia, so we will prepare that as our clip file\n",
    "county_filep = join(REF_DIR_UZ, NATION, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == FIPS]\n",
    "\n",
    "# clip_ref_files will go through all unzipped ref files,\n",
    "# clip them in the clip file geometry, and write them\n",
    "# We pass in arguments we defined in the configuration step\n",
    "# to tell the function where the data can be found (REF_DIR_UZ),\n",
    "# where is is going (REF_DIR_I), and how to update id names\n",
    "# (REF_NAMES_DICT). \n",
    "clip_ref_files(clip_gdf, FIPS,\n",
    "               REF_DIR_UZ, REF_DIR_I, REF_NAMES_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eaafa-7a8f-469b-a64b-1737d3af4b04",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507b11c",
   "metadata": {},
   "source": [
    "This is where we call functions to prepare our DDFs for estimating losses under uncertainty. In some analyses, you may only want to use one set of DDFs, so you only have to call one process_DDF() function. In the future, there may be many more functions to draw from, so we decided the workflow would be most transparent if users had to explicitly call a function to process a particular set of DDFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c538bf-ee9f-485c-a7f3-df85862107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NACCS DDFs, we are just going to call process_naccs\n",
    "# For HAZUS DDFs, we are going to call process_hazus but also\n",
    "# specify how to define the uncertainty around these point estimate\n",
    "# DDFs\n",
    "# In general, the functions could be expanded to allow the user to\n",
    "# specify which building types to consider, but right now\n",
    "# that is baked-in to the implementation in unsafe \n",
    "# Both of these functions will write out all the data you need\n",
    "# for estimating losses later on\n",
    "# We break it out into two functions because not all analyses\n",
    "# will want to represent deep uncertainty in DDFs and will\n",
    "# only call one of the process functions\n",
    "\n",
    "process_naccs(VULN_DIR_UZ, VULN_DIR_I)\n",
    "\n",
    "# .3 was used in Zarekarizi et al. 2020\n",
    "# https://www.nature.com/articles/s41467-020-19188-9\n",
    "# and we are going to use that for this case study\n",
    "# We define this here, instead of in the configuration step, \n",
    "# because seems helpful to define optional arguments, and arguments\n",
    "# that can be thought of as uncertain parameters, (like this one)\n",
    "# closer to the function call. \n",
    "UNIF_UNC = .3\n",
    "process_hazus(VULN_DIR_UZ, VULN_DIR_I, unif_unc=UNIF_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb7e13-17c6-4877-b7af-45ea26eaf385",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da74e1",
   "metadata": {},
   "source": [
    "UNSAFE can ingest CDC Socially Vulnerable Index data and Climate and Economic Justice and Screening Tool data. Users do not have to process data about social vulnerability, but it is a common use case. As such, we developed UNSAFE with preliminary functionality for this type of usage. The function we call below calls subroutines that have code to handle the unique features of the different data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b1b6-c7fb-4639-9d79-0f61f21bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national social vulnerability data\n",
    "# Tell the function which datasets we want processed\n",
    "# In this case study, we will use cejst and svi\n",
    "# which are available nationally and we specified in\n",
    "# our DOWNLOAD configuration dictionary. \n",
    "\n",
    "sovi_list = ['cejst', 'svi']\n",
    "process_national_sovi(sovi_list, FIPS,\n",
    "                      VULN_DIR_R, REF_DIR_I, VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3425-ba65-489d-8edf-f9c0ae862a67",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa129d27",
   "metadata": {},
   "source": [
    "The National Flood Hazard Layer is required for using HAZUS DDFs because these apply different DDF relationships based on whether a house is in the V zone. In our case study, all houses are in the A or X zone, but this is a core function that is needed for other case studies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5dc31-081a-4c17-b5b5-004dce20ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "process_nfhl(FIPS,\n",
    "             POL_DIR_UZ,\n",
    "             POL_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2f93-c107-436b-ac1e-3648b162b64e",
   "metadata": {},
   "source": [
    "## Link flood zones and references to structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699379b0",
   "metadata": {},
   "source": [
    "We will now link our GeoDataFrame of single family structures to the other spatial data we processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28ca6-90bb-431a-bab6-af1953ce8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link flood zones\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently for other\n",
    "# case studies\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "\n",
    "# This function is designed to work in general\n",
    "# for any spatial-based data you want to link\n",
    "# to structures. We demonstrate it with the\n",
    "# nfhl data. \n",
    "get_spatial_var(nsi_sub,\n",
    "                nfhl,\n",
    "                'fz',\n",
    "                FIPS,\n",
    "                EXP_DIR_I,\n",
    "                keep_cols)\n",
    "\n",
    "# Link references\n",
    "# This will do spatial joins for structures within\n",
    "# all the reference spatial files (besides county)\n",
    "# and output a file of fd_id (these are unique strucutre ids)\n",
    "# linked to all of the reference ids\n",
    "get_ref_ids(nsi_sub, FIPS,\n",
    "            REF_ID_NAMES_DICT, REF_DIR_I, EXP_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a394-ad51-4bde-9589-07f9259d1adb",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3f6f8",
   "metadata": {},
   "source": [
    "This is where we link flood depths to structures. We are using the FEMA Flood Risk Database which consists of the 500, 100, 50, and 10 year return periods. We specified in our configuration file that we are only interested in the depth grids corresponding to inland flooding. The purpose of this function is to do point-in-raster sampling for the depth grids indicated by the RET_PERS and HAZ_FILN arguments. Users can add pre-processing code that is unique to the hazard data they are bringing into the UNSAFE framework to call the `get_inundations()` function. For example, if you want to try experimenting with the `full_data_example.ipynb` you can try adding pre-processing code that selects the max inundation across the inland and coastal depth grids in the Flood Risk Database, and then modify the HAZ_FILEN argument to link these depths to the structures. Or, you can modify the RET_PERS argument to serve more generally as a flood scenario indicator and simultaneously modify the HAZ_FILN argument to accommodate reading in the correct files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250956b-b979-4122-8695-dd9a5dde870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "\n",
    "# This code looks different than the full data example\n",
    "# because we created the zip directory in a different way\n",
    "# than the zip directory you download from the fema\n",
    "# risk database\n",
    "haz_dir_uz_clip = join(HAZ_DIR_UZ, 'dg_clipped')\n",
    "depth_df = get_inundations(nsi_sub,\n",
    "                           HAZ_CRS, RET_PERS,\n",
    "                           haz_dir_uz_clip, HAZ_FILEN)\n",
    "\n",
    "# Because we are processing design-event based flood scenarios,\n",
    "# we can provide more helpful column names to prepare our loss\n",
    "# and expected annual loss calculations. We will also\n",
    "# write out our dataframe. \n",
    "ncol = [str(round(100/float(x.replace('_', '.')))) for x in depth_df.columns]\n",
    "depth_df.columns = ncol\n",
    "\n",
    "# Write out dataframe that links fd_id to depths\n",
    "# with columns corresponding to ret_per (i.e. 500, 100, 50, 10)\n",
    "# in our case study\n",
    "nsi_depths_out = join(EXP_DIR_I, FIPS, 'nsi_depths.pqt')\n",
    "prepare_saving(nsi_depths_out)\n",
    "depth_df.reset_index().to_parquet(nsi_depths_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a614-1687-4963-a28a-ba00d423485c",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093730c",
   "metadata": {},
   "source": [
    "This corresponds to steps 10-12 in the workflow. We include example code for estimating losses without uncertainty on the base dataframe that is used for generating the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915863-55ed-4e8d-9a70-6b673105e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate from preparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "\n",
    "base_df = get_base_df(FIPS, EXP_DIR_I)\n",
    "\n",
    "# Generate SOWs based on this dataframe. The function gives\n",
    "# users the option to specify what to treat as uncertain. It could\n",
    "# be improved to give the user more customization on the \"how\" part\n",
    "# Right now, we require that FFE are represented as uncertain, so\n",
    "# users don't have the option to toggle this like they do\n",
    "# with value, stories, and basement. \n",
    "# We specify hazus & naccs for the ddfs we want losses estimated\n",
    "# under\n",
    "# We specify val, stories, and basement as the features\n",
    "# we want to represent with uncertainty\n",
    "# If you generate an ensemble, you are at least considering\n",
    "# ffe uncertainty from the FFE_DICT\n",
    "# We estimate losses for the full ensemble. For now, when deep \n",
    "# uncertainty is specified in the DDF (i.e. you \n",
    "# want to get damages with HAZUS and NACCS) they are estimated on\n",
    "# the same SOWs and that's returned. No synthesis of \n",
    "# deep unceratinties in UNSAFE yet. \n",
    "ens_df_f = generate_ensemble(nsi_sub,\n",
    "                             base_df,\n",
    "                             ['hazus', 'naccs'],\n",
    "                             ['ffe', 'val_struct', 'stories', 'basement'],\n",
    "                             N_SOW,\n",
    "                             FFE_DICT,\n",
    "                             COEF_VARIATION,\n",
    "                             VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a12813",
   "metadata": {},
   "source": [
    "Because the flood events we used are defined as return periods, we can estimate expected annual loss. We do subsetting the loss columns and calling the `get_eal()` function on an ordered list from most to least frequent return period (e.g., 10 year event is ordered before the 500 year event). It is easiest to do this by looping through the damage functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eals = {}\n",
    "for ddf in ['hazus', 'naccs']:\n",
    "    col_sub = [x for x in ens_df_f if ddf + '_loss' in x]\n",
    "    loss_sub = ens_df_f[col_sub]\n",
    "    # Update column name to remove reference to ddf\n",
    "    loss_sub.columns = ['_'.join(x.split('_')[1:]) for x in col_sub]\n",
    "    ret_per_ints = [int(x.split('_')[-1]) for x in col_sub]\n",
    "    rp_list = sorted(ret_per_ints)\n",
    "    eals[ddf] = get_eal(loss_sub, rp_list)\n",
    "eals = pd.DataFrame.from_dict(eals)\n",
    "eals.columns = [x + '_eal' for x in eals.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_df_out = pd.concat([ens_df_f, eals], axis=1)\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "ens_df_out.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969aa-0b7c-4078-9e6d-37b2d5b8cbb0",
   "metadata": {},
   "source": [
    "## Estimate benchmark losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11d23-3c34-472a-a36f-6f06d94daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want benchmark estimates without uncertainty \n",
    "# which we can do with the full_df specified above\n",
    "nounc_df = benchmark_loss(base_df, VULN_DIR_I)\n",
    "\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "prepare_saving(hazus_def_out_filep)\n",
    "nounc_df.to_parquet(hazus_def_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cfd06-8977-466d-9191-39eab0af37dd",
   "metadata": {},
   "source": [
    "# Checking aggregate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb7cfe",
   "metadata": {},
   "source": [
    "You can run the code below to compare the histograms of the expected annual loss estimates for both damage functions. You will not get the exact same result each time you run this notebook, but you should get something very similar! You can compare your output to the picture in `examples/phil_frd_partial/fig/example_output.png` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fda306-01c2-403f-92a1-d1961c76314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=300,\n",
    "                       nrows=2,\n",
    "                       sharex=True,\n",
    "                       gridspec_kw={'hspace': 0},\n",
    "                       height_ratios=[1,3])\n",
    "# When we groupby 'sow_ind' we are looking across ensemble members\n",
    "temp = ens_df_out.groupby(['sow_ind'])[['naccs_eal', 'hazus_eal']].sum()/1e6\n",
    "temp['naccs_eal'].hist(bins=30, color='blue', alpha=.5, label='NACCS DDFs')\n",
    "temp['hazus_eal'].hist(bins=30, color='orange', alpha=.5, label='HAZUS DDFs')\n",
    "ax[1].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[0].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Total Expected Annual Loss ($ Millions)', size=14)\n",
    "ax[1].set_ylabel('Number of Ensemble Members', size=14)\n",
    "ax[1].tick_params(labelsize=12)\n",
    "\n",
    "temp.columns = ['NACCS', 'HAZUS']\n",
    "temp_box = temp.melt(value_name='eal',\n",
    "                     var_name='DDF Type')\n",
    "sns.boxplot(ax=ax[0],\n",
    "            data=temp_box,\n",
    "            x='eal',\n",
    "            hue='DDF Type',\n",
    "            legend=False,\n",
    "            showmeans=True,\n",
    "            meanprops={'markerfacecolor': 'firebrick',\n",
    "                        'markeredgecolor': 'black',\n",
    "                        'marker': 'D'})\n",
    "ax[0].axis('off')\n",
    "\n",
    "\n",
    "# Easier to do a custom legend\n",
    "legend_elements = [Patch(facecolor=sns.color_palette(\"tab10\")[1],\n",
    "                         label='HAZUS DDFs'),\n",
    "                    Patch(facecolor=sns.color_palette(\"tab10\")[0],\n",
    "                         label='NACCS DDFs'),\n",
    "                   Line2D([0], [0], color='r', lw=2, label='No Uncertainty'),\n",
    "                   Line2D([0], [0], marker='D', markerfacecolor='firebrick',\n",
    "                          label='Ensemble Mean',\n",
    "                          ls='',\n",
    "                          markeredgecolor='black', markersize=8),]\n",
    "\n",
    "\n",
    "ax[1].legend(handles=legend_elements,\n",
    "             loc='upper right',\n",
    "             fontsize='x-large',\n",
    "             frameon=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsafe01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
