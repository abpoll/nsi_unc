{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b92c51-9735-4b45-b57b-a0986e5957f0",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5d60bb-bc9a-407f-b003-3b9e05c4b0d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:09:32.052563Z",
     "iopub.status.busy": "2023-10-12T19:09:32.052040Z",
     "iopub.status.idle": "2023-10-12T19:09:32.092408Z",
     "shell.execute_reply": "2023-10-12T19:09:32.090947Z",
     "shell.execute_reply.started": "2023-10-12T19:09:32.052514Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af0c782-b731-4a87-aefa-2c8ac68cedbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:09:32.413595Z",
     "iopub.status.busy": "2023-10-12T19:09:32.413108Z",
     "iopub.status.idle": "2023-10-12T19:09:32.437293Z",
     "shell.execute_reply": "2023-10-12T19:09:32.435628Z",
     "shell.execute_reply.started": "2023-10-12T19:09:32.413547Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/jumbo/keller-lab/projects/icom/nsi_unc/workflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8e5c3a-4cda-43d2-9adb-148ef787d147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:09:32.982832Z",
     "iopub.status.busy": "2023-10-12T19:09:32.982010Z",
     "iopub.status.idle": "2023-10-12T19:09:33.779471Z",
     "shell.execute_reply": "2023-10-12T19:09:33.778765Z",
     "shell.execute_reply.started": "2023-10-12T19:09:32.982779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd\n",
    "from util.files import *\n",
    "from util.const import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed84257-e5dc-4732-8feb-16d267f90184",
   "metadata": {},
   "source": [
    "# Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679ffbda-57c4-46ee-9de8-632dd7ad9d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:12:00.677614Z",
     "iopub.status.busy": "2023-10-12T19:12:00.677335Z",
     "iopub.status.idle": "2023-10-12T19:12:01.258599Z",
     "shell.execute_reply": "2023-10-12T19:12:01.256773Z",
     "shell.execute_reply.started": "2023-10-12T19:12:00.677592Z"
    }
   },
   "outputs": [],
   "source": [
    "# The fill_url function\n",
    "# For any URL/API endpoint, we want to replace\n",
    "# wildcard terms (FIPS, STATE, STATE_ALPHA)\n",
    "# with the appropriate value from the FIPS, STATE, or\n",
    "# STATE_ALPHA config values\n",
    "def fill_url(endpoint, wcard_dict):    \n",
    "    # Get a list of all the wildcards we need to replace for this endpoint\n",
    "    wildcards = [wcard for wcard in URL_WILDCARDS if wcard in endpoint]\n",
    "    # Replace the wildcard with the value stored in a wildcard dictionary\n",
    "    # Defined in download_data.py\n",
    "    for wildcard in wildcards:\n",
    "        endpoint = endpoint.replace(wildcard, wcard_dict[wildcard])\n",
    "        \n",
    "    return endpoint\n",
    "# Example unit test\n",
    "# Might be worth it to have a unit test file? \n",
    "test_endpoint = '{STATE_FIPS}_{STATE_ABBR}'\n",
    "wcard_dict = {'{STATE_FIPS}': '42',\n",
    "              '{STATE_ABBR}': 'PA'}\n",
    "assert fill_url(test_endpoint, wcard_dict) == '42_PA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ddd9f8d-863c-42e0-8cf1-43e42c614903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:12:20.844047Z",
     "iopub.status.busy": "2023-10-12T19:12:20.843477Z",
     "iopub.status.idle": "2023-10-12T19:12:20.881236Z",
     "shell.execute_reply": "2023-10-12T19:12:20.879321Z",
     "shell.execute_reply.started": "2023-10-12T19:12:20.843999Z"
    }
   },
   "outputs": [],
   "source": [
    "# The get_dir helper function\n",
    "# For a list of string tokens, we\n",
    "# are returning a filepath and filename\n",
    "# The last string token is most of the filename\n",
    "# If the first token is api, we need to use the exts dict\n",
    "# and append it to the last string token\n",
    "# If the first token is url, we need to use the endpoint\n",
    "# that is passed here to get the exact ext we are downloading\n",
    "def get_dir(str_tokens, endpoint):\n",
    "    # Get url or api type\n",
    "    end_type = str_tokens[0]\n",
    "    # Get most of the filename\n",
    "    file_pre = str_tokens[-1]\n",
    "    # Join the middle tokens as a filepath\n",
    "    mid_dirs = '/'.join(str_tokens[1:-1])\n",
    "\n",
    "    # Implement the api vs. url processing\n",
    "    if end_type == 'api':\n",
    "        # For example, file_pre will be something like\n",
    "        # \"nsi\" which is also our key in the exts dict\n",
    "        # for the ext we need to use\n",
    "        filename = file_pre + API_EXT[file_pre]\n",
    "    else:\n",
    "        # Ext is after the last '.' character\n",
    "        url_ext = endpoint.split('.')[-1]\n",
    "        filename = file_pre + '.' + url_ext\n",
    "\n",
    "    # Now join the raw directory with the mid_dirs\n",
    "    filepath = join(FR, mid_dirs, filename)\n",
    "    \n",
    "    # Return this directory path and the filename w/ extension\n",
    "    return filepath\n",
    "\n",
    "# Checking the nsi is correct\n",
    "# get_dir(['api', 'exp', 'nsi'], 'does not matter')\n",
    "\n",
    "# Checking something more complex is correct, like\n",
    "# a social vulnerability file which is in a nested directory\n",
    "# structure\n",
    "# get_dir(['url', 'vuln', 'social', 'noaa'],\n",
    "#          'https://coast.noaa.gov/htdata/SocioEconomic/SoVI2010/SoVI_2010_{STATE_ABBR}.zip')\n",
    "\n",
    "# Did visual checks for these. Could create assert statements\n",
    "# based on the path from the project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15964b80-1bfb-4ff9-8445-1a45fa1d03d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:13:12.028691Z",
     "iopub.status.busy": "2023-10-12T19:13:12.028121Z",
     "iopub.status.idle": "2023-10-12T19:13:12.641462Z",
     "shell.execute_reply": "2023-10-12T19:13:12.639659Z",
     "shell.execute_reply.started": "2023-10-12T19:13:12.028642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to process\n",
    "# the DOWNLOAD dataframe for use in \n",
    "# both the dwnld_out_files function\n",
    "# and when downloading files\n",
    "def process_file(file):\n",
    "    # The name follows format like\n",
    "    # county_api_exp_nsi\n",
    "    # which we will use to get \n",
    "    # file directories\n",
    "    name = file[0]\n",
    "    # The endpoint is what we're going to\n",
    "    # put into a requests call\n",
    "    endpoint = file[1]\n",
    "    # Split name from the 1st indexed token onwards\n",
    "    # Like api_exp_nsi\n",
    "    str_tokens = name.split('_')[1:]\n",
    "\n",
    "    return str_tokens, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da5c5d2-555b-4e95-a2f9-624df111091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:13:20.026077Z",
     "iopub.status.busy": "2023-10-12T19:13:20.025516Z",
     "iopub.status.idle": "2023-10-12T19:13:20.059547Z",
     "shell.execute_reply": "2023-10-12T19:13:20.057872Z",
     "shell.execute_reply.started": "2023-10-12T19:13:20.026031Z"
    }
   },
   "outputs": [],
   "source": [
    "# The dwnld_out_files function\n",
    "# For each URL/API endpoint, we want to return the output\n",
    "# version of that file\n",
    "# This is the function we need for the download_data\n",
    "# rule output\n",
    "def dwnld_out_files(files):\n",
    "    # For each file that needs to be downloaded\n",
    "    # Get the filepath\n",
    "    # Return the list of these out files\n",
    "    out_list = []\n",
    "    for file in files.itertuples():\n",
    "        str_tokens, endpoint = process_file(file)\n",
    "        \n",
    "        # Helper function to return \n",
    "        # our filepath from our str_tokens\n",
    "        # The last token is our filename\n",
    "        # Everything after the second token and before this\n",
    "        # are file directories\n",
    "        # For API endpoints, we need to access the \"ext\" element\n",
    "        # to get the filename (urls include the file ext)\n",
    "        # We use the exts dict for this\n",
    "        filepath = get_dir(str_tokens, endpoint)\n",
    "        \n",
    "        # Add this filepath to our out_list\n",
    "        out_list.append(filepath)\n",
    "\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2174b1b3-a8d3-4c05-bb63-2095d4a6ef8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:29:51.605913Z",
     "iopub.status.busy": "2023-10-12T19:29:51.605362Z",
     "iopub.status.idle": "2023-10-12T19:29:52.293115Z",
     "shell.execute_reply": "2023-10-12T19:29:52.291305Z",
     "shell.execute_reply.started": "2023-10-12T19:29:51.605864Z"
    }
   },
   "outputs": [],
   "source": [
    "# The download_url helper function\n",
    "# from https://stackoverflow.com/questions/9419162/\n",
    "# download-returned-zip-file-from-url\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0bdb9a-11f4-42d4-93c4-5f1a6c4e8324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:29:52.295925Z",
     "iopub.status.busy": "2023-10-12T19:29:52.295451Z",
     "iopub.status.idle": "2023-10-12T19:29:52.447361Z",
     "shell.execute_reply": "2023-10-12T19:29:52.445776Z",
     "shell.execute_reply.started": "2023-10-12T19:29:52.295881Z"
    }
   },
   "outputs": [],
   "source": [
    "# The downlload_api helper function\n",
    "# TODO: it may make sense to have some more\n",
    "# configuration data about\n",
    "# downloading from different apis\n",
    "# so want to split this from the download_url\n",
    "# function\n",
    "def download_api(url, save_path):\n",
    "    r = requests.get(url)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        fd.write(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9569979c-d18a-4568-89c8-a7e812d4d741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T19:29:52.449508Z",
     "iopub.status.busy": "2023-10-12T19:29:52.449056Z",
     "iopub.status.idle": "2023-10-12T19:29:52.483154Z",
     "shell.execute_reply": "2023-10-12T19:29:52.481766Z",
     "shell.execute_reply.started": "2023-10-12T19:29:52.449465Z"
    }
   },
   "outputs": [],
   "source": [
    "# The download_raw function\n",
    "# We are going to iterate through our \n",
    "# DOWNLOAD dataframe and\n",
    "# 1) clean the endpoint\n",
    "# 2) get the out filepath\n",
    "# 3) download the data\n",
    "# 4) write it in the out_filepath\n",
    "def download_raw(files, wcard_dict):\n",
    "    for file in files.itertuples():\n",
    "        # Get the str_tokens and endpoint from the dataframe row\n",
    "        str_tokens, endpoint = process_file(file)\n",
    "        # Get the out filepath\n",
    "        out_filepath = get_dir(str_tokens, endpoint)\n",
    "        # \"Clean\" the endpoint with the wcard_dict\n",
    "        endpoint = fill_url(endpoint, wcard_dict)\n",
    "\n",
    "        # If api, call download_api helper function\n",
    "        download_api(endpoint, out_filepath)\n",
    "\n",
    "        # If url, call download_url helper function\n",
    "        # and write file\n",
    "        download_url(endpoint, out_filepath)\n",
    "\n",
    "        # TODO log what is being done\n",
    "        print('Downloaded from: ' + str(endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d78d2-29f1-4651-ae98-2d79e06b838c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
